{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hplp/intro-to-training-jagadeemte/blob/main/Copy_of_PTandTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start by importing necessary packages\n",
        "You will begin by importing necessary libraries for this notebook. Run the cell below to do so."
      ],
      "metadata": {
        "id": "qFR0_os7r97e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch and Intro to Training"
      ],
      "metadata": {
        "id": "dcUHnV4tr3T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import thop\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n"
      ],
      "metadata": {
        "id": "o-K6uCK8sAtN",
        "outputId": "75a18743-a267-4eca-8766-9e1ed15ee3d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking the torch version and CUDA access\n",
        "Let's start off by checking the current torch version, and whether you have CUDA availablity."
      ],
      "metadata": {
        "id": "zDPegMqisFcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"torch is using version:\", torch.__version__, \"with CUDA=\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "tdfzsLrPsLyl",
        "outputId": "8ea168fb-5450-4ebc-b6f9-a434d0e56270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch is using version: 2.5.1+cu121 with CUDA= True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, you will see CUDA = False, meaning that the Colab session does not have access to a GPU. To remedy this, click the Runtime menu on top and select \"Change Runtime Type\", then select \"T4 GPU\".\n",
        "\n",
        "Re-run the import cell above, and the CUDA version / check. It should show now CUDA = True\n",
        "\n",
        "Sometimes in Colab you get a message that your Session has crashed, if that happens you need to go to the Runtime menu on top and select \"Restart session\".\n",
        "\n",
        "You won't be using the GPU just yet, but this prepares the instance for when you will.\n",
        "\n",
        "**Please note that the GPU is a scarce resource which may not be available at all time. Additionally, there are also usage limits that you may run into (although not likely for this assignment). When that happens you need to try again later/next day/different time of the day. Another reason to start the assignment early!**"
      ],
      "metadata": {
        "id": "kWQIH1ybssV0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Brief Introduction to PyTorch\n",
        "\n",
        "PyTorch, or torch, is a machine learning framework developed my Facebook AI Research, which competes with TensorFlow, JAX, Caffe and others.\n",
        "\n",
        "Roughly speaking, these frameworks can be split into dynamic and static defintion frameworks.\n",
        "\n",
        "**Static Network Definition:** The architecture and computation flow are defined simultaneously. The order and manner in which data flows through the layers are fixed upon definition. These frameworks also tend to declare parameter shapes implicitly via the compute graph. This is typical of TensorFlow and JAX.\n",
        "\n",
        "**Dynamic Network Definition:** The architecture (layers/modules) is defined independently of the computation flow, often during the object's initialization. This allows for dynamic computation graphs where the flow of data can change during runtime based on conditions. Since the network exists independent of the compute graph, the parameter shapes must be declared explitly. PyTorch follows this approach.\n",
        "\n",
        "All ML frameworks support automatic differentiation, which is necessary to train a model (i.e. perform back propagation).\n",
        "\n",
        "Let's consider a typical pytorch module. Such modules will inherit from the torch.nn.Module class, which provides many built in functions such as a wrapper for `__call__`, operations to move the module between devices (e.g. `cuda()`, `cpu()`), data-type conversion (e.g. `half()`, `float()`), and parameter and child management (e.g. `state_dict()`, `parameters()`)."
      ],
      "metadata": {
        "id": "ekG8rSD4tbpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inherit from torch.nn.Module\n",
        "class MyModule(nn.Module):\n",
        "  # constructor called upon creation\n",
        "  def __init__(self):\n",
        "    # the module has to initialize the parent first, which is what sets up the wrapper behavior\n",
        "    super().__init__()\n",
        "\n",
        "    # we can add sub-modules and parameters by assigning them to self\n",
        "    self.my_param = nn.Parameter(torch.zeros(4,8)) # this is how you define a raw parameter of shape 4x5\n",
        "    self.my_sub_module = nn.Linear(8,12)       # this is how you define a linear layer (tensorflow calls them Dense) of shape 8x12\n",
        "\n",
        "    # we can also add lists of modules, for example, the sequential layer\n",
        "    self.net = nn.Sequential(  # this layer type takes in a collection of modules rather than a list\n",
        "        nn.Linear(4,4),\n",
        "        nn.Linear(4,8),\n",
        "        nn.Linear(8,12)\n",
        "    )\n",
        "\n",
        "    # the above when calling self.net(x), will execute each module in the order they appear in a list\n",
        "    # it would be equivelent to x = self.net[2](self.net[1](self.net[0](x)))\n",
        "\n",
        "    # you can also create a list that doesn't execute\n",
        "    self.net_list = nn.ModuleList([\n",
        "        nn.Linear(7,7),\n",
        "        nn.Linear(7,9),\n",
        "        nn.Linear(9,14)\n",
        "    ])\n",
        "\n",
        "    # sometimes you will also see constant variables added to the module post init\n",
        "    foo = torch.Tensor([4])\n",
        "    self.register_buffer('foo', foo) # buffers allow .to(device, type) to apply\n",
        "\n",
        "  # let's define a forward function, which gets executed when calling the module, and defines the forward compute graph\n",
        "  def forward(self, x):\n",
        "\n",
        "    # if x is of shape Bx4\n",
        "    h1 =  x @ self.my_param # tensor-tensor multiplication uses the @ symbol\n",
        "    # then h1 is now shape Bx8, because my_param is 4x8... 2x4 * 4x8 = 2x8\n",
        "\n",
        "    h1 = self.my_sub_module(h1) # you execute a sub-module by calling it\n",
        "    # now, h1 is of shape Bx12, because my_sub_module was a 8x12 matrix\n",
        "\n",
        "    h2 = self.net(x)\n",
        "    # similarly, h2 is of shape Bx12, because that's the output of the sequence\n",
        "    # Bx4 -(4x4)-> Bx4 -(4x8)-> Bx8 -(8x12)-> Bx12\n",
        "\n",
        "    # since h1 and h2 are the same shape, they can be added together element-wise\n",
        "    return h1 + h2\n"
      ],
      "metadata": {
        "id": "lFxVVeLTsX7e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can instantiate the module and perform a forward pass by calling it."
      ],
      "metadata": {
        "id": "gGVD-Rfy3YXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the module\n",
        "module = MyModule()\n",
        "\n",
        "# you can print the module to get a high-level summary of it\n",
        "print(\"=== printing the module ===\")\n",
        "print(module)\n",
        "print()\n",
        "# notice that the sub-module name is in parenthesis, and so are the list indicies\n",
        "\n",
        "# let's view the shape of one of the weight tensors\n",
        "print(\"my_sub_module weight tensor shape=\", module.my_sub_module.weight.shape)\n",
        "# the above works because nn.Linear has a member called .weight and .bias\n",
        "# to view the shape of my_param, you would use module.my_param\n",
        "# and to view the shape of the 2nd elment in net_list, you would use module.net_list[1].weight\n",
        "\n",
        "# you can iterate through all of the parameters via the state dict\n",
        "print()\n",
        "print(\"=== Listing parameters from the state_dict ===\")\n",
        "for key,value in module.state_dict().items():\n",
        "  print(f\"{key}: {value.shape}\")\n"
      ],
      "metadata": {
        "id": "K5SduF4H194T",
        "outputId": "a010342e-0c2d-44af-e47b-c147380592ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== printing the module ===\n",
            "MyModule(\n",
            "  (my_sub_module): Linear(in_features=8, out_features=12, bias=True)\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=4, bias=True)\n",
            "    (1): Linear(in_features=4, out_features=8, bias=True)\n",
            "    (2): Linear(in_features=8, out_features=12, bias=True)\n",
            "  )\n",
            "  (net_list): ModuleList(\n",
            "    (0): Linear(in_features=7, out_features=7, bias=True)\n",
            "    (1): Linear(in_features=7, out_features=9, bias=True)\n",
            "    (2): Linear(in_features=9, out_features=14, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "my_sub_module weight tensor shape= torch.Size([12, 8])\n",
            "\n",
            "=== Listing parameters from the state_dict ===\n",
            "my_param: torch.Size([4, 8])\n",
            "foo: torch.Size([1])\n",
            "my_sub_module.weight: torch.Size([12, 8])\n",
            "my_sub_module.bias: torch.Size([12])\n",
            "net.0.weight: torch.Size([4, 4])\n",
            "net.0.bias: torch.Size([4])\n",
            "net.1.weight: torch.Size([8, 4])\n",
            "net.1.bias: torch.Size([8])\n",
            "net.2.weight: torch.Size([12, 8])\n",
            "net.2.bias: torch.Size([12])\n",
            "net_list.0.weight: torch.Size([7, 7])\n",
            "net_list.0.bias: torch.Size([7])\n",
            "net_list.1.weight: torch.Size([9, 7])\n",
            "net_list.1.bias: torch.Size([9])\n",
            "net_list.2.weight: torch.Size([14, 9])\n",
            "net_list.2.bias: torch.Size([14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can perform a forward pass by first creating a tensor to send through\n",
        "x = torch.zeros(2,4)\n",
        "# then you call the module (this invokes MyModule.forward() )\n",
        "y = module(x)\n",
        "\n",
        "# then you can print the result and shape\n",
        "print(x, x.shape)\n",
        "print(y, y.shape)"
      ],
      "metadata": {
        "id": "RZquyefY8r7W",
        "outputId": "70ba1d7b-86fb-45dd-87fd-2f07f1774e0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]]) torch.Size([2, 4])\n",
            "tensor([[-0.0946, -0.2721, -0.0908, -0.9849, -0.1707, -0.0207, -0.1869, -0.0110,\n",
            "          0.2822,  0.0379, -0.1346, -0.2241],\n",
            "        [-0.0946, -0.2721, -0.0908, -0.9849, -0.1707, -0.0207, -0.1869, -0.0110,\n",
            "          0.2822,  0.0379, -0.1346, -0.2241]], grad_fn=<AddBackward0>) torch.Size([2, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please check the cell below to notice the following:\n",
        "\n",
        "1.   `x` above was created with the shape 2x4, and in the forward pass, it gets manipulated into a 2x12 tensor. This last dimension is explicit, while the first is called the batch dimmension, and only exists on data (a.k.a. activations). The output shape can be seen in the print statement from y.shape\n",
        "2.   You can view the shape of a tensor by using `.shape`, this is a very helpful trick for debugging tensor shape errors\n",
        "3.   In the output, there's a `grad_fn` component, this is the hook created by the forward trace to be used in back-propagation via automatic differentiation. The function name is `AddBackward`, because the last operation performed was `h1+h2`.\n",
        "\n",
        "We might not always want to trace the compute graph though, such as during inference. In such cases, you can use the `torch.no_grad()` context manager.\n"
      ],
      "metadata": {
        "id": "Iifk09KA3m7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can perform a forward pass by first creating a tensor to send through\n",
        "x = torch.zeros(2,4)\n",
        "# then you call the module (this invokes MyModule.forward() )\n",
        "with torch.no_grad():\n",
        "  y = module(x)\n",
        "\n",
        "# then you can print the result and shape\n",
        "print(y, y.shape)\n",
        "# notice how the grad_fn is no longer part of the output tensor, that's because not_grad() disables the graph generation"
      ],
      "metadata": {
        "id": "2AkENjSg-NDe",
        "outputId": "ddec22d1-7125-48d4-a61b-623e5fef09a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0946, -0.2721, -0.0908, -0.9849, -0.1707, -0.0207, -0.1869, -0.0110,\n",
            "          0.2822,  0.0379, -0.1346, -0.2241],\n",
            "        [-0.0946, -0.2721, -0.0908, -0.9849, -0.1707, -0.0207, -0.1869, -0.0110,\n",
            "          0.2822,  0.0379, -0.1346, -0.2241]]) torch.Size([2, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aside from passing a tensor through a model with the `no_grad()` context, you can also detach a tensor from the compute graph by calling `.detach()`. This will effectively make a copy of the original tensor, which allows it to be converted to numpy and visualized with matplotlib.\n",
        "\n",
        "**Note:** Tensors with a `grad_fn` property cannot be plotted and must first be detached."
      ],
      "metadata": {
        "id": "3kHh2aBOqxkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer-Perceptron (MLP) Prediction of MNIST\n",
        "\n",
        "With some basics out of the way, let's create a MLP for training MNIST.\n",
        "You can start by defining a simple torch model."
      ],
      "metadata": {
        "id": "t08BDXVur1z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    # define the constructor for the network\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # the input projection layer - projects into d=128\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        # the first hidden layer - compresses into d=64\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        # the final output layer - splits into 10 classes (digits 0-9)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    # define the forward pass compute graph\n",
        "    def forward(self, x):\n",
        "        # x is of shape BxHxW\n",
        "\n",
        "        # we first need to unroll the 2D image using view\n",
        "        # we set the first dim to be -1 meanining \"everything else\", the reason being that x is of shape BxHxW, where B is the batch dim\n",
        "        # we want to maintain different tensors for each training sample in the batch, which means the output should be of shape BxF where F is the feature dim\n",
        "        x = x.view(-1, 28*28)\n",
        "        # x is of shape Bx784\n",
        "\n",
        "        # project-in and apply a non-linearity (ReLU activation function)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        # x is of shape Bx128\n",
        "\n",
        "        # middle-layer and apply a non-linearity (ReLU activation function)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        # x is of shape Bx64\n",
        "\n",
        "        # project out into the 10 classes\n",
        "        x = self.fc3(x)\n",
        "        # x is of shape Bx10\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ud7V-wA0rPIp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you can begin training, you have to do a little boiler-plate to load the dataset. From the previous assignment, you saw how a hosted dataset can be loaded with TensorFlow. With pytorch it's a little more complicated, as you need to manually condition the input data."
      ],
      "metadata": {
        "id": "CyyFDqlbusSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a transformation for the input images. This uses torchvision.transforms, and .Compose will act similarly to nn.Sequential\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # first convert to a torch tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # then normalize the input\n",
        "])\n",
        "\n",
        "# let's download the train and test datasets, applying the above transform - this will get saved locally into ./data, which is in the Colab instance\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "# we need to set the mini-batch (commonly referred to as \"batch\"), for now we can use 64\n",
        "batch_size = 64\n",
        "\n",
        "# then we need to create a dataloader for the train dataset, and we will also create one for the test dataset to evaluate performance\n",
        "# additionally, we will set the batch size in the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# the torch dataloaders allow us to access the __getitem__ method, which returns a tuple of (data, label)\n",
        "# additionally, the dataloader will pre-colate the training samples into the given batch_size\n"
      ],
      "metadata": {
        "id": "nd_dVCuUuqY2",
        "outputId": "8733eefa-dfe3-422b-8dc1-ec02862d3a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 14.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 436kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.07MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the first element of the test_loader, and verify both the tensor shapes and data types. You can check the data-type with `.dtype`\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "Edit the cell below to print out the first element shapes, dtype, and identify which is the training sample and which is the training label."
      ],
      "metadata": {
        "id": "lWtGZmUB5XcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first item from the test_loader\n",
        "first_item = next(iter(test_loader))\n",
        "\n",
        "# Extract the training sample (image data) and training label\n",
        "data, label = first_item\n",
        "\n",
        "# Print out the shapes and data types\n",
        "print(\"Data (Training Sample) Shape:\", data.shape)  # Shape of the batch of images\n",
        "print(\"Data (Training Sample) Dtype:\", data.dtype)  # Data type of the images\n",
        "\n",
        "print(\"Label (Training Label) Shape:\", label.shape)  # Shape of the batch of labels\n",
        "print(\"Label (Training Label) Dtype:\", label.dtype)  # Data type of the labels\n",
        "\n",
        "# Identify the training sample and label\n",
        "print(\"\\nThe 'Data' tensor contains the training samples (images).\")\n",
        "print(\"The 'Label' tensor contains the corresponding labels (digits 0-9).\")\n"
      ],
      "metadata": {
        "id": "PLB89iKQ5XBI",
        "outputId": "1318b2b4-d1d6-4307-fe67-6f2fc2dd5056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data (Training Sample) Shape: torch.Size([64, 1, 28, 28])\n",
            "Data (Training Sample) Dtype: torch.float32\n",
            "Label (Training Label) Shape: torch.Size([64])\n",
            "Label (Training Label) Dtype: torch.int64\n",
            "\n",
            "The 'Data' tensor contains the training samples (images).\n",
            "The 'Label' tensor contains the corresponding labels (digits 0-9).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the dataset loaded, we can instantiate the MLP model, the loss (or criterion function), and the optimizer for training."
      ],
      "metadata": {
        "id": "XW9dKZMl3050"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = MLP()\n",
        "\n",
        "# you can print the model as well, but notice how the activation functions are missing. This is because they were called in the forward pass\n",
        "# and not declared in the constructor\n",
        "print(model)\n",
        "\n",
        "# you can also count the model parameters\n",
        "param_count = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Model has {param_count:,} trainable parameters\")\n",
        "\n",
        "# for a critereon (loss) function, you will use Cross-Entropy Loss. This is the most common criterion used for multi-class prediction,\n",
        "# and is also used by tokenized transformer models it takes in an un-normalized probability distribution (i.e. without softmax) over\n",
        "# N classes (in our case, 10 classes with MNIST). This distribution is then compared to an integer label which is < N.\n",
        "# For MNIST, the prediction might be [-0.0056, -0.2044,  1.1726,  0.0859,  1.8443, -0.9627,  0.9785, -1.0752, 1.1376,  1.8220], with the label 3.\n",
        "# Cross-entropy can be thought of as finding the difference between the predicted distribution and the one-hot distribution\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# then you can instantiate the optimizer. You will use Stochastic Gradient Descent (SGD), and can set the learning rate to 0.1 with a momentum\n",
        "# factor of 0.5. the first input to the optimizer is the list of model parameters, which is obtained by calling .parameters() on the model object\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "7ZC7VVqm5ylL",
        "outputId": "4248a9bb-fba3-4ad7-efb4-6baf79da0d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "Model has 109,386 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you can define a training, and test loop"
      ],
      "metadata": {
        "id": "xsDmDIdt-ixB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0"
      ],
      "metadata": {
        "id": "NxCih-WJJKXk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# declare the train function\n",
        "def cpu_train(epoch, train_losses, steps, current_step):\n",
        "\n",
        "    # set the model in training mode - this doesn't do anything for us right now, but it is good practiced and needed with other layers such as\n",
        "    # batch norm and dropout\n",
        "    model.train()\n",
        "\n",
        "    # Create tqdm progress bar to help keep track of the training progress\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "\n",
        "    # loop over the dataset. Recall what comes out of the data loader, and then by wrapping that with enumerate() we get an index into the\n",
        "    # iterator list which we will call batch_idx\n",
        "    for batch_idx, (data, target) in pbar:\n",
        "\n",
        "        # during training, the first step is to zero all of the gradients through the optimizer\n",
        "        # this resets the state so that we can begin back propogation with the updated parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # then you can apply a forward pass, which includes evaluating the loss (criterion)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # given that you want to minimize the loss, you need to call .backward() on the result, which invokes the grad_fn property\n",
        "        loss.backward()\n",
        "\n",
        "        # the backward step will automatically differentiate the model and apply a gradient property to each of the parameters in the network\n",
        "        # so then all you have to do is call optimizer.step() to apply the gradients to the current parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # increment the step count\n",
        "        current_step += 1\n",
        "\n",
        "        # you should add some output to the progress bar so that you know which epoch you are training, and what the current loss is\n",
        "        if batch_idx % 100 == 0:\n",
        "\n",
        "            # append the last loss value\n",
        "            train_losses.append(loss.item())\n",
        "            steps.append(current_step)\n",
        "\n",
        "            desc = (f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
        "                    f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "            pbar.set_description(desc)\n",
        "\n",
        "    return current_step\n",
        "\n",
        "# declare a test function, this will help you evaluate the model progress on a dataset which is different from the training dataset\n",
        "# doing so prevents cross-contamination and misleading results due to overfitting\n",
        "def cpu_test(test_losses, test_accuracy, steps, current_step):\n",
        "\n",
        "    # put the model into eval mode, this again does not currently do anything for you, but it is needed with other layers like batch_norm\n",
        "    # and dropout\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Create tqdm progress bar\n",
        "    pbar = tqdm(test_loader, total=len(test_loader), desc=\"Testing...\")\n",
        "\n",
        "    # since you are not training the model, and do not need back-propagation, you can use a no_grad() context\n",
        "    with torch.no_grad():\n",
        "        # iterate over the test set\n",
        "        for data, target in pbar:\n",
        "            # like with training, run a forward pass through the model and evaluate the criterion\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item() # you are using .item() to get the loss value rather than the tensor itself\n",
        "\n",
        "            # you can also check the accuracy by sampling the output - you can use greedy sampling which is argmax (maximum probability)\n",
        "            # in general, you would want to normalize the logits first (the un-normalized output of the model), which is done via .softmax()\n",
        "            # however, argmax is taking the maximum value, which will be the same index for the normalized and un-normalized distributions\n",
        "            # so we can skip a step and take argmax directly\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    # append the final test loss\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy.append(correct/len(test_loader.dataset))\n",
        "    steps.append(current_step)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n"
      ],
      "metadata": {
        "id": "PdN0UYib8dBx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train for 10 epochs\n",
        "for epoch in range(0, 10):\n",
        "    current_step = cpu_train(current_epoch, train_losses, train_steps, current_step)\n",
        "    cpu_test(test_losses, test_accuracy, test_steps, current_step)\n",
        "    current_epoch += 1"
      ],
      "metadata": {
        "id": "tp6R3sRhUu9l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "713387cc-9833-4697-bf53-0482aafb0cbd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.143317: 100%|██████████| 938/938 [00:15<00:00, 60.29it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 75.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.2653, Accuracy: 9226/10000 (92%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.119472: 100%|██████████| 938/938 [00:14<00:00, 65.74it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 70.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1943, Accuracy: 9410/10000 (94%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.083593: 100%|██████████| 938/938 [00:14<00:00, 63.08it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 76.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1536, Accuracy: 9542/10000 (95%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.075049: 100%|██████████| 938/938 [00:14<00:00, 63.55it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 76.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1371, Accuracy: 9591/10000 (96%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.106909: 100%|██████████| 938/938 [00:14<00:00, 65.87it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 61.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1156, Accuracy: 9655/10000 (97%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.202814: 100%|██████████| 938/938 [00:14<00:00, 63.11it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 71.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1114, Accuracy: 9647/10000 (96%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.054692: 100%|██████████| 938/938 [00:14<00:00, 65.11it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 75.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0962, Accuracy: 9702/10000 (97%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.107146: 100%|██████████| 938/938 [00:14<00:00, 66.52it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 54.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0879, Accuracy: 9722/10000 (97%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.017227: 100%|██████████| 938/938 [00:14<00:00, 65.45it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 75.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0851, Accuracy: 9731/10000 (97%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.100554: 100%|██████████| 938/938 [00:14<00:00, 65.84it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 73.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0801, Accuracy: 9755/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**\n",
        "\n",
        "Using the skills you acquired in the previous assignment edit the cell below to use matplotlib to visualize the loss for training and validation for the first 10 epochs. They should be plotted on the same graph, labeled, and use a log-scale on the y-axis."
      ],
      "metadata": {
        "id": "PapjPPn7HEDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the losses for the first 10 epochs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming train_losses and test_losses have been populated during training and testing\n",
        "\n",
        "# For the first 10 epochs, we plot the losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot training losses\n",
        "plt.plot(train_steps[:10], train_losses[:10], label='Training Loss', color='blue')\n",
        "\n",
        "# Plot test (validation) losses\n",
        "plt.plot(test_steps[:10], test_losses[:10], label='Validation Loss', color='red')\n",
        "\n",
        "# Set the scale of the y-axis to logarithmic\n",
        "plt.yscale('log')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses (Log Scale)')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dRk-7A8L-xAf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "5c0bb11d-fd20-47f8-cfff-e7963b715964"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAIjCAYAAADxz9EgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAajBJREFUeJzt3XlcVPX+x/H3sIMIKCKI4m4q5r6l5lKSuKS5tHsLNfNWWpnt1zS1xbZ7bzelsltpdbO9XEpzN83MXVNRc9+XXBBXEDi/P85vBkcQEQfOzPB6Ph7zYDjnzJnP4LF4+/1+P8dmGIYhAAAAAIBL+FhdAAAAAAB4E0IWAAAAALgQIQsAAAAAXIiQBQAAAAAuRMgCAAAAABciZAEAAACACxGyAAAAAMCFCFkAAAAA4EKELAAAAABwIUIWAK/Vr18/Va1atVCvHTVqlGw2m2sLcjO7du2SzWbTpEmTiv29bTabRo0a5fh+0qRJstls2rVr1xVfW7VqVfXr18+l9VzLtYLC+frrr1W2bFmdPn3a6lLcxsKFC2Wz2bRw4cKrfu3777+vypUrKz093fWFAbhqhCwAxc5msxXoUZhfNOBajz32mGw2m7Zt23bZY4YPHy6bzaY//vijGCu7egcOHNCoUaO0du1aq0txsAfdt956y+pSilVWVpZefPFFPfroowoNDXVsr1q1qm699VYLK8tx+vRpvfjii7r++utVqlQpRUZGqlGjRnr88cd14MABq8vLpV+/fsrIyNCECROsLgWAJD+rCwBQ8nz22WdO33/66aeaM2dOru1169a9pvf573//q+zs7EK99oUXXtBzzz13Te/vDfr27atx48Zp8uTJGjlyZJ7HfPHFF6pfv74aNGhQ6Pe57777dPfddyswMLDQ57iSAwcOaPTo0apataoaNWrktO9arhVcvenTp2vLli0aNGiQ1aXk6cKFC2rXrp02b96spKQkPfroozp9+rQ2btyoyZMnq1evXoqNjbW6TCdBQUFKSkrSv/71Lz366KNePxIPuDtCFoBi97e//c3p+99//11z5szJtf1SZ8+eVUhISIHfx9/fv1D1SZKfn5/8/PhPZMuWLVWzZk198cUXeYaspUuXaufOnXrttdeu6X18fX3l6+t7Tee4FtdyreDqTZw4UW3atFHFihWtLiVPU6ZM0Zo1a/T555/r3nvvddp3/vx5ZWRkWFRZ/u6880698cYbWrBggW6++WarywFKNKYLAnBLHTp00PXXX69Vq1apXbt2CgkJ0T/+8Q9J0tSpU9WtWzfFxsYqMDBQNWrU0EsvvaSsrCync1y6zubiqVkffPCBatSoocDAQDVv3lwrVqxwem1ea7JsNpuGDBmiKVOm6Prrr1dgYKDq1aunn3/+OVf9CxcuVLNmzRQUFKQaNWpowoQJBV7ntXjxYt1xxx2qXLmyAgMDFRcXpyeeeELnzp3L9flCQ0O1f/9+9ezZU6GhoYqKitJTTz2V62eRmpqqfv36KTw8XBEREUpKSlJqauoVa5HM0azNmzdr9erVufZNnjxZNptN99xzjzIyMjRy5Eg1bdpU4eHhKlWqlNq2basFCxZc8T3yWpNlGIZefvllVapUSSEhIbrpppu0cePGXK89fvy4nnrqKdWvX1+hoaEKCwtTly5dtG7dOscxCxcuVPPmzSVJ/fv3d0xJta9Hy2tN1pkzZ/Tkk08qLi5OgYGBql27tt566y0ZhuF03NVcF4V15MgRPfDAA4qOjlZQUJAaNmyoTz75JNdxX375pZo2barSpUsrLCxM9evX13/+8x/H/gsXLmj06NGqVauWgoKCFBkZqRtvvFFz5sxxOs/mzZt1++23q2zZsgoKClKzZs00bdo0p2MKeq5LnT9/Xj///LMSEhIK9bPIzMzUSy+95Pj7W7VqVf3jH//ItRYpOztbo0aNUmxsrOP6SUlJKdCavu3bt0uS2rRpk2tfUFCQwsLCnLZt3rxZd955p6KiohQcHKzatWtr+PDhjv27d+/WI488otq1ays4OFiRkZG64447CrQGUZKWLVumzp07Kzw8XCEhIWrfvr2WLFmS67imTZuqbNmymjp1aoHOC6Do8M+0ANzWsWPH1KVLF919993629/+pujoaEnmL+ShoaEaNmyYQkNDNX/+fI0cOVJpaWl68803r3jeyZMn69SpU/r73/8um82mN954Q71799aOHTuuOKLx66+/6vvvv9cjjzyi0qVL65133lGfPn20Z88eRUZGSpLWrFmjzp07q0KFCho9erSysrI0ZswYRUVFFehzf/PNNzp79qwefvhhRUZGavny5Ro3bpz27dunb775xunYrKwsJSYmqmXLlnrrrbc0d+5c/fOf/1SNGjX08MMPSzLDym233aZff/1VDz30kOrWrasffvhBSUlJBaqnb9++Gj16tCZPnqwmTZo4vffXX3+ttm3bqnLlyjp69Kg+/PBD3XPPPXrwwQd16tQpffTRR0pMTNTy5ctzTdG7kpEjR+rll19W165d1bVrV61evVqdOnXKNYqwY8cOTZkyRXfccYeqVaumw4cPa8KECWrfvr1SUlIUGxurunXrasyYMRo5cqQGDRqktm3bSpJat26d53sbhqEePXpowYIFeuCBB9SoUSPNmjVLTz/9tPbv369///vfTscX5LoorHPnzqlDhw7atm2bhgwZomrVqumbb75Rv379lJqaqscff1ySNGfOHN1zzz3q2LGjXn/9dUnSpk2btGTJEscxo0aN0tixYzVw4EC1aNFCaWlpWrlypVavXq1bbrlFkrRx40bHKNNzzz2nUqVK6euvv1bPnj313XffqVevXgU+V15WrVqljIwMp2vpagwcOFCffPKJbr/9dj355JNatmyZxo4dq02bNumHH35wHPf888/rjTfeUPfu3ZWYmKh169YpMTFR58+fv+J7VKlSRZI5lfmFF17I9x9H/vjjD7Vt21b+/v4aNGiQqlatqu3bt2v69Ol65ZVXJEkrVqzQb7/9prvvvluVKlXSrl279N5776lDhw5KSUnJd4R+/vz56tKli5o2baoXX3xRPj4+mjhxom6++WYtXrxYLVq0cDq+SZMmeQYwAMXMAACLDR482Lj0P0ft27c3JBnvv/9+ruPPnj2ba9vf//53IyQkxDh//rxjW1JSklGlShXH9zt37jQkGZGRkcbx48cd26dOnWpIMqZPn+7Y9uKLL+aqSZIREBBgbNu2zbFt3bp1hiRj3Lhxjm3du3c3QkJCjP379zu2bd261fDz88t1zrzk9fnGjh1r2Gw2Y/fu3U6fT5IxZswYp2MbN25sNG3a1PH9lClTDEnGG2+84diWmZlptG3b1pBkTJw48Yo1NW/e3KhUqZKRlZXl2Pbzzz8bkowJEyY4zpmenu70uhMnThjR0dHGgAEDnLZLMl588UXH9xMnTjQkGTt37jQMwzCOHDliBAQEGN26dTOys7Mdx/3jH/8wJBlJSUmObefPn3eqyzDMP+vAwECnn82KFSsu+3kvvVbsP7OXX37Z6bjbb7/dsNlsTtdAQa+LvNivyTfffPOyx7z99tuGJON///ufY1tGRobRqlUrIzQ01EhLSzMMwzAef/xxIywszMjMzLzsuRo2bGh069Yt35o6duxo1K9f3+nvUnZ2ttG6dWujVq1aV3WuvHz44YeGJGP9+vW59lWpUiXfc65du9aQZAwcONBp+1NPPWVIMubPn28YhmEcOnTI8PPzM3r27Ol03KhRo3JdP3k5e/asUbt2bUOSUaVKFaNfv37GRx99ZBw+fDjXse3atTNKly7t9HfTMAyn6zavv9NLly41JBmffvqpY9uCBQsMScaCBQsc56hVq5aRmJiY63zVqlUzbrnlllznHTRokBEcHJzv5wNQ9JguCMBtBQYGqn///rm2BwcHO56fOnVKR48eVdu2bXX27Flt3rz5iue96667VKZMGcf39lGNHTt2XPG1CQkJqlGjhuP7Bg0aKCwszPHarKwszZ07Vz179nRaGF+zZk116dLliueXnD/fmTNndPToUbVu3VqGYWjNmjW5jn/ooYecvm/btq3TZ5kxY4b8/PwcI1uSuQbq0UcfLVA9krmObt++fVq0aJFj2+TJkxUQEKA77rjDcc6AgABJ5lSt48ePKzMzU82aNctzqmF+5s6dq4yMjFwL+IcOHZrr2MDAQPn4mP87y8rK0rFjxxQaGqratWtf9fvazZgxQ76+vnrsscectj/55JMyDEMzZ8502n6l6+JazJgxQzExMbrnnnsc2/z9/fXYY4/p9OnT+uWXXyRJEREROnPmTL7T9SIiIrRx40Zt3bo1z/3Hjx/X/Pnzdeeddzr+bh09elTHjh1TYmKitm7dqv379xfoXJdz7NgxSXL6O1hQM2bMkCQNGzbMafuTTz4pSfrpp58kSfPmzVNmZqYeeeQRp+MKes0HBwdr2bJlevrppyWZo+cPPPCAKlSooEcffdQxNfGvv/7SokWLNGDAAFWuXNnpHBdftxf/nb5w4YKOHTummjVrKiIiIt9rdO3atdq6davuvfdeHTt2zPHncebMGXXs2FGLFi3K1bClTJkyOnfunM6ePVugzwqgaBCyALitihUrOn5pv9jGjRvVq1cvhYeHKywsTFFRUY6mGSdPnrzieS/9Zcj+y96JEyeu+rX219tfe+TIEZ07d041a9bMdVxe2/KyZ88e9evXT2XLlnWss2rfvr2k3J8vKCgo1zTEi+uRzPUgFSpUcGqVLUm1a9cuUD2SdPfdd8vX11eTJ0+WZK6r+eGHH9SlSxenX5Y/+eQTNWjQwLFGJyoqSj/99FOB/lwutnv3bklSrVq1nLZHRUXl+uU8Oztb//73v1WrVi0FBgaqXLlyioqK0h9//HHV73vx+8fGxqp06dJO2+0dL+312V3purgWu3fvVq1atRxB8nK1PPLII7ruuuvUpUsXVapUSQMGDMi1LmzMmDFKTU3Vddddp/r16+vpp592ar2/bds2GYahESNGKCoqyunx4osvSjKv8YKc60qMS9a2FfRn4ePjk+vvUkxMjCIiIhw/C/vXS48rW7ZsgcNdeHi43njjDe3atUu7du3SRx99pNq1a2v8+PF66aWXJOX8w8z111+f77nOnTunkSNHOtb32a/R1NTUfK9Re4BNSkrK9efx4YcfKj09Pdfr7T9XugsC1mJNFgC3dfG//tqlpqaqffv2CgsL05gxY1SjRg0FBQVp9erVevbZZwvUhvtyXewK8kvftby2ILKysnTLLbfo+PHjevbZZ1WnTh2VKlVK+/fvV79+/XJ9vuLqyFe+fHndcsst+u6775ScnKzp06fr1KlT6tu3r+OY//3vf+rXr5969uypp59+WuXLl5evr6/Gjh3raCRQFF599VWNGDFCAwYM0EsvvaSyZcvKx8dHQ4cOLba27EV9XRRE+fLltXbtWs2aNUszZ87UzJkzNXHiRN1///2OJhnt2rXT9u3bNXXqVM2ePVsffvih/v3vf+v999/XwIEDHT+vp556SomJiXm+jz24XOlcl2Nfo3bixAlVqlSpUJ+1uANElSpVNGDAAPXq1UvVq1fX559/rpdffrnAr3/00Uc1ceJEDR06VK1atVJ4eLhsNpvuvvvufK9R+74333zzsmsaL/3HkxMnTigkJCTP/34CKD6ELAAeZeHChTp27Ji+//57tWvXzrF9586dFlaVo3z58goKCsrz5r353dDXbv369frzzz/1ySef6P7773dsv1LHtvxUqVJF8+bN0+nTp51+IduyZctVnadv3776+eefNXPmTE2ePFlhYWHq3r27Y/+3336r6tWr6/vvv3f6Jdg+AnK1NUvmv+RXr17dsf2vv/7KNTr07bff6qabbtJHH33ktD01NVXlypVzfH81v5hXqVJFc+fO1alTp5xGs+zTUe31FYcqVarojz/+UHZ2ttNoVl61BAQEqHv37urevbuys7P1yCOPaMKECRoxYoQjHJUtW1b9+/dX//79dfr0abVr106jRo3SwIEDHT9rf3//AnX/y+9cl1OnTh1J5t/Z+vXrX/XPIjs7W1u3bnW6j97hw4eVmprq+FnYv27btk3VqlVzHHfs2LFrGl0sU6aMatSooQ0bNkiS4+dl//5yvv32WyUlJemf//ynY9v58+ev2OHTPgU1LCyswN0Yd+7cec33GARw7ZguCMCj2EcMLh4hyMjI0LvvvmtVSU58fX2VkJCgKVOm6MCBA47t27Zty7WO53Kvl5w/n2EYTm24r1bXrl2VmZmp9957z7EtKytL48aNu6rz9OzZUyEhIXr33Xc1c+ZM9e7dW0FBQfnWvmzZMi1duvSqa05ISJC/v7/GjRvndL63334717G+vr65Roy++eYbx9ohu1KlSklSgVrXd+3aVVlZWRo/frzT9n//+9+y2WwFXl/nCl27dtWhQ4f01VdfObZlZmZq3LhxCg0NdUwlta91svPx8XHcINq+hujSY0JDQ1WzZk3H/vLly6tDhw6aMGGCDh48mKuWv/76y/H8Sue6nKZNmyogIEArV67M97i8dO3aVVLu6+Bf//qXJKlbt26SpI4dO8rPz8/pmpeU68/zctatW6ejR4/m2r57926lpKQ4ptpGRUWpXbt2+vjjj7Vnzx6nYy++JvO6RseNG5frVguXatq0qWrUqKG33npLp0+fzrX/4j8Pu9WrV1+2ayaA4sNIFgCP0rp1a5UpU0ZJSUl67LHHZLPZ9NlnnxXrtKwrGTVqlGbPnq02bdro4Ycfdvyyfv3112vt2rX5vrZOnTqqUaOGnnrqKe3fv19hYWH67rvvrulf37t37642bdroueee065duxQfH6/vv//+qtcrhYaGqmfPno51WRdPFZSkW2+9Vd9//7169eqlbt26aefOnXr//fcVHx+f5y+I+bHf72vs2LG69dZb1bVrV61Zs0YzZ850Gp2yv++YMWPUv39/tW7dWuvXr9fnn3/uNAImmaMCERERev/991W6dGmVKlVKLVu2dBrpsOvevbtuuukmDR8+XLt27VLDhg01e/ZsTZ06VUOHDnVqcuEK8+bNy7O1eM+ePTVo0CBNmDBB/fr106pVq1S1alV9++23WrJkid5++23HSNvAgQN1/Phx3XzzzapUqZJ2796tcePGqVGjRo6Rjfj4eHXo0MFxP6WVK1fq22+/1ZAhQxzvmZycrBtvvFH169fXgw8+qOrVq+vw4cNaunSp9u3b57j/WEHOlZegoCB16tRJc+fO1ZgxY3Lt37ZtW55T8Ro3bqxu3bopKSlJH3zwgWPq8PLly/XJJ5+oZ8+euummmyRJ0dHRevzxx/XPf/5TPXr0UOfOnbVu3TrH9XOlUc05c+boxRdfVI8ePXTDDTcoNDRUO3bs0Mcff6z09HSNGjXKcew777yjG2+8UU2aNNGgQYNUrVo17dq1Sz/99JPj7/utt96qzz77TOHh4YqPj9fSpUs1d+7cK7b39/Hx0YcffqguXbqoXr166t+/vypWrKj9+/drwYIFCgsL0/Tp0x3Hr1q1SsePH9dtt92W73kBFINi72cIAJe4XAv3evXq5Xn8kiVLjBtuuMEIDg42YmNjjWeeecaYNWuWU+tjw7h8C/e82mXrkpbil2vhPnjw4FyvrVKlSq6W0PPmzTMaN25sBAQEGDVq1DA+/PBD48knnzSCgoIu81PIkZKSYiQkJBihoaFGuXLljAcffNDREvzi9uNJSUlGqVKlcr0+r9qPHTtm3HfffUZYWJgRHh5u3HfffcaaNWsK3MLd7qeffjIkGRUqVMjVNj07O9t49dVXjSpVqhiBgYFG48aNjR9//DHXn4NhXLmFu2EYRlZWljF69GijQoUKRnBwsNGhQwdjw4YNuX7e58+fN5588knHcW3atDGWLl1qtG/f3mjfvr3T+06dOtWIj493tNO3f/a8ajx16pTxxBNPGLGxsYa/v79Rq1Yt480333RqpW3/LAW9Li5lvyYv9/jss88MwzCMw4cPG/379zfKlStnBAQEGPXr18/15/btt98anTp1MsqXL28EBAQYlStXNv7+978bBw8edBzz8ssvGy1atDAiIiKM4OBgo06dOsYrr7xiZGRkOJ1r+/btxv3332/ExMQY/v7+RsWKFY1bb73V+Pbbb6/6XHn5/vvvDZvNZuzZsyfXz+xyP4sHHnjAMAzDuHDhgjF69GijWrVqhr+/vxEXF2c8//zzTi3nDcO8pcCIESOMmJgYIzg42Lj55puNTZs2GZGRkcZDDz2Ub307duwwRo4cadxwww1G+fLlDT8/PyMqKsro1q2bo038xTZs2GD06tXLiIiIMIKCgozatWsbI0aMcOw/ceKE488vNDTUSExMNDZv3pzrGrm0hbvdmjVrjN69exuRkZFGYGCgUaVKFePOO+805s2b53Tcs88+a1SuXDnXNQqg+NkMw43++RcAvFjPnj0L1fIa8DZZWVmKj4/XnXfe6ejUVxxSU1NVpkwZvfzyyxo+fHixvW9xSE9PV9WqVfXcc885bj4NwDqsyQKAInDu3Dmn77du3aoZM2aoQ4cO1hQEuBFfX1+NGTNGycnJVz2VtKAu/Tso5azl8sa/hxMnTpS/v3+u++YBsAYjWQBQBCpUqKB+/fqpevXq2r17t9577z2lp6drzZo1ue79BMD1Jk2apEmTJqlr164KDQ3Vr7/+qi+++EKdOnXSrFmzrC4PgJej8QUAFIHOnTvriy++0KFDhxQYGKhWrVrp1VdfJWABxaRBgwby8/PTG2+8obS0NEczjKu5vxUAFBYjWQAAAADgQqzJAgAAAAAXImQBAAAAgAuxJisf2dnZOnDggEqXLn3FGxcCAAAA8F6GYejUqVOKjY2Vj0/+Y1WErHwcOHBAcXFxVpcBAAAAwE3s3btXlSpVyvcYQlY+SpcuLcn8QYaFhVlcDQAAAACrpKWlKS4uzpER8kPIyod9imBYWBghCwAAAECBlhHR+AIAAAAAXIiQBQAAAAAuRMgCAAAAABdiTRYAAAA8imEYyszMVFZWltWlwMv4+vrKz8/vmm/fRMgCAACAx8jIyNDBgwd19uxZq0uBlwoJCVGFChUUEBBQ6HMQsgAAAOARsrOztXPnTvn6+io2NlYBAQHXPOIA2BmGoYyMDP3111/auXOnatWqdcWbDl8OIQsAAAAeISMjQ9nZ2YqLi1NISIjV5cALBQcHy9/fX7t371ZGRoaCgoIKdR4aXwAAAMCjFHZ0ASgIV1xfXKEAAAAA4EKELAAAAABwIUIWAAAA4GGqVq2qt99+u8DHL1y4UDabTampqUVWE3IQsgAAAIAiYrPZ8n2MGjWqUOddsWKFBg0aVODjW7durYMHDyo8PLxQ71dQhDkT3QUBAACAInLw4EHH86+++kojR47Uli1bHNtCQ0Mdzw3DUFZWlvz8rvwrelRU1FXVERAQoJiYmKt6DQqPkSwAAAB4LMOQzpwp/odhFKy+mJgYxyM8PFw2m83x/ebNm1W6dGnNnDlTTZs2VWBgoH799Vdt375dt912m6KjoxUaGqrmzZtr7ty5Tue9dLqgzWbThx9+qF69eikkJES1atXStGnTHPsvHWGaNGmSIiIiNGvWLNWtW1ehoaHq3LmzUyjMzMzUY489poiICEVGRurZZ59VUlKSevbsWdg/Lp04cUL333+/ypQpo5CQEHXp0kVbt2517N+9e7e6d++uMmXKqFSpUqpXr55mzJjheG3fvn0VFRWl4OBg1apVSxMnTix0LUWJkAUAAACPdfasFBpa/I+zZ133GZ577jm99tpr2rRpkxo0aKDTp0+ra9eumjdvntasWaPOnTure/fu2rNnT77nGT16tO6880798ccf6tq1q/r27avjx4/n87M7q7feekufffaZFi1apD179uipp55y7H/99df1+eefa+LEiVqyZInS0tI0ZcqUa/qs/fr108qVKzVt2jQtXbpUhmGoa9euunDhgiRp8ODBSk9P16JFi7R+/Xq9/vrrjtG+ESNGKCUlRTNnztSmTZv03nvvqVy5ctdUT1FhuiAAAABgoTFjxuiWW25xfF+2bFk1bNjQ8f1LL72kH374QdOmTdOQIUMue55+/frpnnvukSS9+uqreuedd7R8+XJ17tw5z+MvXLig999/XzVq1JAkDRkyRGPGjHHsHzdunJ5//nn16tVLkjR+/HjHqFJhbN26VdOmTdOSJUvUunVrSdLnn3+uuLg4TZkyRXfccYf27NmjPn36qH79+pKk6tWrO16/Z88eNW7cWM2aNZNkjua5K0KWh9i/X1q5UgoPlzp0sLoaAAAA9xASIp0+bc37uoo9NNidPn1ao0aN0k8//aSDBw8qMzNT586du+JIVoMGDRzPS5UqpbCwMB05cuSyx4eEhDgCliRVqFDBcfzJkyd1+PBhtWjRwrHf19dXTZs2VXZ29lV9PrtNmzbJz89PLVu2dGyLjIxU7dq1tWnTJknSY489pocfflizZ89WQkKC+vTp4/hcDz/8sPr06aPVq1erU6dO6tmzpyOsuRumC3qIWbOknj2lN96wuhIAAAD3YbNJpUoV/8Nmc91nKFWqlNP3Tz31lH744Qe9+uqrWrx4sdauXav69esrIyMj3/P4+/tf8rOx5RuI8jreKOhisyIycOBA7dixQ/fdd5/Wr1+vZs2aady4cZKkLl26aPfu3XriiSd04MABdezY0Wl6ozshZHkIezOYQ4esrQMAAABFa8mSJerXr5969eql+vXrKyYmRrt27SrWGsLDwxUdHa0VK1Y4tmVlZWn16tWFPmfdunWVmZmpZcuWObYdO3ZMW7ZsUXx8vGNbXFycHnroIX3//fd68skn9d///texLyoqSklJSfrf//6nt99+Wx988EGh6ylKTBf0ENHR5tfDh62tAwAAAEWrVq1a+v7779W9e3fZbDaNGDGi0FP0rsWjjz6qsWPHqmbNmqpTp47GjRunEydOyFaAYbz169erdOnSju9tNpsaNmyo2267TQ8++KAmTJig0qVL67nnnlPFihV12223SZKGDh2qLl266LrrrtOJEye0YMEC1a1bV5I0cuRINW3aVPXq1VN6erp+/PFHxz53Q8jyEPaQdeSIlJ0t+TAGCQAA4JX+9a9/acCAAWrdurXKlSunZ599VmlpacVex7PPPqtDhw7p/vvvl6+vrwYNGqTExET5+vpe8bXt2rVz+t7X11eZmZmaOHGiHn/8cd16663KyMhQu3btNGPGDMfUxaysLA0ePFj79u1TWFiYOnfurH//+9+SzHt9Pf/889q1a5eCg4PVtm1bffnll67/4C5gM6yeeOnG0tLSFB4erpMnTyosLMzSWjIypMBA8/lff0lu2q0SAACgyJw/f147d+5UtWrVFBQUZHU5JU52drbq1q2rO++8Uy+99JLV5RSZy11nV5MNGMnyEAEBUtmy0vHj5pRBQhYAAACK0u7duzV79my1b99e6enpGj9+vHbu3Kl7773X6tLcHpPOPAjNLwAAAFBcfHx8NGnSJDVv3lxt2rTR+vXrNXfuXLddB+VOGMnyINHRUkoKzS8AAABQ9OLi4rRkyRKry/BIjGR5EEayAAAAAPdHyPIgtHEHAAAA3B8hy4MwkgUAAAC4P0KWB2EkCwAAAHB/hCwPQsgCAAAA3B8hy4MwXRAAAABwf4QsD2IfyfrrLykry9paAAAAUHw6dOigoUOHOr6vWrWq3n777XxfY7PZNGXKlGt+b1edpyQhZHmQqCjJZjMD1rFjVlcDAACAK+nevbs6d+6c577FixfLZrPpjz/+uOrzrlixQoMGDbrW8pyMGjVKjRo1yrX94MGD6tKli0vf61KTJk1SREREkb5HcSJkeRB/fyky0nzOuiwAAAD398ADD2jOnDnat29frn0TJ05Us2bN1KBBg6s+b1RUlEJCQlxR4hXFxMQoMDCwWN7LWxCyPAzrsgAAAC5iGNKZM8X/MIwClXfrrbcqKipKkyZNctp++vRpffPNN3rggQd07Ngx3XPPPapYsaJCQkJUv359ffHFF/me99Lpglu3blW7du0UFBSk+Ph4zZkzJ9drnn32WV133XUKCQlR9erVNWLECF24cEGSOZI0evRorVu3TjabTTabzVHzpdMF169fr5tvvlnBwcGKjIzUoEGDdPr0acf+fv36qWfPnnrrrbdUoUIFRUZGavDgwY73Kow9e/botttuU2hoqMLCwnTnnXfq8EWjDuvWrdNNN92k0qVLKywsTE2bNtXKlSslSbt371b37t1VpkwZlSpVSvXq1dOMGTMKXUtB+BXp2T1UcnKykpOTleWGC5+io6UNGxjJAgAAkCSdPSuFhhb/+54+LZUqdcXD/Pz8dP/992vSpEkaPny4bDabJOmbb75RVlaW7rnnHp0+fVpNmzbVs88+q7CwMP3000+67777VKNGDbVo0eKK75Gdna3evXsrOjpay5Yt08mTJ53Wb9mVLl1akyZNUmxsrNavX68HH3xQpUuX1jPPPKO77rpLGzZs0M8//6y5c+dKksLDw3Od48yZM0pMTFSrVq20YsUKHTlyRAMHDtSQIUOcguSCBQtUoUIFLViwQNu2bdNdd92lRo0a6cEHH7zi58nr89kD1i+//KLMzEwNHjxYd911lxYuXChJ6tu3rxo3bqz33ntPvr6+Wrt2rfz9/SVJgwcPVkZGhhYtWqRSpUopJSVFoUV8zRCy8jB48GANHjxYaWlpeV5cVmIkCwAAwLMMGDBAb775pn755Rd16NBBkjlVsE+fPgoPD1d4eLieeuopx/GPPvqoZs2apa+//rpAIWvu3LnavHmzZs2apdjYWEnSq6++mmsd1QsvvOB4XrVqVT311FP68ssv9cwzzyg4OFihoaHy8/NTjP0XzjxMnjxZ58+f16effqpS/x8yx48fr+7du+v1119X9P93aitTpozGjx8vX19f1alTR926ddO8efMKFbLmzZun9evXa+fOnYqLi5Mkffrpp6pXr55WrFih5s2ba8+ePXr66adVp04dSVKtWrUcr9+zZ4/69Omj+vXrS5KqV69+1TVcLUKWh+FeWQAAABcJCTFHlax43wKqU6eOWrdurY8//lgdOnTQtm3btHjxYo0ZM0aSlJWVpVdffVVff/219u/fr4yMDKWnpxd4zdWmTZsUFxfnCFiS1KpVq1zHffXVV3rnnXe0fft2nT59WpmZmQoLCyvw57C/V8OGDR0BS5LatGmj7OxsbdmyxRGy6tWrJ19fX8cxFSpU0Pr166/qvS5+z7i4OEfAkqT4+HhFRERo06ZNat68uYYNG6aBAwfqs88+U0JCgu644w7VqFFDkvTYY4/p4Ycf1uzZs5WQkKA+ffoUah3c1WBNlochZAEAAFzEZjOn7RX34/+n/RXUAw88oO+++06nTp3SxIkTVaNGDbVv316S9Oabb+o///mPnn32WS1YsEBr165VYmKiMjIyXPZjWrp0qfr27auuXbvqxx9/1Jo1azR8+HCXvsfF7FP17Gw2m7Kzs4vkvSSzM+LGjRvVrVs3zZ8/X/Hx8frhhx8kSQMHDtSOHTt03333af369WrWrJnGjRtXZLVIhCyPw3RBAAAAz3PnnXfKx8dHkydP1qeffqoBAwY41mctWbJEt912m/72t7+pYcOGql69uv78888Cn7tu3brau3evDh486Nj2+++/Ox3z22+/qUqVKho+fLiaNWumWrVqaffu3U7HBAQEXLEnQd26dbVu3TqdOXPGsW3JkiXy8fFR7dq1C1zz1bB/vr179zq2paSkKDU1VfHx8Y5t1113nZ544gnNnj1bvXv31sSJEx374uLi9NBDD+n777/Xk08+qf/+979FUqsdIcvDMJIFAADgeUJDQ3XXXXfp+eef18GDB9WvXz/Hvlq1amnOnDn67bfftGnTJv3973936px3JQkJCbruuuuUlJSkdevWafHixRo+fLjTMbVq1dKePXv05Zdfavv27XrnnXccIz12VatW1c6dO7V27VodPXpU6enpud6rb9++CgoKUlJSkjZs2KAFCxbo0Ucf1X333eeYKlhYWVlZWrt2rdNj06ZNSkhIUP369dW3b1+tXr1ay5cv1/3336/27durWbNmOnfunIYMGaKFCxdq9+7dWrJkiVasWKG6detKkoYOHapZs2Zp586dWr16tRYsWODYV1QIWR6GkSwAAADP9MADD+jEiRNKTEx0Wj/1wgsvqEmTJkpMTFSHDh0UExOjnj17Fvi8Pj4++uGHH3Tu3Dm1aNFCAwcO1CuvvOJ0TI8ePfTEE09oyJAhatSokX777TeNGDHC6Zg+ffqoc+fOuummmxQVFZVnG/mQkBDNmjVLx48fV/PmzXX77berY8eOGj9+/NX9MPJw+vRpNW7c2OnRvXt32Ww2TZ06VWXKlFG7du2UkJCg6tWr66uvvpIk+fr66tixY7r//vt13XXX6c4771SXLl00evRoSWZ4Gzx4sOrWravOnTvruuuu07vvvnvN9ebHZhgFbPJfAtm7C548efKqFwUWlUOHpAoVJB8fKSNDumg9IQAAgFc7f/68du7cqWrVqikoKMjqcuClLnedXU02YCTLw0RFmQErO1v66y+rqwEAAABwKUKWh/H1lcqVM5+zLgsAAABwP4QsD2Rfl0XIAgAAANwPIcsD2Ru30PwCAAAAcD+ELA9EG3cAAFCS0bcNRckV1xchywPRxh0AAJRE/v7+kqSzZ89aXAm8mf36sl9vheHnqmJQfBjJAgAAJZGvr68iIiJ05MgRSeY9m2w2m8VVwVsYhqGzZ8/qyJEjioiIkO813CuJkOWBGMkCAAAlVcz//yJkD1qAq0VERDius8IiZHkgRrIAAEBJZbPZVKFCBZUvX14XLlywuhx4GX9//2sawbIjZHkgRrIAAEBJ5+vr65JfhoGiQOMLD2QfyTp2TMrMtLYWAAAAAM4IWR4oMlLy9ZUMQ/rrL6urAQAAAHAxQpYH8vWVoqLM50wZBAAAANwLIctD0fwCAAAAcE+ELA9F8wsAAADAPRGyPBQjWQAAAIB7ImR5KEayAAAAAPdEyPJQjGQBAAAA7omQ5aEYyQIAAADcEyHLQzGSBQAAALgnQpaHso9kEbIAAAAA90LI8lD2kaxjx6QLF6ytBQAAAEAOQpaHKltW8vMznx85Ym0tAAAAAHIQsjyUj49Uvrz5nOYXAAAAgPsgZHkwml8AAAAA7oeQ5cFo4w4AAAC4H0KWB2MkCwAAAHA/hCwPxkgWAAAA4H4IWR6MkSwAAADA/RCyPBg3JAYAAADcDyHLg9lHspguCAAAALgPQpYHYyQLAAAAcD+ELA9mH8k6cUJKT7e2FgAAAAAmQpYHK1NG8vc3nx85Ym0tAAAAAEyELA9ms7EuCwAAAHA3hCwPRxt3AAAAwL0QsjwcNyQGAAAA3Ashy8MxkgUAAAC4F0KWh6ONOwAAAOBeCFkejsYXAAAAgHshZHk4RrIAAAAA90LI8nCMZAEAAADuhZDl4Wh8AQAAALgXQpaHs08XPHlSOn/e2loAAAAAELI8Xni4FBBgPmc0CwAAALAeIcvD2WzckBgAAABwJ4QsL8C6LAAAAMB9ELK8AG3cAQAAAPdByPICtHEHAAAA3AchywswkgUAAAC4D0KWF2AkCwAAAHAfhCwvQOMLAAAAwH0QsrwALdwBAAAA90HI8gKMZAEAAADug5DlBewjWadOSWfPWlsLAAAAUNIRsrxA6dJSUJD5nNEsAAAAwFqELC9gs7EuCwAAAHAXhCwvERVlfj161No6AAAAgJKOkOUl7CHrr7+srQMAAAAo6QhZXqJcOfMrI1kAAACAtQhZXoKRLAAAAMA9ELK8BCNZAAAAgHsgZHkJRrIAAAAA90DI8hKMZAEAAADugZDlJQhZAAAAgHsgZHkJpgsCAAAA7oGQ5SXsI1lpaVJGhrW1AAAAACUZIctLRERIvr7mc6YMAgAAANYhZHkJHx8pMtJ8TsgCAAAArEPI8iKsywIAAACsR8jyInQYBAAAAKxHyPIijGQBAAAA1iNkeRFGsgAAAADreX3I+vHHH1W7dm3VqlVLH374odXlFCl7yGIkCwAAALCOn9UFFKXMzEwNGzZMCxYsUHh4uJo2bapevXop0t6Gz8vYpwsykgUAAABYx6tHspYvX6569eqpYsWKCg0NVZcuXTR79myryyoyTBcEAAAArOfWIWvRokXq3r27YmNjZbPZNGXKlFzHJCcnq2rVqgoKClLLli21fPlyx74DBw6oYsWKju8rVqyo/fv3F0fplqDxBQAAAGA9tw5ZZ86cUcOGDZWcnJzn/q+++krDhg3Tiy++qNWrV6thw4ZKTEzUkSNHirlS98BIFgAAAGA9tw5ZXbp00csvv6xevXrluf9f//qXHnzwQfXv31/x8fF6//33FRISoo8//liSFBsb6zRytX//fsXGxl72/dLT05WWlub08CQXr8kyDGtrAQAAAEoqtw5Z+cnIyNCqVauUkJDg2Obj46OEhAQtXbpUktSiRQtt2LBB+/fv1+nTpzVz5kwlJiZe9pxjx45VeHi44xEXF1fkn8OV7CNZFy5IHpYPAQAAAK/hsSHr6NGjysrKUnR0tNP26OhoHTp0SJLk5+enf/7zn7rpppvUqFEjPfnkk/l2Fnz++ed18uRJx2Pv3r1F+hlcLShICg01n7MuCwAAALCGV7dwl6QePXqoR48eBTo2MDBQgYGBRVxR0SpXTjp92pwyWLOm1dUAAAAAJY/HjmSVK1dOvr6+Onz4sNP2w4cPKyYmxqKqrMcNiQEAAABreWzICggIUNOmTTVv3jzHtuzsbM2bN0+tWrWysDJrcUNiAAAAwFpuPV3w9OnT2rZtm+P7nTt3au3atSpbtqwqV66sYcOGKSkpSc2aNVOLFi309ttv68yZM+rfv7+FVVuLNu4AAACAtdw6ZK1cuVI33XST4/thw4ZJkpKSkjRp0iTddddd+uuvvzRy5EgdOnRIjRo10s8//5yrGUZJwg2JAQAAAGu5dcjq0KGDjCvc8GnIkCEaMmRIMVXk/hjJAgAAAKzlsWuykDdGsgAAAABrEbK8DCNZAAAAgLUIWV6GkSwAAADAWoQsL8NIFgAAAGAtQpaXsY9knTwpZWRYWwsAAABQEhGy8pCcnKz4+Hg1b97c6lKuWkSE5PP/f6rHjllaCgAAAFAi2Ywr9UgvwdLS0hQeHq6TJ08qLCzM6nIKrHx5c03WH39I9etbXQ0AAADg+a4mGzCS5YVofgEAAABYh5DlhWh+AQAAAFiHkOWFGMkCAAAArEPI8kKMZAEAAADWIWR5IUayAAAAAOsQsrwQI1kAAACAdQhZXoiRLAAAAMA6hCwvxEgWAAAAYB1ClheyhyxGsgAAAIDiR8jyQvbpgkePSoZhbS0AAABASUPI8kL2kawLF6RTp6ytBQAAAChpCFleKDhYKlXKfM6UQQAAAKB4EbK8FM0vAAAAAGsQsvKQnJys+Ph4NW/e3OpSCo027gAAAIA1CFl5GDx4sFJSUrRixQqrSyk0RrIAAAAAaxCyvBQjWQAAAIA1CFleipEsAAAAwBqELC/FDYkBAAAAaxCyvNTFNyQGAAAAUHwIWV6K6YIAAACANQhZXorGFwAAAIA1CFleipEsAAAAwBqELC9lH8lKTZUuXLC0FAAAAKBEIWR5qTJlJJ///9M9dszaWgAAAICShJDlpXx8pMhI8znrsgAAAIDiQ8jyYqzLAgAAAIofIcuLcUNiAAAAoPgRsrwYNyQGAAAAih8hy4sxXRAAAAAofoSsPCQnJys+Pl7Nmze3upRrwg2JAQAAgOJHyMrD4MGDlZKSohUrVlhdyjVhJAsAAAAofoQsL8ZIFgAAAFD8CFlejJEsAAAAoPgRsrwYI1kAAABA8SNkebGLR7IMw9paAAAAgJKCkOXF7CErI0M6dcraWgAAAICSgpDlxUJCzIfEuiwAAACguBCyvJx9NIt1WQAAAEDxIGR5OXvzC0ayAAAAgOJByPJytHEHAAAAihchy8vRxh0AAAAoXoQsL8dIFgAAAFC8CFlejpEsAAAAoHgRsrwcI1kAAABA8SJkeTlGsgAAAIDiRcjycoxkAQAAAMWLkOXluBkxAAAAULwIWV7OPl0wNVW6cMHSUgAAAIASgZDl5cqUkWw28/nx49bWAgAAAJQEhKw8JCcnKz4+Xs2bN7e6lGvm6ytFRprPmTIIAAAAFD1CVh4GDx6slJQUrVixwupSXILmFwAAAEDxIWSVALRxBwAAAIoPIasEYCQLAAAAKD6ErBKAkSwAAACg+BCySgBGsgAAAIDiQ8gqAbghMQAAAFB8CFklgH264JEj1tYBAAAAlASErBKgVi3z6x9/SIZhbS0AAACAtyNklQANG0p+fuaarN27ra4GAAAA8G6ErBIgKMgMWpK0fLm1tQAAAADejpBVQjRvbn5dscLaOgAAAABvR8gqIVq0ML8ykgUAAAAULUJWCWEfyVq1SsrKsrYWAAAAwJsRskqIunWlUqWkM2ekTZusrgYAAADwXoSsEsLXV2ra1HzOuiwAAACg6BCyShDWZQEAAABFj5BVgtBhEAAAACh6hKwSxD6StW6ddP68tbUAAAAA3oqQVYJUqSKVKydlZppBCwAAAIDrEbJKEJstZzSLKYMAAABA0SBklTD2dVk0vwAAAACKBiGrhGEkCwAAAChahKw8JCcnKz4+Xs3twz5exP6RNm+WTp60thYAAADAG9kMwzCsLsJdpaWlKTw8XCdPnlRYWJjV5bhMtWrSrl3SvHnSzTdbXQ0AAADg/q4mGzCSVQJxU2IAAACg6BCySiBuSgwAAAAUHUJWCcRIFgAAAFB0CFklUJMmko+PtG+fdPCg1dUAAAAA3oWQVQKFhkrx8eZzpgwCAAAArlWokLV3717t27fP8f3y5cs1dOhQffDBBy4rDEWLdVkAAABA0ShUyLr33nu1YMECSdKhQ4d0yy23aPny5Ro+fLjGjBnj0gJRNFiXBQAAABSNQoWsDRs2qMX//5b+9ddf6/rrr9dvv/2mzz//XJMmTXJlfSgiF49kcac0AAAAwHUKFbIuXLigwMBASdLcuXPVo0cPSVKdOnV0kE4KHqF+fSkwUDpxQtq+3epqAAAAAO9RqJBVr149vf/++1q8eLHmzJmjzp07S5IOHDigyMhIlxaIohEQIDVqZD5nXRYAAADgOoUKWa+//romTJigDh066J577lHDhg0lSdOmTXNMI4T7Y10WAAAA4Hp+hXlRhw4ddPToUaWlpalMmTKO7YMGDVJISIjLikPRosMgAAAA4HqFGsk6d+6c0tPTHQFr9+7devvtt7VlyxaVL1/epQWi6NhHslavljIzra0FAAAA8BaFClm33XabPv30U0lSamqqWrZsqX/+85/q2bOn3nvvPZcWiKJTq5YUFiadOydt3Gh1NQAAAIB3KFTIWr16tdq2bStJ+vbbbxUdHa3du3fr008/1TvvvOPSAlF0fHxypgyyLgsAAABwjUKFrLNnz6p06dKSpNmzZ6t3797y8fHRDTfcoN27d7u0QBQt1mUBAAAArlWokFWzZk1NmTJFe/fu1axZs9SpUydJ0pEjRxQWFubSAlG06DAIAAAAuFahQtbIkSP11FNPqWrVqmrRooVatWolyRzVaty4sUsLRNGyj2Rt2CCdPWttLQAAAIA3sBmGYRTmhYcOHdLBgwfVsGFD+fiYWW358uUKCwtTnTp1XFqkVdLS0hQeHq6TJ0967QidYUgVK0oHD0pLlkitW1tdEQAAAOB+riYbFOo+WZIUExOjmJgY7du3T5JUqVIlbkTsgWw2qX59M2Rt3kzIAgAAAK5VoaYLZmdna8yYMQoPD1eVKlVUpUoVRURE6KWXXlJ2drara0QRu+468+uWLdbWAQAAAHiDQo1kDR8+XB999JFee+01tWnTRpL066+/atSoUTp//rxeeeUVlxaJolW7tvn1zz+trQMAAADwBoUKWZ988ok+/PBD9ejRw7GtQYMGqlixoh555BFClodhJAsAAABwnUJNFzx+/HiezS3q1Kmj48ePX3NRKF72kLVtm5SVZW0tAAAAgKcrVMhq2LChxo8fn2v7+PHj1aBBg2suCsWrcmUpMFC6cEHiXtIAAADAtSnUdME33nhD3bp109y5cx33yFq6dKn27t2rGTNmuLRAKyQnJys5OVlZJWRYx8dHqlXLvFfWli1S9epWVwQAAAB4rkKNZLVv315//vmnevXqpdTUVKWmpqp3797auHGjPvvsM1fXWOwGDx6slJQUrVixwupSig3NLwAAAADXKPR9smJjY3M1uFi3bp0++ugjffDBB9dcGIqXfV0WIQsAAAC4NoUayYL3ocMgAAAA4BqELEhiuiAAAADgKoQsSMoZydq7VzpzxtpaAAAAAE92VWuyevfune/+1NTUa6kFFoqMlMqWlY4fN++X1bCh1RUBAAAAnumqQlZ4ePgV999///3XVBCsU7u2tHSpOWWQkAUAAAAUzlWFrIkTJxZVHXAD111nhiyaXwAAAACFx5osOND8AgAAALh2hCw4cK8sAAAA4NoRsuBw8b2yDMPaWgAAAABPRciCQ82aks0mpaZKR49aXQ0AAADgmQhZcAgOlipXNp/T/AIAAAAoHEIWnLAuCwAAALg2hCw4ocMgAAAAcG0IWXBycfMLAAAAAFePkAUnTBcEAAAArg0hC07s0wW3bZOysqytBQAAAPBEhCw4iYuTAgOljAxp926rqwEAAAA8DyELTnx9pVq1zOdMGQQAAACuHiELudD8AgAAACg8QhZyofkFAAAAUHiELOTCvbIAAACAwiNkIRemCwIAAACFR8hCLvaQtXevdPastbUAAAAAnoaQhVzKlZPKljWfb9tmbS0AAACApyFkIU9MGQQAAAAKh5CFPNH8AgAAACgcQhbyRBt3AAAAoHAIWcgT0wUBAACAwiFkIU/26YJbtkiGUTTvceCANHNm0ZwbAAAAsAohC3mqWdP8mpoqHT1aNO/Rt6/Utau0YEHRnB8AAACwAiELeQoOlipXNp8Xxbqs48elRYvM56tWuf78AAAAgFUIWbisouwwOHeulJ1tPmfdFwAAALwJIQuXVZDmF3/9JS1ZcvXnnjUr5zkdDAEAAOBNCFl5SE5OVnx8vJo3b251KZa6Uhv3TZukhg2lG2+UFi4s+HkNg5AFAAAA70XIysPgwYOVkpKiFStWWF2KpfKbLrhundS+vXTwoPn9558X/LwpKdL+/VJAgPn9oUNSWtq11QoAAAC4C0IWLss+krVtm5SVlbN9+XKpQwdzqmDFiua2KVOkzMyCndc+inXTTVJ0tPl861ZXVAwAAABYj5CFy6pcWQoMlNLTpT17zG2LF0sJCWZr91atzBGtyEizzbu9W+CVzJ5tfk1M5KbHAAAA8D6ELFyWr2/O/bK2bJHmzDGD0alT5ijU7NlmwOrZ0zzmu++ufM5z56RffjGfd+p05XVfAAAAgKchZCFf9hD07rvSrbeaIalLF+mnn6TQUHPf7bebX7//Pqct++UsXiydP29OM4yPL9o28QAAAIAVCFnIlz0ETZ8uZWRIvXpJP/xg3qzY7uabpfBws4HFb7/lfz77eqzERMlmYyQLAAAA3oeQhXzZQ5Ak3Xuv9PXX5jqtiwUESLfdZj7/9tv8z3dxyLr4/H/+abZ2BwAAADwdIQv56tJFuv566YknpE8/lfz88j6uTx/z63ffXX7K4L590saNko+P2TxDkqpXN78/dUo6fNj19QMAAADF7TK/MgOmmBhp/forH9epk7lGa98+acUKqWXL3MfYuwo2by6VLWs+DwyUqlaVduwwm2vExLisdAAAAMASjGTBJYKCzMYY0uWnDF46VdCO5hcAAADwJoQsuIy9y+B33+VeX5WVZbaAl3KHLJpfAAAAwJsQsuAynTubXQd37pTWrHHet3KldOKE2YWwRQvnfYQsAAAAeBNCFlymVCmpa1fz+aU3JrZPFUxIyN08g5AFAAAAb0LIgkvZuwx++63zlEF7yOrUKfdr7CFr+3YpM7MQb7p4sbRrVyFeCAAAALgeIQsu1a2b2THwzz/Ndu2SlJoqLVtmPr90PZYkVapkTjO8cKEQWencOalvX6lOHemZZ8w3AwAAACxEyIJLhYXljFbZuwzOm2c2vqhdW6pSJfdrfHykWrXM51c9ZfD4cfPF6enSm29KNWpI//mPlJFR6M8AAAAAXAtCFlzu4i6D0uVbt1+s0OuyKlaU5s6VfvpJio83Q9fQoebzS+csAgAAAMWAkAWX697dbG6xYYN5g+EiDVmSZLOZHTfWrZM++ECKjjYXeN1xh9SmjbR0aSFOCgAAABQOIQsuV6aM2UVQkl59VdqzRwoIkNq3v/xr7CFry5ZreGM/P+nBB6Vt26SRI6WQEDNgtW5tBq5t267h5AAAAEDBELJQJOxdBj/91Pzatq3Z4v1yXNrGPTRUGj1a2rpVeuABc9HXt9+aUwiHDpWOHXPBmwAAAAB5I2ShSPTsKfn65nyf31RBKSdk7dsnnTnjoiJiY6UPP5TWrjXvlHzhgtkUo0YNs0nG+fMueiMAAAAgByELRaJcOefpgVcKWZGR5kMqgll99etLM2dKs2dLDRtKJ0+a7d7r1JEmT5ays138hgAAACjJCFkoMvYugzExZs65EpdOGczLLbdIq1ZJkyaZXQl37zbvsdWypfTLL0X0pgAAAChpCFkoMvffby6JGj/ebAB4JUUesiRzDmNSkvkmL79srt9auVLq0EG67TZp8+YifHMAAACUBIQsFJlSpcwlUfYmGFfikg6DBRUSIg0fbs5NfPhhM3xNmyZdf730yCPSkSPFUAQAAAC8ESELbqNYRrIuFR0tvfuueVOvHj2krCzpvfekmjWlV16Rzp4txmIAAADgDQhZcBu1a5tft2yRDKOY37xOHWnqVGnhQqlZM+nUKemFF8zkN2mSGb4AAACAAiBkwW3UrGl+TU218FZW7dtLy5ZJn38uVaki7d8v9e8vNW0qzZljUVEAAADwJIQsuI3gYKlyZfN5sU4ZvJSPj3TvvWYTjDfekMLDpXXrpE6dpC5dpPXrLSwOAAAA7o6QBbdSrM0vriQoSHr6aWn7dunxxyU/P+nnn6VGjaSBA6UDB6yuEAAAAG6IkAW3YknziyuJjJTeflvatMm8+Vd2tvTRR1KtWtKLL0qnT1tdIQAAANwIIQtuxS1Dll3NmtI330hLlkitWpmdB8eMMcPWf/8rZWZaXSEAAADcACELbsXeYdAtQ5Zd69Zm0PrmG6lGDenQIWnQIKlhQ2nGDAtaIwIAAMCdELLgVuwjWVu3mrPy3JbNZk4dTEkxpxKWLWs+79ZNSkiQ1qyxukIAAABYhJAFt1KliuTvL6WnS3v3Wl1NAQQEmE0xtm83m2QEBEjz55st3++/30M+BAAAAFyJkAW34uubc78st+gwWFAREWa79y1bzPbvhiF99pk5NPf889LJk1ZXCAAAgGJCyILbcevmF1dStap5I+MVK8wbG58/L732mpkck5OlCxesrhAAAABFjJAFt+MRzS+upFkzacECaepU8wMdPSoNGSJdf700ZQrNMQAAALwYIQtux6NHsi5ms0k9ekjr10vvvitFRZkfqlcvqV07adkyqysEAABAESBkwe14Tciy8/eXHn5Y2rZNGj5cCg6Wfv1VuuEG6e67pZ07ra4QAAAALkTIgtuxh6xdu8wlTV4jLEx6+WUzPfbrZ450ffWVVKeO9OST0vHjVlcIAAAAFyBkwe2UL2/mEcMwO6N7nUqVpIkTzXtpJSRIGRnSv/5lNsf417/M/vUAAADwWIQsuB2bzQunDOalYUNp9mxp5kyzIcaJE+aIVt260tdf0xwDAADAQxGy4Ja8osNgQdhsUufO0tq10ocfShUqmGu07rpLatXKXLsFAAAAj0LIglsqESNZF/P1lR54QNq6VRo9WipVyuw+2Lat1Lt3CfpBAAAAeD5CFtySPWRt2WJtHcWuVClp5EgzbA0aJPn4SD/8INWrJz36qPTXX1ZXCAAAgCsgZMEt1aljft20qYQuTapQQZowQfrjD6lrVykzUxo/3myO8frr0rlzVlcIAACAyyBkwS3Vrm0uVzp+vIQP3tSrJ/30kzR3rtSokZSWJj33nPkD+t//pOxsqysEAADAJQhZcEvBwVK1aubzlBRra3ELHTtKq1ZJn35qtoDfu1e67z6peXNpwQKrqwMAAMBFCFlwW/Hx5ldC1v/z8TGD1Z9/SmPHSqVLS6tXSzffLHXvzg8KAADATRCy8pCcnKz4+Hg1b97c6lJKNELWZQQHm1MGt2+XhgyR/PykH3+U6teXHnpIOnTI6goBAABKNEJWHgYPHqyUlBStWLHC6lJKNELWFURFSePGSRs3Sr16meuzJkwwm2O89JJ05ozVFQIAAJRIhCy4LUJWAV13nfT999KiRVKLFma4GjnS3P7xx2ZnQgAAABQbQhbclr2N++HDZpdBXEHbttLvv0tffilVrSodOGDe4LhaNenll80fJAAAAIocIQtuq3RpqXJl8/mmTdbW4jFsNumuu6TNm6V//lMqV07at08aMUKKi5PuvVf69dcSevMxAACA4kHIgltjymAhBQZKw4aZrd4/+0y64QbpwgXpiy/MEa9GjaQPPmDdFgAAQBEgZMGt1a1rfiVkFVJQkPS3v0lLl5r32RowwNz2xx/S3/8uxcZKjz8ubdlidaUAAABeg5AFt8ZIlgs1aSJ99JG0f785lbBGDSktTXrnHXMBXEKC9MMPNMoAAAC4RoQsuDVCVhEoW9acSvjnn9LPP5s3MrbZpHnzpN69perVpVdeoVEGAABAIRGy4Nbs0wX37TMHXeBCPj5SYqI0bZq0Y4d5g+Ny5cx1XC+8kNMoY8kSGmUAAABcBUIW3FqZMlKFCuZzOgwWoapVpbFjzYD16adSy5Y5jTJuvFFq3JhGGQAAAAVEyILbs08ZJGQVg6Ag6b77zPttrVyZ0yhj3ToaZQAAABQQIQtuj3VZFmnaNKdRxltv5W6UccstNMoAAADIAyELbo+QZbGyZaUnnzQbZcycmdMoY+5cGmUAAADkgZAFt0fIchM+PlLnzjmNMp59VoqMpFEGAADAJQhZcHv2kLVrF30X3EbVqtJrr5ltH2mUAQAA4ISQBbdXrpz5MAz6LbidKzXKqFhRGjqUPzgAAFCiELLgEZgy6AHyapRx8qT0n//kNMqYMoVGGQAAwOsRsuARCFke5NJGGbfemtMoo1cvGmUAAACvR8iCRyBkeSB7o4zp0y/fKKNvXxplAAAAr0PIgkfghsQe7uJGGZ98ktMoY/LknEYZ//0vjTIAAIBXIGTBI9hD1rZtUnq6tbXgGgQFSfffn9Moo3//nEYZgwblNMr480+rKwUAACg0QhY8QkyMFBEhZWfz+7fXaNpU+vhjc3Tr0kYZtWtLnTrRKAMAAHgkQhY8gs3GuiyvFRmZd6OMOXNyGmW8+iqNMgAAgMcgZMFjELK83MWNMrZvl555JqdRxvDhOY0yfvuNRhkAAMCtEbLgMerWNb8SskqAatWk11/PaZTRokVOo4w2baQmTWiUAQAA3BYhCx6DkawSyN4oY9kyacWKnEYZa9fmNMp44gkW6gEAALdCyILHsIesP/80BzVQwjRr5twoo3p1s1HG22/nNMqYOpVGGQAAwHKELHiMuDgpNNT8HXrbNqurgWXsjTK2bpVmzJC6dctplNGzZ06jjCNHrK4UAACUUIQseAybLWddFjclhnx8pC5dpB9/zLtRRqVKNMoAAACWIGTBo7AuC3m6uFHGpEk0ygAAAJYiZMGjELKQr6AgKSmJRhkAAMBShCx4FEIWCuziRhlvvpm7UUarVtLjj0uffWbOP83KsrpiAADgJWyGwWKFy0lLS1N4eLhOnjypsLAwq8uBpB07pBo1pMBAc+aXr6/VFcFjZGdLs2ZJyclmw4xL/9NXqpQ5rbBZM6lpU/NrrVrm2i8AAFDiXU02IGTlg5DlfrKypNKlpXPnzOZyNWtaXRE80u7d0uLF0sqV5mPNGuns2dzHlS6dE7zs4atGDYIXAAAlECHLRQhZ7qlxY3OJzdSpUo8eVlcDr5CVJW3ebAauVatygtf587mPDQ83w5Z9tKtZM7Pxhs1W/HUDAIBiczXZwK+YagJcJj7eDFkpKYQsuIivr1SvnvlISjK3ZWaaa7Xso12rVpkX3smT0vz55sOuTJmc0GX/WqUKwQsAgBKKkAWPY29+wb2yUKT8/KT69c1H//7mtgsXpI0bc0a7Vq6U/vhDOnFCmjvXfNhFRjqPdjVtat5Rm+AFAIDXI2TB49BhEJbx95caNTIfDzxgbsvIkDZscJ5quH69dOyYNHu2+bCLinIOXc2aSbGxBC8AALwMa7LywZos97Rli1SnjtkMLi2NHgRwQ+npZtC6eKrhhg3mFMRLxcTknmpYoULx1wwAAPJF4wsXIWS5p8xMKSTEnLm1a5e59AVwe+fOmVMLL55qmJKS9/25YmNzTzWMji7+mgEAgAONL+DV/PzMe8lu2GD+jkrIgkcIDpZatjQfdmfPSuvWOU813LRJOnDAfEyfnnNspUrOoatpU3P6IQAAcDuELHik+Hjp8GGz0RvgsUJCpFatzIfdmTNmF8OLpxpu3izt22c+pkzJObZKldwjXmXLFvenAAAAl2C6YD6YLui+0tOlwECrqwCKyalT5n27Lp5q+OefeR9brZrz+q4mTcwW8wAA4JqwJstFCFkA3NbJk2bwuniq4bZteR9bo4bzaFeTJuZNlQEAQIERslyEkAXAo6SmSqtXO0813LEj72Ovu855qmHjxlLp0sVaLgAAnoSQ5SKELAAe7/hxM2xdPNVw9+7cx9lsZkeZ5s2lTp2krl1Z3wUAwEUIWS5CyALglY4ezQld9q979zof4+srtWkj9eghde9ujnwBAFCCEbJchJAFoMQ4csQMXIsXSz/+aN5M+WK1a5thq0cPsxuiH81pAQAlCyHLRQhZAEqsXbvM+3RNny4tXGje/duubFmpWzczdCUmSvz3EQBQAhCyXISQBQCS0tKkWbOkadOkGTPMdV52/v5Shw450wq5OzgAwEsRslyEkAUAl8jMlH77zRzhmjYt9/266tfPCVzNm0s+PtbUCQCAixGyXISQBQBXsGVLzrTCX3+VsrNz9kVHS7feaoauhAQpJMS6OgEAuEaELBchZAHAVTh2TJo50xzh+vln6dSpnH1BQWbQ6t7dDF6xsdbVCQBAIRCyXISQBQCFlJEhLVpkBq7p081GGhdr1ixnWmHDhuZ9ugAAcGOELBchZAGACxiGtGFDzjqu5cvNbXZxcWbY6t5duukmKTDQuloBALgMQpaLELIAoAgcOiT99JMZumbPls6dy9kXGip16mSOcnXtKkVFWVcnAAAXIWS5CCELAIrYuXPS/PnmCNePP0oHDuTss9mk1q1zRrnq1mVaIQDAMoQsFyFkAUAxys6WVq/OmVa4dq3z/ho1zLDVo4d0443mPboAACgmhCwXIWQBgIX27DFHt6ZPN0e7MjJy9kVESF26mKGrSxfzewAAihAhy0UIWQDgJk6dkubMMUe4fvpJOno0Z5+fn9S2bU63who1rKsTAOC1CFkuQsgCADeUlSUtW5bTHj4lxXl/fHzOOq4bbpB8fa2pEwDgVQhZLkLIAgAPsH17zjquRYvMEGZXrpzUrZs5ytWpk9m9EACAQiBkuQghCwA8zIkT0s8/m6Frxgzp5MmcfQEB0s0354xyxcVZVycAwOMQslyEkAUAHuzCBenXX3OmFW7f7ry/UaOcdVxNmkg+PpaUCQDwDIQsFyFkAYCXMAxp0yYzbE2fLv32m7nNLjZWuvVWM3B17CgFB1tXKwDALRGyXISQBQBe6q+/zOmE06ZJs2ZJZ87k7AsOlm65xRzl6tZNiomxrk4AgNsgZLkIIQsASoDz56WFC3OaZ+zb57y/ZUtzhKt5c6lMmZxHRASdCwGgBCFkuQghCwBKGMOQ1q3LWce1cmX+x4eF5QSuiwNYQbYFBBTDBwIAuAohy0UIWQBQwh04IP34o3kD5J07ze6FqanS6dPXfu6QkMIHtOBgyWa79hoAAAVGyHIRQhYAIE8XLphh68SJnOBlf36lbSdPOjfdKIyAgMIHtNKlCWgAUAhXkw38iqkmAAC8h7+/FBVlPq5WVpaUlla4gHbihPn6jAzp8GHzcbV8fc3QVZiAFh7OOjQAKABCFgAAxcnXNye0XC3DMKcqFiacnTghpaebIe3YMfNRGOHhzsGrfHmpTRuz9X3duoySAYCYLpgvpgsCALzKuXOFD2gXt7m/nJgY6eabzcDVsaNUpUoRfyAAKD6syXIRQhYAAP8vI8MMXZeGsV27pAULpMWLzXb4F6tRIydw3XRT4aZXAoCbIGS5CCELAIACSk+Xli6V5s0zH8uXm1MTL9agQU7oatfObMIBAB6CkOUihCwAAAopLc0c3bKHrj/+cN7v6yu1aJETulq1kgIDrakVAAqAkOUihCwAAFzkyBFzWuH8+Wbo2r7deX9QkHTjjTmhq0kTOhkCcCuELBchZAEAUER2784Z5Zo/Xzp0yHl/RITUoUNOIw06FwKwGCHLRQhZAAAUA8OQNm3KCV0LF5o3bb5YhQo5gevmm+lcCKDYEbJchJAFAIAFsrKk1atzQtevv9K5EIDlCFmX6NWrlxYuXKiOHTvq22+/LfDrCFkAALiB8+el33+ncyEASxGyLrFw4UKdOnVKn3zyCSELAABPl5YmLVqU00SDzoUAigEhKw8LFy7U+PHjCVkAAHgbe+dCexONSzsXBgebnQvta7roXAigEK4mG/gUU02XtWjRInXv3l2xsbGy2WyaMmVKrmOSk5NVtWpVBQUFqWXLllq+fHnxFwoAANxT+fLSXXdJH3wgbdsm7dolffSRdO+9UkyMdO6cNGeO9Pzz5ghXuXJSr17SuHFSSorZeAMAXMjP6gLOnDmjhg0basCAAerdu3eu/V999ZWGDRum999/Xy1bttTbb7+txMREbdmyReXLl5ckNWrUSJmZmbleO3v2bMXGxhb5ZwAAAG6kShVpwADzkVfnwtRUacoU8yHRuRCAy7nVdEGbzaYffvhBPXv2dGxr2bKlmjdvrvHjx0uSsrOzFRcXp0cffVTPPfdcgc9dkOmC6enpSk9Pd3yflpamuLg4pgsCAOAtMjOlNWvoXAjgqnnUdMH8ZGRkaNWqVUpISHBs8/HxUUJCgpYuXery9xs7dqzCw8Mdj7i4OJe/BwAAsJCfn9S8ufTcc+YUwhMnzHVcL7xgNsjw9TXXdH3wgTkFsXx5qWFDadgw6aefpFOnrP4EADyAW4eso0ePKisrS9HR0U7bo6OjdejSO8PnIyEhQXfccYdmzJihSpUqXTagPf/88zp58qTjsXfv3muqHwAAuLmgIHO06qWXpN9+k44fl6ZPl4YONdvCS2b3wn//W7r1VqlsWalNG2nECHPq4UUzYADAzvI1WcVh7ty5BTouMDBQgbR4BQCg5AoLM8PUrbea31/cuXDePGnHDjOM/fab9PLLOZ0L7eu56FwIQG4essqVKydfX18dPnzYafvhw4cVExNjUVUAAKDEsHcuvOsu8/tdu3LuzzV/vnTokDntcM4cc39EhNShgxm6WraU6tThxshACeTWISsgIEBNmzbVvHnzHM0wsrOzNW/ePA0ZMsTa4gAAQMlTterVdS6UpLg4qW5dKT7e+WtkpCUfAUDRszxknT59Wtu2bXN8v3PnTq1du1Zly5ZV5cqVNWzYMCUlJalZs2Zq0aKF3n77bZ05c0b9+/e3sGoAAFDi2WxmYIqPlx591OxcuHq1OcI1f765luvwYWnvXvMxe7bz66OicgLXxeErNtY8NwCPZXkL94ULF+qmm27KtT0pKUmTJk2SJI0fP15vvvmmDh06pEaNGumdd95Ry5Yti7y2q2nTCAAAkMvx4+Zol/2RkmJ+3b378q8JC8sdvOrWNUfRWO8FWOZqsoHlIcudEbIAAECROHNG2rw5d/jatk3Kysr7NUFBUu3aucNXrVpSQEDx1g+UQIQsFyFkAQCAYpWRIW3d6hy8Nm0yA9nl2sX7+ko1a+YOX3XqSKVKFW/9gBcjZLkIIQsAALiFrCyzs+HFwcv+PL8bJFepkve6rzJliq10wFsQslyEkAUAANyaYUgHDuQErotD2F9/Xf510dF5h6+YGJpuAJdByHIRQhYAAPBYR4/mHvXatMnsdHg54eG5W83XrWuOiPn4FF/tgBsiZLkIIQsAAHidU6fMNV6XTj3csUPKzs77NcHB5hqvS8NXzZqSv3/x1g9YhJB1jZKTk5WcnKysrCz9+eefhCwAAOD9zp83m25cOvXwzz/Nhhx58fMzuxteOvWwdm0zmAFehJDlIoxkAQCAEi8z0xzlymvq4Zkzeb/GZjPv65XXuq/w8GItH3AVQpaLELIAAAAuIztb2rcvd/BKSTFvwnw5UVFmd8OwMDNw2b9e/Dy/r6GhrA+DJa4mG/gVU00AAADwJj4+UuXK5iMxMWe7YZidDfMKXwcOmPvy63x4JTabVLp0wUJZfvsCA+mkiCJDyAIAAIDr2GxS+fLmo317530nT5r3+zp5UkpLM79e/PxKXzMzzRCXlmY+roW/f8FHzy4X2EqXNtelAZfgqgAAAEDxCA+XGjYs3GsNw2zOUZAwlt8++82bL1ww29wfPXptn6lUqYIHtcsFt1KlGFXzMoQsAAAAuD+bzexYGBxs3jS5sLKzzaBV0NGzywW38+fN8505Yz4OHix8TT4+ZtiKjpYqVZIqVnT+an8eFcV6NA9ByAIAAEDJ4eOTM4oUF1f482RkXH1Qyyu4ZWebj9RU87Fly+Xf099fio11Dl6Xfo2N5d5lboCQBQAAAFytgACpXDnzUViGIZ09mxO4Dh0yOzbu329+vfj54cPmFMfdu83H5dhs5ojY5UbD7F9LlSp83bgiWrjngxbuAAAAcAsXLphTEvMKYBd/vXChYOeLiLh8ALNvK1OGtWIXoYU7AAAA4E38/XNa5l9OdrbZyONyo2H790t795pryOzTEzduvPz5goLyXyNWqZI5aubr6+pP6/EYycoHI1kAAADwKvYW+JeOgl0axgraddHXV6pQIf8wFhtrBjYPx0gWAAAAgNxstpzGH/Hxlz/u/HkzbOU1JdEeyg4elLKycr7PT7ly+a8Rq1TJ7LDoJQhZAAAAAJwFBUk1apiPy8nMNBty5LdObN8+KT09555ka9de/nylS+fdMbFSJallS/MG1x6C6YL5YLogAAAAcA0MQzp+PP+pifv2md0V8zN1qtSjR/HUfBlMF7xGycnJSk5OVlZWltWlAAAAAJ7LZpMiI81Hw4aXP+706fynJlarVnw1uwAjWflgJAsAAACAdHXZwKeYagIAAACAEoGQBQAAAAAuRMgCAAAAABciZAEAAACACxGyAAAAAMCFCFkAAAAA4EKELAAAAABwIUIWAAAAALgQIQsAAAAAXIiQBQAAAAAuRMgCAAAAABciZAEAAACACxGyAAAAAMCFCFkAAAAA4EKELAAAAABwIUJWHpKTkxUfH6/mzZtbXQoAAAAAD2MzDMOwugh3lZaWpvDwcJ08eVJhYWFWlwMAAADAIleTDRjJAgAAAAAX8rO6AHdmH+RLS0uzuBIAAAAAVrJngoJMBCRk5ePUqVOSpLi4OIsrAQAAAOAOTp06pfDw8HyPYU1WPrKzs3XgwAGVLl1aNpvN6nKUlpamuLg47d27lzVicBmuKxQFrisUBa4rFBWuLRSEYRg6deqUYmNj5eOT/6orRrLy4ePjo0qVKlldRi5hYWH8BwAux3WFosB1haLAdYWiwrWFK7nSCJYdjS8AAAAAwIUIWQAAAADgQoQsDxIYGKgXX3xRgYGBVpcCL8J1haLAdYWiwHWFosK1BVej8QUAAAAAuBAjWQAAAADgQoQsAAAAAHAhQhYAAAAAuBAhCwAAAABciJDlIZKTk1W1alUFBQWpZcuWWr58udUlwU2MHTtWzZs3V+nSpVW+fHn17NlTW7ZscTrm/PnzGjx4sCIjIxUaGqo+ffro8OHDTsfs2bNH3bp1U0hIiMqXL6+nn35amZmZTscsXLhQTZo0UWBgoGrWrKlJkyYV9ceDm3jttddks9k0dOhQxzauKxTW/v379be//U2RkZEKDg5W/fr1tXLlSsd+wzA0cuRIVahQQcHBwUpISNDWrVudznH8+HH17dtXYWFhioiI0AMPPKDTp087HfPHH3+obdu2CgoKUlxcnN54441i+XwofllZWRoxYoSqVaum4OBg1ahRQy+99JIu7u/GdYViZcDtffnll0ZAQIDx8ccfGxs3bjQefPBBIyIiwjh8+LDVpcENJCYmGhMnTjQ2bNhgrF271ujatatRuXJl4/Tp045jHnroISMuLs6YN2+esXLlSuOGG24wWrdu7difmZlpXH/99UZCQoKxZs0aY8aMGUa5cuWM559/3nHMjh07jJCQEGPYsGFGSkqKMW7cOMPX19f4+eefi/XzovgtX77cqFq1qtGgQQPj8ccfd2znukJhHD9+3KhSpYrRr18/Y9myZcaOHTuMWbNmGdu2bXMc89prrxnh4eHGlClTjHXr1hk9evQwqlWrZpw7d85xTOfOnY2GDRsav//+u7F48WKjZs2axj333OPYf/LkSSM6Otro27evsWHDBuOLL74wgoODjQkTJhTr50XxeOWVV4zIyEjjxx9/NHbu3Gl88803RmhoqPGf//zHcQzXFYoTIcsDtGjRwhg8eLDj+6ysLCM2NtYYO3ashVXBXR05csSQZPzyyy+GYRhGamqq4e/vb3zzzTeOYzZt2mRIMpYuXWoYhmHMmDHD8PHxMQ4dOuQ45r333jPCwsKM9PR0wzAM45lnnjHq1avn9F533XWXkZiYWNQfCRY6deqUUatWLWPOnDlG+/btHSGL6wqF9eyzzxo33njjZfdnZ2cbMTExxptvvunYlpqaagQGBhpffPGFYRiGkZKSYkgyVqxY4Thm5syZhs1mM/bv328YhmG8++67RpkyZRzXmv29a9eu7eqPBDfQrVs3Y8CAAU7bevfubfTt29cwDK4rFD+mC7q5jIwMrVq1SgkJCY5tPj4+SkhI0NKlSy2sDO7q5MmTkqSyZctKklatWqULFy44XUN16tRR5cqVHdfQ0qVLVb9+fUVHRzuOSUxMVFpamjZu3Og45uJz2I/hOvRugwcPVrdu3XL92XNdobCmTZumZs2a6Y477lD58uXVuHFj/fe//3Xs37lzpw4dOuR0XYSHh6tly5ZO11ZERISaNWvmOCYhIUE+Pj5atmyZ45h27dopICDAcUxiYqK2bNmiEydOFPXHRDFr3bq15s2bpz///FOStG7dOv3666/q0qWLJK4rFD8/qwtA/o4ePaqsrCynX1IkKTo6Wps3b7aoKrir7OxsDR06VG3atNH1118vSTp06JACAgIUERHhdGx0dLQOHTrkOCava8y+L79j0tLSdO7cOQUHBxfFR4KFvvzyS61evVorVqzItY/rCoW1Y8cOvffeexo2bJj+8Y9/aMWKFXrssccUEBCgpKQkx7WR13Vx8XVTvnx5p/1+fn4qW7as0zHVqlXLdQ77vjJlyhTJ54M1nnvuOaWlpalOnTry9fVVVlaWXnnlFfXt21eSuK5Q7AhZgBcZPHiwNmzYoF9//dXqUuDh9u7dq8cff1xz5sxRUFCQ1eXAi2RnZ6tZs2Z69dVXJUmNGzfWhg0b9P777yspKcni6uCpvv76a33++eeaPHmy6tWrp7Vr12ro0KGKjY3luoIlmC7o5sqVKydfX99cHbsOHz6smJgYi6qCOxoyZIh+/PFHLViwQJUqVXJsj4mJUUZGhlJTU52Ov/gaiomJyfMas+/L75iwsDBGG7zQqlWrdOTIETVp0kR+fn7y8/PTL7/8onfeeUd+fn6Kjo7mukKhVKhQQfHx8U7b6tatqz179kjKuTby+/9eTEyMjhw54rQ/MzNTx48fv6rrD97j6aef1nPPPae7775b9evX13333acnnnhCY8eOlcR1heJHyHJzAQEBatq0qebNm+fYlp2drXnz5qlVq1YWVgZ3YRiGhgwZoh9++EHz58/PNY2hadOm8vf3d7qGtmzZoj179jiuoVatWmn9+vVO/3OZM2eOwsLCHL8MtWrVyukc9mO4Dr1Tx44dtX79eq1du9bxaNasmfr27et4znWFwmjTpk2u20z8+eefqlKliiSpWrVqiomJcbou0tLStGzZMqdrKzU1VatWrXIcM3/+fGVnZ6tly5aOYxYtWqQLFy44jpkzZ45q167NlC4vdPbsWfn4OP9a6+vrq+zsbElcV7CA1Z03cGVffvmlERgYaEyaNMlISUkxBg0aZERERDh17ELJ9fDDDxvh4eHGwoULjYMHDzoeZ8+edRzz0EMPGZUrVzbmz59vrFy50mjVqpXRqlUrx357q+1OnToZa9euNX7++WcjKioqz1bbTz/9tLFp0yYjOTmZVtslzMXdBQ2D6wqFs3z5csPPz8945ZVXjK1btxqff/65ERISYvzvf/9zHPPaa68ZERERxtSpU40//vjDuO222/Jstd24cWNj2bJlxq+//mrUqlXLqdV2amqqER0dbdx3333Ghg0bjC+//NIICQmh1baXSkpKMipWrOho4f79998b5cqVM5555hnHMVxXKE6ELA8xbtw4o3LlykZAQIDRokUL4/fff7e6JLgJSXk+Jk6c6Djm3LlzxiOPPGKUKVPGCAkJMXr16mUcPHjQ6Ty7du0yunTpYgQHBxvlypUznnzySePChQtOxyxYsMBo1KiRERAQYFSvXt3pPeD9Lg1ZXFcorOnTpxvXX3+9ERgYaNSpU8f44IMPnPZnZ2cbI0aMMKKjo43AwECjY8eOxpYtW5yOOXbsmHHPPfcYoaGhRlhYmNG/f3/j1KlTTsesW7fOuPHGG43AwECjYsWKxmuvvVbknw3WSEtLMx5//HGjcuXKRlBQkFG9enVj+PDhTq3Wua5QnGyGcdGtsAEAAAAA14Q1WQAAAADgQoQsAAAAAHAhQhYAAAAAuBAhCwAAAABciJAFAAAAAC5EyAIAAAAAFyJkAQAAAIALEbIAAAAAwIUIWQAAAADgQoQsAECJ8tdff+nhhx9W5cqVFRgYqJiYGCUmJmrJkiWSJJvNpilTplhbJADAo/lZXQAAAMWpT58+ysjI0CeffKLq1avr8OHDmjdvno4dO2Z1aQAAL2EzDMOwuggAAIpDamqqypQpo4ULF6p9+/a59letWlW7d+92fF+lShXt2rVLkjR16lSNHj1aKSkpio2NVVJSkoYPHy4/P/PfK202m959911NmzZNCxcuVIUKFfTGG2/o9ttvL5bPBgBwH0wXBACUGKGhoQoNDdWUKVOUnp6ea/+KFSskSRMnTtTBgwcd3y9evFj333+/Hn/8caWkpGjChAmaNGmSXnnlFafXjxgxQn369NG6devUt29f3X333dq0aVPRfzAAgFthJAsAUKJ89913evDBB3Xu3Dk1adJE7du31913360GDRpIMkekfvjhB/Xs2dPxmoSEBHXs2FHPP/+8Y9v//vc/PfPMMzpw4IDjdQ899JDee+89xzE33HCDmjRponfffbd4PhwAwC0wkgUAKFH69OmjAwcOaNq0aercubMWLlyoJk2aaNKkSZd9zbp16zRmzBjHSFhoaKgefPBBHTx4UGfPnnUc16pVK6fXtWrVipEsACiBaHwBAChxgoKCdMstt+iWW27RiBEjNHDgQL344ovq169fnsefPn1ao0ePVu/evfM8FwAAF2MkCwBQ4sXHx+vMmTOSJH9/f2VlZTntb9KkibZs2aKaNWvmevj45Pyv9Pfff3d63e+//666desW/QcAALgVRrIAACXGsWPHdMcdd2jAgAFq0KCBSpcurZUrV+qNN97QbbfdJsnsMDhv3jy1adNGgYGBKlOmjEaOHKlbb71VlStX1u233y4fHx+tW7dOGzZs0Msvv+w4/zfffKNmzZrpxhtv1Oeff67ly5fro48+surjAgAsQuMLAECJkZ6erlGjRmn27Nnavn27Lly4oLi4ON1xxx36xz/+oeDgYE2fPl3Dhg3Trl27VLFiRUcL91mzZmnMmDFas2aN/P39VadOHQ0cOFAPPvigJLPxRXJysqZMmaJFixapQoUKev3113XnnXda+IkBAFYgZAEA4AJ5dSUEAJRMrMkCAAAAABciZAEAAACAC9H4AgAAF2D2PQDAjpEsAAAAAHAhQhYAAAAAuBAhCwAAAABciJAFAAAAAC5EyAIAAAAAFyJkAQAAAIALEbIAAAAAwIUIWQAAAADgQv8HHHErDy1002cAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**\n",
        "\n",
        "The model may be able to train for a bit longer. Edit the cell below to modify the previous training code to also report the time per epoch and the time for 10 epochs with testing. You can use `time.time()` to get the current time in seconds.\n",
        "Then run the model for another 10 epochs, printing out the execution time at the end, and replot the loss functions with the extra 10 epochs below."
      ],
      "metadata": {
        "id": "KOeNII5-J2gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the losses for 20 epochs\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to track time during training\n",
        "for epoch in range(10, 20):  # Continue from epoch 10 to 19 for a total of 20 epochs\n",
        "    start_time = time.time()  # Record the start time of the epoch\n",
        "\n",
        "    # Train for one epoch\n",
        "    current_step = cpu_train(epoch, train_losses, train_steps, current_step)\n",
        "\n",
        "    # Test after training\n",
        "    cpu_test(test_losses, test_accuracy, test_steps, current_step)\n",
        "\n",
        "    # Measure time after epoch ends\n",
        "    epoch_time = time.time() - start_time  # Calculate time taken for the epoch\n",
        "    print(f'Epoch {epoch+1} completed in {epoch_time:.2f} seconds.')\n",
        "\n",
        "    # Increment the epoch counter\n",
        "    current_epoch += 1\n",
        "\n",
        "# Calculate the total time for 10 additional epochs (epoch 10 to 19)\n",
        "total_time = time.time() - start_time\n",
        "print(f'\\nTotal time for 10 epochs (with testing): {total_time:.2f} seconds.')\n",
        "\n"
      ],
      "metadata": {
        "id": "dhI_yXAjKjzJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "31d862a9-b9bf-4a90-be94-ac724d9f19e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.071250: 100%|██████████| 938/938 [00:14<00:00, 64.34it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 72.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0818, Accuracy: 9737/10000 (97%)\n",
            "\n",
            "Epoch 11 completed in 16.75 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.049412: 100%|██████████| 938/938 [00:15<00:00, 62.23it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 60.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0761, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Epoch 12 completed in 17.68 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.100223: 100%|██████████| 938/938 [00:14<00:00, 64.54it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 73.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0729, Accuracy: 9764/10000 (98%)\n",
            "\n",
            "Epoch 13 completed in 16.67 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.013847: 100%|██████████| 938/938 [00:14<00:00, 63.93it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 74.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0698, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Epoch 14 completed in 16.78 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.159503: 100%|██████████| 938/938 [00:15<00:00, 61.24it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 74.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0798, Accuracy: 9761/10000 (98%)\n",
            "\n",
            "Epoch 15 completed in 17.45 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.038621: 100%|██████████| 938/938 [00:14<00:00, 64.94it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 75.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0681, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Epoch 16 completed in 16.54 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.007841: 100%|██████████| 938/938 [00:14<00:00, 64.57it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 64.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0687, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Epoch 17 completed in 16.98 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.036781: 100%|██████████| 938/938 [00:14<00:00, 62.97it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 73.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0698, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Epoch 18 completed in 17.04 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.012288: 100%|██████████| 938/938 [00:14<00:00, 64.45it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 72.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0680, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Epoch 19 completed in 16.72 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.022509: 100%|██████████| 938/938 [00:14<00:00, 64.51it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 53.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0705, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Epoch 20 completed in 17.49 seconds.\n",
            "\n",
            "Total time for 10 epochs (with testing): 17.49 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replot the losses for the first 20 epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot training losses\n",
        "plt.plot(train_steps[:20], train_losses[:20], label='Training Loss', color='blue')\n",
        "\n",
        "# Plot test (validation) losses\n",
        "plt.plot(test_steps[:20], test_losses[:20], label='Validation Loss', color='red')\n",
        "\n",
        "# Set the scale of the y-axis to logarithmic\n",
        "plt.yscale('log')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses (Log Scale)')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "axbm2Wmtz-Km",
        "outputId": "fff52b05-dc37-40bd-d259-db87c2c14815"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAIjCAYAAADxz9EgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAdP5JREFUeJzt3Xd4FNX+x/HPpieEJJRACIReQ1WaoBQlSpMmigUFlGIBG+BF5VL1imL9CbEr2OsVsKA0QRBRUIogSJMmvYUQWkhyfn/M3U2W9GST3U3er+fZZ3dnZmfOTgbIh3POd2zGGCMAAAAAgEv4uLsBAAAAAFCSELIAAAAAwIUIWQAAAADgQoQsAAAAAHAhQhYAAAAAuBAhCwAAAABciJAFAAAAAC5EyAIAAAAAFyJkAQAAAIALEbIAlFhDhgxRzZo1C/TZyZMny2azubZBHmb37t2y2WyaPXt2sR/bZrNp8uTJjvezZ8+WzWbT7t27c/1szZo1NWTIEJe2pzDXCgrms88+U/ny5ZWUlOTupniMZcuWyWazadmyZfn+7Guvvabq1avrwoULrm8YgHwjZAEodjabLU+PgvyiAdd64IEHZLPZtGPHjmy3GT9+vGw2m/74449ibFn+HThwQJMnT9b69evd3RQHe9B97rnn3N2UYpWamqpJkybp/vvvV2hoqGN5zZo1df3117uxZemSkpI0adIkNWnSRGXKlFGFChXUokULPfjggzpw4IC7m5fJkCFDlJycrNdff93dTQEgyc/dDQBQ+rz//vtO79977z0tWrQo0/JGjRoV6jhvvvmm0tLSCvTZf//733r00UcLdfySYODAgZoxY4Y++ugjTZw4McttPv74YzVt2lTNmjUr8HHuuOMO3XLLLQoMDCzwPnJz4MABTZkyRTVr1lSLFi2c1hXmWkH+ff3119q6datGjBjh7qZk6eLFi+rYsaP++usvDR48WPfff7+SkpL0559/6qOPPlK/fv0UHR3t7mY6CQoK0uDBg/XCCy/o/vvvL/E98YCnI2QBKHa333670/tffvlFixYtyrT8UmfPnlVISEiej+Pv71+g9kmSn5+f/Pz4K7Jt27aqW7euPv744yxD1qpVq7Rr1y49/fTThTqOr6+vfH19C7WPwijMtYL8mzVrlq688kpVrVrV3U3J0ty5c7Vu3Tp9+OGHuu2225zWnT9/XsnJyW5qWc4GDBig6dOna+nSpbrmmmvc3RygVGO4IACP1LlzZzVp0kS///67OnbsqJCQED3++OOSpHnz5qlnz56Kjo5WYGCg6tSpoyeeeEKpqalO+7h0nk3GoVlvvPGG6tSpo8DAQLVu3Vpr1qxx+mxWc7JsNptGjRqluXPnqkmTJgoMDFTjxo31/fffZ2r/smXL1KpVKwUFBalOnTp6/fXX8zzPa8WKFbrppptUvXp1BQYGKiYmRg8//LDOnTuX6fuFhoZq//796tu3r0JDQxUZGamxY8dmOhcJCQkaMmSIwsPDFRERocGDByshISHXtkhWb9Zff/2ltWvXZlr30UcfyWaz6dZbb1VycrImTpyoli1bKjw8XGXKlFGHDh20dOnSXI+R1ZwsY4yefPJJVatWTSEhIbr66qv1559/ZvrsiRMnNHbsWDVt2lShoaEKCwtT9+7dtWHDBsc2y5YtU+vWrSVJd955p2NIqn0+WlZzss6cOaMxY8YoJiZGgYGBatCggZ577jkZY5y2y891UVBHjhzR0KFDVblyZQUFBal58+Z69913M233ySefqGXLlipbtqzCwsLUtGlT/d///Z9j/cWLFzVlyhTVq1dPQUFBqlChgq666iotWrTIaT9//fWXbrzxRpUvX15BQUFq1aqVvvrqK6dt8rqvS50/f17ff/+94uLiCnQuUlJS9MQTTzj+/NasWVOPP/54prlIaWlpmjx5sqKjox3Xz+bNm/M0p2/nzp2SpCuvvDLTuqCgIIWFhTkt++uvvzRgwABFRkYqODhYDRo00Pjx4x3r9+zZo/vuu08NGjRQcHCwKlSooJtuuilPcxAl6ddff1W3bt0UHh6ukJAQderUSStXrsy0XcuWLVW+fHnNmzcvT/sFUHT4b1oAHuv48ePq3r27brnlFt1+++2qXLmyJOsX8tDQUI0ePVqhoaH64YcfNHHiRCUmJurZZ5/Ndb8fffSRTp8+rbvvvls2m03Tp0/XDTfcoL///jvXHo2ffvpJX375pe677z6VLVtWL7/8svr376+9e/eqQoUKkqR169apW7duqlKliqZMmaLU1FRNnTpVkZGRefren3/+uc6ePat7771XFSpU0OrVqzVjxgz9888/+vzzz522TU1NVdeuXdW2bVs999xzWrx4sZ5//nnVqVNH9957ryQrrPTp00c//fST7rnnHjVq1Ehz5szR4MGD89SegQMHasqUKfroo490+eWXOx37s88+U4cOHVS9enUdO3ZMb731lm699VYNHz5cp0+f1ttvv62uXbtq9erVmYbo5WbixIl68skn1aNHD/Xo0UNr167Vddddl6kX4e+//9bcuXN10003qVatWjp8+LBef/11derUSZs3b1Z0dLQaNWqkqVOnauLEiRoxYoQ6dOggSWrfvn2WxzbGqHfv3lq6dKmGDh2qFi1aaMGCBXrkkUe0f/9+vfjii07b5+W6KKhz586pc+fO2rFjh0aNGqVatWrp888/15AhQ5SQkKAHH3xQkrRo0SLdeuut6tKli5555hlJ0pYtW7Ry5UrHNpMnT9a0adM0bNgwtWnTRomJifrtt9+0du1aXXvttZKkP//809HL9Oijj6pMmTL67LPP1LdvX/33v/9Vv3798ryvrPz+++9KTk52upbyY9iwYXr33Xd14403asyYMfr11181bdo0bdmyRXPmzHFs99hjj2n69Onq1auXunbtqg0bNqhr1646f/58rseoUaOGJGso87///e8c/3Pkjz/+UIcOHeTv768RI0aoZs2a2rlzp77++mv95z//kSStWbNGP//8s2655RZVq1ZNu3fv1quvvqrOnTtr8+bNOfbQ//DDD+revbtatmypSZMmycfHR7NmzdI111yjFStWqE2bNk7bX3755VkGMADFzACAm40cOdJc+tdRp06djCTz2muvZdr+7NmzmZbdfffdJiQkxJw/f96xbPDgwaZGjRqO97t27TKSTIUKFcyJEyccy+fNm2ckma+//tqxbNKkSZnaJMkEBASYHTt2OJZt2LDBSDIzZsxwLOvVq5cJCQkx+/fvdyzbvn278fPzy7TPrGT1/aZNm2ZsNpvZs2eP0/eTZKZOneq07WWXXWZatmzpeD937lwjyUyfPt2xLCUlxXTo0MFIMrNmzcq1Ta1btzbVqlUzqampjmXff/+9kWRef/11xz4vXLjg9LmTJ0+aypUrm7vuustpuSQzadIkx/tZs2YZSWbXrl3GGGOOHDliAgICTM+ePU1aWppju8cff9xIMoMHD3YsO3/+vFO7jLF+1oGBgU7nZs2aNdl+30uvFfs5e/LJJ522u/HGG43NZnO6BvJ6XWTFfk0+++yz2W7z0ksvGUnmgw8+cCxLTk427dq1M6GhoSYxMdEYY8yDDz5owsLCTEpKSrb7at68uenZs2eOberSpYtp2rSp05+ltLQ00759e1OvXr187Ssrb731lpFkNm7cmGldjRo1ctzn+vXrjSQzbNgwp+Vjx441kswPP/xgjDHm0KFDxs/Pz/Tt29dpu8mTJ2e6frJy9uxZ06BBAyPJ1KhRwwwZMsS8/fbb5vDhw5m27dixoylbtqzTn01jjNN1m9Wf6VWrVhlJ5r333nMsW7p0qZFkli5d6thHvXr1TNeuXTPtr1atWubaa6/NtN8RI0aY4ODgHL8fgKLHcEEAHiswMFB33nlnpuXBwcGO16dPn9axY8fUoUMHnT17Vn/99Veu+7355ptVrlw5x3t7r8bff/+d62fj4uJUp04dx/tmzZopLCzM8dnU1FQtXrxYffv2dZoYX7duXXXv3j3X/UvO3+/MmTM6duyY2rdvL2OM1q1bl2n7e+65x+l9hw4dnL7L/Pnz5efn5+jZkqw5UPfff3+e2iNZ8+j++ecfLV++3LHso48+UkBAgG666SbHPgMCAiRZQ7VOnDihlJQUtWrVKsuhhjlZvHixkpOTM03gf+ihhzJtGxgYKB8f65+z1NRUHT9+XKGhoWrQoEG+j2s3f/58+fr66oEHHnBaPmbMGBlj9N133zktz+26KIz58+crKipKt956q2OZv7+/HnjgASUlJenHH3+UJEVEROjMmTM5DteLiIjQn3/+qe3bt2e5/sSJE/rhhx80YMAAx5+tY8eO6fjx4+ratau2b9+u/fv352lf2Tl+/LgkOf0ZzKv58+dLkkaPHu20fMyYMZKkb7/9VpK0ZMkSpaSk6L777nPaLq/XfHBwsH799Vc98sgjkqze86FDh6pKlSq6//77HUMTjx49quXLl+uuu+5S9erVnfaR8brN+Gf64sWLOn78uOrWrauIiIgcr9H169dr+/btuu2223T8+HHHz+PMmTPq0qWLli9fnqlgS7ly5XTu3DmdPXs2T98VQNEgZAHwWFWrVnX80p7Rn3/+qX79+ik8PFxhYWGKjIx0FM04depUrvu99Jch+y97J0+ezPdn7Z+3f/bIkSM6d+6c6tatm2m7rJZlZe/evRoyZIjKly/vmGfVqVMnSZm/X1BQUKZhiBnbI1nzQapUqeJUKluSGjRokKf2SNItt9wiX19fffTRR5KseTVz5sxR9+7dnX5Zfvfdd9WsWTPHHJ3IyEh9++23efq5ZLRnzx5JUr169ZyWR0ZGZvrlPC0tTS+++KLq1aunwMBAVaxYUZGRkfrjjz/yfdyMx4+OjlbZsmWdltsrXtrbZ5fbdVEYe/bsUb169RxBMru23Hfffapfv766d++uatWq6a677so0L2zq1KlKSEhQ/fr11bRpUz3yyCNOpfd37NghY4wmTJigyMhIp8ekSZMkWdd4XvaVG3PJ3La8ngsfH59Mf5aioqIUERHhOBf250u3K1++fJ7DXXh4uKZPn67du3dr9+7devvtt9WgQQPNnDlTTzzxhKT0/5hp0qRJjvs6d+6cJk6c6JjfZ79GExIScrxG7QF28ODBmX4eb731li5cuJDp8/bzSnVBwL2YkwXAY2X831+7hIQEderUSWFhYZo6darq1KmjoKAgrV27VuPGjctTGe7sqtjl5Ze+wnw2L1JTU3XttdfqxIkTGjdunBo2bKgyZcpo//79GjJkSKbvV1wV+SpVqqRrr71W//3vfxUfH6+vv/5ap0+f1sCBAx3bfPDBBxoyZIj69u2rRx55RJUqVZKvr6+mTZvmKCRQFJ566ilNmDBBd911l5544gmVL19ePj4+euihh4qtLHtRXxd5UalSJa1fv14LFizQd999p++++06zZs3SoEGDHEUyOnbsqJ07d2revHlauHCh3nrrLb344ot67bXXNGzYMMf5Gjt2rLp27ZrlcezBJbd9Zcc+R+3kyZOqVq1agb5rcQeIGjVq6K677lK/fv1Uu3Ztffjhh3ryySfz/Pn7779fs2bN0kMPPaR27dopPDxcNptNt9xyS47XqH3ds88+m+2cxkv/8+TkyZMKCQnJ8u9PAMWHkAXAqyxbtkzHjx/Xl19+qY4dOzqW79q1y42tSlepUiUFBQVlefPenG7oa7dx40Zt27ZN7777rgYNGuRYnlvFtpzUqFFDS5YsUVJSktMvZFu3bs3XfgYOHKjvv/9e3333nT766COFhYWpV69ejvVffPGFateurS+//NLpl2B7D0h+2yxZ/5Nfu3Ztx/KjR49m6h364osvdPXVV+vtt992Wp6QkKCKFSs63ufnF/MaNWpo8eLFOn36tFNvln04qr19xaFGjRr6448/lJaW5tSblVVbAgIC1KtXL/Xq1UtpaWm677779Prrr2vChAmOcFS+fHndeeeduvPOO5WUlKSOHTtq8uTJGjZsmONc+/v756n6X077yk7Dhg0lWX9mmzZtmu9zkZaWpu3btzvdR+/w4cNKSEhwnAv7844dO1SrVi3HdsePHy9U72K5cuVUp04dbdq0SZIc58v+PjtffPGFBg8erOeff96x7Pz587lW+LQPQQ0LC8tzNcZdu3YV+h6DAAqP4YIAvIq9xyBjD0FycrJeeeUVdzXJia+vr+Li4jR37lwdOHDAsXzHjh2Z5vFk93nJ+fsZY5zKcOdXjx49lJKSoldffdWxLDU1VTNmzMjXfvr27auQkBC98sor+u6773TDDTcoKCgox7b/+uuvWrVqVb7bHBcXJ39/f82YMcNpfy+99FKmbX19fTP1GH3++eeOuUN2ZcqUkaQ8la7v0aOHUlNTNXPmTKflL774omw2W57n17lCjx49dOjQIX366aeOZSkpKZoxY4ZCQ0MdQ0ntc53sfHx8HDeIts8hunSb0NBQ1a1b17G+UqVK6ty5s15//XUdPHgwU1uOHj3qeJ3bvrLTsmVLBQQE6Lfffstxu6z06NFDUubr4IUXXpAk9ezZU5LUpUsX+fn5OV3zkjL9PLOzYcMGHTt2LNPyPXv2aPPmzY6htpGRkerYsaPeeecd7d2712nbjNdkVtfojBkzMt1q4VItW7ZUnTp19NxzzykpKSnT+ow/D7u1a9dmWzUTQPGhJwuAV2nfvr3KlSunwYMH64EHHpDNZtP7779frMOycjN58mQtXLhQV155pe69917HL+tNmjTR+vXrc/xsw4YNVadOHY0dO1b79+9XWFiY/vvf/xbqf9979eqlK6+8Uo8++qh2796t2NhYffnll/merxQaGqq+ffs65mVlHCooSddff72+/PJL9evXTz179tSuXbv02muvKTY2NstfEHNiv9/XtGnTdP3116tHjx5at26dvvvuO6feKftxp06dqjvvvFPt27fXxo0b9eGHHzr1gElWr0BERIRee+01lS1bVmXKlFHbtm2dejrsevXqpauvvlrjx4/X7t271bx5cy1cuFDz5s3TQw895FTkwhWWLFmSZWnxvn37asSIEXr99dc1ZMgQ/f7776pZs6a++OILrVy5Ui+99JKjp23YsGE6ceKErrnmGlWrVk179uzRjBkz1KJFC0fPRmxsrDp37uy4n9Jvv/2mL774QqNGjXIcMz4+XldddZWaNm2q4cOHq3bt2jp8+LBWrVqlf/75x3H/sbzsKytBQUG67rrrtHjxYk2dOjXT+h07dmQ5FO+yyy5Tz549NXjwYL3xxhuOocOrV6/Wu+++q759++rqq6+WJFWuXFkPPvignn/+efXu3VvdunXThg0bHNdPbr2aixYt0qRJk9S7d29dccUVCg0N1d9//6133nlHFy5c0OTJkx3bvvzyy7rqqqt0+eWXa8SIEapVq5Z2796tb7/91vHn/frrr9f777+v8PBwxcbGatWqVVq8eHGu5f19fHz01ltvqXv37mrcuLHuvPNOVa1aVfv379fSpUsVFhamr7/+2rH977//rhMnTqhPnz457hdAMSj2eoYAcInsSrg3btw4y+1XrlxprrjiChMcHGyio6PNv/71L7NgwQKn0sfGZF/CPaty2bqkpHh2JdxHjhyZ6bM1atTIVBJ6yZIl5rLLLjMBAQGmTp065q233jJjxowxQUFB2ZyFdJs3bzZxcXEmNDTUVKxY0QwfPtxREjxj+fHBgwebMmXKZPp8Vm0/fvy4ueOOO0xYWJgJDw83d9xxh1m3bl2eS7jbffvtt0aSqVKlSqay6Wlpaeapp54yNWrUMIGBgeayyy4z33zzTaafgzG5l3A3xpjU1FQzZcoUU6VKFRMcHGw6d+5sNm3alOl8nz9/3owZM8ax3ZVXXmlWrVplOnXqZDp16uR03Hnz5pnY2FhHOX37d8+qjadPnzYPP/ywiY6ONv7+/qZevXrm2WefdSqlbf8ueb0uLmW/JrN7vP/++8YYYw4fPmzuvPNOU7FiRRMQEGCaNm2a6ef2xRdfmOuuu85UqlTJBAQEmOrVq5u7777bHDx40LHNk08+adq0aWMiIiJMcHCwadiwofnPf/5jkpOTnfa1c+dOM2jQIBMVFWX8/f1N1apVzfXXX2+++OKLfO8rK19++aWx2Wxm7969mc5Zdudi6NChxhhjLl68aKZMmWJq1apl/P39TUxMjHnsscecSs4bY91SYMKECSYqKsoEBweba665xmzZssVUqFDB3HPPPTm27++//zYTJ040V1xxhalUqZLx8/MzkZGRpmfPno4y8Rlt2rTJ9OvXz0RERJigoCDToEEDM2HCBMf6kydPOn5+oaGhpmvXruavv/7KdI1cWsLdbt26deaGG24wFSpUMIGBgaZGjRpmwIABZsmSJU7bjRs3zlSvXj3TNQqg+NmM8aD//gWAEqxv374FKnkNlDSpqamKjY3VgAEDHJX6ikNCQoLKlSunJ598UuPHjy+24xaHCxcuqGbNmnr00UcdN58G4D7MyQKAInDu3Dmn99u3b9f8+fPVuXNn9zQI8CC+vr6aOnWq4uPj8z2UNK8u/TMopc/lKol/DmfNmiV/f/9M980D4B70ZAFAEahSpYqGDBmi2rVra8+ePXr11Vd14cIFrVu3LtO9nwC43uzZszV79mz16NFDoaGh+umnn/Txxx/ruuuu04IFC9zdPAAlHIUvAKAIdOvWTR9//LEOHTqkwMBAtWvXTk899RQBCygmzZo1k5+fn6ZPn67ExERHMYz83N8KAAqKniwAAAAAcCHmZAEAAACACxGyAAAAAMCFmJOVg7S0NB04cEBly5bN9caFAAAAAEouY4xOnz6t6Oho+fjk3FdFyMrBgQMHFBMT4+5mAAAAAPAQ+/btU7Vq1XLchpCVg7Jly0qyTmRYWJibWwMAAADAXRITExUTE+PICDkhZOXAPkQwLCyMkAUAAAAgT9OIKHwBAAAAAC5EyAIAAAAAFyJkAQAAAIALMScLAAAAXsUYo5SUFKWmprq7KShhfH195efnV+jbNxGyAAAA4DWSk5N18OBBnT171t1NQQkVEhKiKlWqKCAgoMD7IGQBAADAK6SlpWnXrl3y9fVVdHS0AgICCt3jANgZY5ScnKyjR49q165dqlevXq43Hc4OIQsAAABeITk5WWlpaYqJiVFISIi7m4MSKDg4WP7+/tqzZ4+Sk5MVFBRUoP1Q+AIAAABepaC9C0BeuOL64goFAAAAABciZAEAAACACxGyAAAAAC9Ts2ZNvfTSS3neftmyZbLZbEpISCiyNiEdIQsAAAAoIjabLcfH5MmTC7TfNWvWaMSIEXnevn379jp48KDCw8MLdLy8IsxZqC4IAAAAFJGDBw86Xn/66aeaOHGitm7d6lgWGhrqeG2MUWpqqvz8cv8VPTIyMl/tCAgIUFRUVL4+g4KjJwsAAABeyxjpzJnifxiTt/ZFRUU5HuHh4bLZbI73f/31l8qWLavvvvtOLVu2VGBgoH766Sft3LlTffr0UeXKlRUaGqrWrVtr8eLFTvu9dLigzWbTW2+9pX79+ikkJET16tXTV1995Vh/aQ/T7NmzFRERoQULFqhRo0YKDQ1Vt27dnEJhSkqKHnjgAUVERKhChQoaN26cBg8erL59+xb0x6WTJ09q0KBBKleunEJCQtS9e3dt377dsX7Pnj3q1auXypUrpzJlyqhx48aaP3++47MDBw5UZGSkgoODVa9ePc2aNavAbSlKhCwAAAB4rbNnpdDQ4n+cPeu67/Doo4/q6aef1pYtW9SsWTMlJSWpR48eWrJkidatW6du3bqpV69e2rt3b477mTJligYMGKA//vhDPXr00MCBA3XixIkczt1ZPffcc3r//fe1fPly7d27V2PHjnWsf+aZZ/Thhx9q1qxZWrlypRITEzV37txCfdchQ4bot99+01dffaVVq1bJGKMePXro4sWLkqSRI0fqwoULWr58uTZu3KhnnnnG0ds3YcIEbd68Wd999522bNmiV199VRUrVixUe4oKwwUBAAAAN5o6daquvfZax/vy5curefPmjvdPPPGE5syZo6+++kqjRo3Kdj9DhgzRrbfeKkl66qmn9PLLL2v16tXq1q1blttfvHhRr732murUqSNJGjVqlKZOnepYP2PGDD322GPq16+fJGnmzJmOXqWC2L59u7766iutXLlS7du3lyR9+OGHiomJ0dy5c3XTTTdp79696t+/v5o2bSpJql27tuPze/fu1WWXXaZWrVpJsnrzPBUhy0vs2yf9/rtUvrzUsaO7WwMAAOAZQkKkpCT3HNdV7KHBLikpSZMnT9a3336rgwcPKiUlRefOncu1J6tZs2aO12XKlFFYWJiOHDmS7fYhISGOgCVJVapUcWx/6tQpHT58WG3atHGs9/X1VcuWLZWWlpav72e3ZcsW+fn5qW3bto5lFSpUUIMGDbRlyxZJ0gMPPKB7771XCxcuVFxcnPr37+/4Xvfee6/69++vtWvX6rrrrlPfvn0dYc3TMFzQSyxdKvXrJz31lLtbAgAA4DlsNqlMmeJ/2Gyu+w5lypRxej927FjNmTNHTz31lFasWKH169eradOmSk5OznE//v7+l5wbW46BKKvtTV4nmxWRYcOG6e+//9Ydd9yhjRs3qlWrVpoxY4YkqXv37tqzZ48efvhhHThwQF26dHEa3uhJCFlewl545vRp97YDAAAARWvlypUaMmSI+vXrp6ZNmyoqKkq7d+8u1jaEh4ercuXKWrNmjWNZamqq1q5dW+B9NmrUSCkpKfr1118dy44fP66tW7cqNjbWsSwmJkb33HOPvvzyS40ZM0ZvvvmmY11kZKQGDx6sDz74QC+99JLeeOONArenKDFc0EvYQ5Y7usMBAABQfOrVq6cvv/xSvXr1ks1m04QJEwo8RK8w7r//fk2bNk1169ZVw4YNNWPGDJ08eVK2PHTjbdy4UWXLlnW8t9lsat68ufr06aPhw4fr9ddfV9myZfXoo4+qatWq6tOnjyTpoYceUvfu3VW/fn2dPHlSS5cuVaNGjSRJEydOVMuWLdW4cWNduHBB33zzjWOdpyFkeQn7NUrIAgAAKNleeOEF3XXXXWrfvr0qVqyocePGKTExsdjbMW7cOB06dEiDBg2Sr6+vRowYoa5du8rX1zfXz3a8pIiAr6+vUlJSNGvWLD344IO6/vrrlZycrI4dO2r+/PmOoYupqakaOXKk/vnnH4WFhalbt2568cUXJVn3+nrssce0e/duBQcHq0OHDvrkk09c/8VdwGbcPfDSgyUmJio8PFynTp1SWFiYW9uycaPUrJlUqZJ0+LBbmwIAAOAW58+f165du1SrVi0FBQW5uzmlTlpamho1aqQBAwboiSeecHdzikx211l+sgE9WV6COVkAAAAoTnv27NHChQvVqVMnXbhwQTNnztSuXbt02223ubtpHo/CF17CPlzw3DkpNdW9bQEAAEDJ5+Pjo9mzZ6t169a68sortXHjRi1evNhj50F5EnqyvIS9J0uSzpyR3Dx6EQAAACVcTEyMVq5c6e5meCV6srxEYKBkn2PIkEEAAADAcxGyvITNRhl3AAAAwBsQsrwIZdwBAAAAz0fI8iL0ZAEAAACej5DlRSjjDgAAAHg+QpYXYbggAAAA4PkIWV6E4YIAAAClU+fOnfXQQw853tesWVMvvfRSjp+x2WyaO3duoY/tqv2UJoQsL8JwQQAAAO/Sq1cvdevWLct1K1askM1m0x9//JHv/a5Zs0YjRowobPOcTJ48WS1atMi0/ODBg+revbtLj3Wp2bNnKyIiokiPUZwIWV6EniwAAADvMnToUC1atEj//PNPpnWzZs1Sq1at1KxZs3zvNzIyUiEhIa5oYq6ioqIUGBhYLMcqKQhZWYiPj1dsbKxat27t7qY4YU4WAADAJYyRzpwp/ocxeWre9ddfr8jISM2ePdtpeVJSkj7//HMNHTpUx48f16233qqqVasqJCRETZs21ccff5zjfi8dLrh9+3Z17NhRQUFBio2N1aJFizJ9Zty4capfv75CQkJUu3ZtTZgwQRcvXpRk9SRNmTJFGzZskM1mk81mc7T50uGCGzdu1DXXXKPg4GBVqFBBI0aMUFKGX1CHDBmivn376rnnnlOVKlVUoUIFjRw50nGsgti7d6/69Omj0NBQhYWFacCAATp8+LBj/YYNG3T11VerbNmyCgsLU8uWLfXbb79Jkvbs2aNevXqpXLlyKlOmjBo3bqz58+cXuC154Veke/dSI0eO1MiRI5WYmKjw8HB3N8eBniwAAIBLnD2b/ktScUpKksqUyXUzPz8/DRo0SLNnz9b48eNls9kkSZ9//rlSU1N16623KikpSS1bttS4ceMUFhamb7/9VnfccYfq1KmjNm3a5HqMtLQ03XDDDapcubJ+/fVXnTp1ymn+ll3ZsmU1e/ZsRUdHa+PGjRo+fLjKli2rf/3rX7r55pu1adMmff/991q8eLEkZfl78JkzZ9S1a1e1a9dOa9as0ZEjRzRs2DCNGjXKKUguXbpUVapU0dKlS7Vjxw7dfPPNatGihYYPH57r98nq+9kD1o8//qiUlBSNHDlSN998s5YtWyZJGjhwoC677DK9+uqr8vX11fr16+Xv7y/J+t0+OTlZy5cvV5kyZbR582aFFvE1Q8jyIszJAgAA8D533XWXnn32Wf3444/q3LmzJGuoYP/+/RUeHq7w8HCNHTvWsf3999+vBQsW6LPPPstTyFq8eLH++usvLViwQNHR0ZKkp556KtM8qn//+9+O1zVr1tTYsWP1ySef6F//+peCg4MVGhoqPz8/RUVFZXusjz76SOfPn9d7772nMv8LmTNnzlSvXr30zDPPqHLlypKkcuXKaebMmfL19VXDhg3Vs2dPLVmypEAha8mSJdq4caN27dqlmJgYSdJ7772nxo0ba82aNWrdurX27t2rRx55RA0bNpQk1atXz/H5vXv3qn///mratKkkqXbt2vluQ34RsrwIwwUBAAAuERLinl+O8jEfqmHDhmrfvr3eeecdde7cWTt27NCKFSs0depUSVJqaqqeeuopffbZZ9q/f7+Sk5N14cKFPM+52rJli2JiYhwBS5LatWuXabtPP/1UL7/8snbu3KmkpCSlpKQoLCwsz9/DfqzmzZs7ApYkXXnllUpLS9PWrVsdIatx48by9fV1bFOlShVt3LgxX8fKeMyYmBhHwJKk2NhYRUREaMuWLWrdurVGjx6tYcOG6f3331dcXJxuuukm1alTR5L0wAMP6N5779XChQsVFxen/v37F2geXH4wJ8uLMFwQAADgEjabNWyvuB//G/aXV0OHDtV///tfnT59WrNmzVKdOnXUqVMnSdKzzz6r//u//9O4ceO0dOlSrV+/Xl27dlVycrLLTtOqVas0cOBA9ejRQ998843WrVun8ePHu/QYGdmH6tnZbDalpaUVybEkqzLin3/+qZ49e+qHH35QbGys5syZI0kaNmyY/v77b91xxx3auHGjWrVqpRkzZhRZWyRClldhuCAAAIB3GjBggHx8fPTRRx/pvffe01133eWYn7Vy5Ur16dNHt99+u5o3b67atWtr27Zted53o0aNtG/fPh08eNCx7JdffnHa5ueff1aNGjU0fvx4tWrVSvXq1dOePXuctgkICFBqamqux9qwYYPOnDnjWLZy5Ur5+PioQYMGeW5zfti/3759+xzLNm/erISEBMXGxjqW1a9fXw8//LAWLlyoG264QbNmzXKsi4mJ0T333KMvv/xSY8aM0ZtvvlkkbbUjZHkRerIAAAC8U2hoqG6++WY99thjOnjwoIYMGeJYV69ePS1atEg///yztmzZorvvvtupcl5u4uLiVL9+fQ0ePFgbNmzQihUrNH78eKdt6tWrp7179+qTTz7Rzp079fLLLzt6euxq1qypXbt2af369Tp27JguXLiQ6VgDBw5UUFCQBg8erE2bNmnp0qW6//77dccddziGChZUamqq1q9f7/TYsmWL4uLi1LRpUw0cOFBr167V6tWrNWjQIHXq1EmtWrXSuXPnNGrUKC1btkx79uzRypUrtWbNGjVq1EiS9NBDD2nBggXatWuX1q5dq6VLlzrWFRVClhdhThYAAID3Gjp0qE6ePKmuXbs6zZ/697//rcsvv1xdu3ZV586dFRUVpb59++Z5vz4+PpozZ47OnTunNm3aaNiwYfrPf/7jtE3v3r318MMPa9SoUWrRooV+/vlnTZgwwWmb/v37q1u3brr66qsVGRmZZRn5kJAQLViwQCdOnFDr1q114403qkuXLpo5c2b+TkYWkpKSdNlllzk9evXqJZvNpnnz5qlcuXLq2LGj4uLiVLt2bX366aeSJF9fXx0/flyDBg1S/fr1NWDAAHXv3l1TpkyRZIW3kSNHqlGjRurWrZvq16+vV155pdDtzYnNmDwW+S+F7CXcT506le9JgUXhzz+lJk2kihWlo0fd3RoAAIDidf78ee3atUu1atVSUFCQu5uDEiq76yw/2YCeLC/CnCwAAADA8xGyvIh9uOCFC1IhbpgNAAAAoAgRsrxIxhtTZyjoAgAAAMCDELK8SECAZL/lAEMGAQAAAM9EyPIylHEHAAClHXXbUJRccX0RsrwMZdwBAEBp5f+/IT1nz551c0tQktmvL/v1VhB+rmoMigc9WQAAoLTy9fVVRESEjhw5Ism6Z5PNZnNzq1BSGGN09uxZHTlyRBEREfL19S3wvghZXoYy7gAAoDSLioqSJEfQAlwtIiLCcZ0VFCHLyzBcEAAAlGY2m01VqlRRpUqVdJF72sDF/P39C9WDZUfI8jIMFwQAALCGDrril2GgKFD4wsswXBAAAADwbIQsL0NPFgAAAODZCFlehjlZAAAAgGcjZHkZerIAAAAAz0bI8jLMyQIAAAA8GyHLyzBcEAAAAPBshCwvw3BBAAAAwLMRsrwMwwUBAAAAz0bI8jL0ZAEAAACejZDlZZiTBQAAAHg2QpaXoScLAAAA8GyELC/DnCwAAADAsxGyvIx9uODFi1JysnvbAgAAACAzQpaXKVMm/TVDBgEAAADPQ8jyMv7+UmCg9ZohgwAAAIDnIWR5IYpfAAAAAJ6LkOWFKOMOAAAAeC5ClheiJwsAAADwXIQsL0QZdwAAAMBzEbK8EMMFAQAAAM9FyPJCDBcEAAAAPBchywsxXBAAAADwXIQsL0RPFgAAAOC5CFleiDlZAAAAgOciZHkherIAAAAAz0XI8kLMyQIAAAA8FyHLCzFcEAAAAPBchCwvxHBBAAAAwHMRsrwQwwUBAAAAz0XI8kL0ZAEAAACei5DlhZiTBQAAAHguQpYXoicLAAAA8FyELC+UcU6WMe5tCwAAAABnhCwvZB8umJoqXbjg3rYAAAAAcEbI8kJlyqS/ZsggAAAA4FkIWV7I11cKDrZeU8YdAAAA8CyELC9F8QsAAADAMxGyvBRl3AEAAADPRMjyUhkrDAIAAADwHIQsL8VwQQAAAMAzEbK8FMMFAQAAAM9EyPJS9GQBAAAAnomQ5aWYkwUAAAB4JkKWl7KHrDNn3NsOAAAAAM4IWV4qJMR6JmQBAAAAnoWQ5aXKlLGeCVkAAACAZyFkeSl7yDp71r3tAAAAAOCMkOWlGC4IAAAAeCZClpdiuCAAAADgmQhZXoqQBQAAAHgmQpaXYk4WAAAA4JkIWV6KOVkAAACAZyJkeSmGCwIAAACeiZDlpRguCAAAAHgmQpaXYrggAAAA4JkIWV7K3pN14YKUmuretgAAAABIR8jyUvaQJdGbBQAAAHgSQpaXCgqSbDbrNfOyAAAAAM9R4kPWN998owYNGqhevXp666233N0cl7HZmJcFAAAAeCI/dzegKKWkpGj06NFaunSpwsPD1bJlS/Xr108VKlRwd9NcokwZK2ARsgAAAADPUaJ7slavXq3GjRuratWqCg0NVffu3bVw4UJ3N8tlKOMOAAAAeB6PDlnLly9Xr169FB0dLZvNprlz52baJj4+XjVr1lRQUJDatm2r1atXO9YdOHBAVatWdbyvWrWq9u/fXxxNLxbckBgAAADwPB4dss6cOaPmzZsrPj4+y/WffvqpRo8erUmTJmnt2rVq3ry5unbtqiNHjhRzS92DOVkAAACA5/HokNW9e3c9+eST6tevX5brX3jhBQ0fPlx33nmnYmNj9dprrykkJETvvPOOJCk6Otqp52r//v2Kjo7O9ngXLlxQYmKi08OT0ZMFAAAAeB6PDlk5SU5O1u+//664uDjHMh8fH8XFxWnVqlWSpDZt2mjTpk3av3+/kpKS9N1336lr167Z7nPatGkKDw93PGJiYor8exQGc7IAAAAAz+O1IevYsWNKTU1V5cqVnZZXrlxZhw4dkiT5+fnp+eef19VXX60WLVpozJgxOVYWfOyxx3Tq1CnHY9++fUX6HQqL4YIAAACA5ynRJdwlqXfv3urdu3eetg0MDFRgYGARt8h1GC4IAAAAeB6v7cmqWLGifH19dfjwYaflhw8fVlRUlJtaVbwIWQAAAIDn8dqQFRAQoJYtW2rJkiWOZWlpaVqyZInatWvnxpYVH+ZkAQAAAJ7Ho4cLJiUlaceOHY73u3bt0vr161W+fHlVr15do0eP1uDBg9WqVSu1adNGL730ks6cOaM777zTja0uPszJAgAAADyPR4es3377TVdffbXj/ejRoyVJgwcP1uzZs3XzzTfr6NGjmjhxog4dOqQWLVro+++/z1QMo6RiuCAAAADgeTw6ZHXu3FnGmBy3GTVqlEaNGlVMLfIsDBcEAAAAPI/XzskCPVkAAACAJyJkeTHmZAEAAACeh5DlxejJAgAAADwPIcuLMScLAAAA8DyELC/GcEEAAADA8xCyshAfH6/Y2Fi1bt3a3U3JEcMFAQAAAM9jM7nVSC/FEhMTFR4erlOnTiksLMzdzclk3z6penUpIEC6cMHdrQEAAABKrvxkA3qyvJi9Jys5WUpJcW9bAAAAAFgIWV7MPidLYsggAAAA4CkIWV4sMFDy+d9PkJAFAAAAeAZClhez2SjjDgAAAHgaQpaXo4w7AAAA4FkIWV6OMu4AAACAZyFkeTlCFgAAAOBZCFlejjlZAAAAgGchZHk55mQBAAAAnoWQ5eUYLggAAAB4FkKWl2O4IAAAAOBZCFlejp4sAAAAwLMQsrIQHx+v2NhYtW7d2t1NyRVzsgAAAADPQsjKwsiRI7V582atWbPG3U3JFT1ZAAAAgGchZHk55mQBAAAAnoWQ5eUYLggAAAB4FkKWl2O4IAAAAOBZCFlejuGCAAAAgGchZHk5erIAAAAAz0LI8nLMyQIAAAA8CyHLy9GTBQAAAHgWQpaXY04WAAAA4FkIWV6OniwAAADAsxCyvBxzsgAAAADPQsjycvaerJQU6eJF97YFAAAAACHL69lDlkRvFgAAAOAJCFlezt9f8vW1XhOyAAAAAPcjZHk5m43iFwAAAIAnIWRlIT4+XrGxsWrdurW7m5InlHEHAAAAPAchKwsjR47U5s2btWbNGnc3JU/oyQIAAAA8ByGrBKCMOwAAAOA5CFklAD1ZAAAAgOcgZJUAzMkCAAAAPAchqwRguCAAAADgOQhZJQDDBQEAAADPQcgqARguCAAAAHgOQlYJQE8WAAAA4DkIWSUAc7IAAAAAz0HIKgHoyQIAAAA8ByGrBGBOFgAAAOA5CFklAD1ZAAAAgOcgZJUAzMkCAAAAPAchqwRguCAAAADgOQhZJQDDBQEAAADPQcgqARguCAAAAHgOQlYJQE8WAAAA4DkIWVmIj49XbGysWrdu7e6m5AlzsgAAAADPYTPGGHc3wlMlJiYqPDxcp06dUlhYmLubk60DB6SqVSVfX+niRclmc3eLAAAAgJIlP9mAnqwSwD4nKzVVSk52b1sAAACA0o6QVQLYhwtKzMsCAAAA3I2QVQL4+1sPiXlZAAAAgLsRskoIKgwCAAAAnoGQVUJwrywAAADAMxCySojQUOv59Gn3tgMAAAAo7QhZJURUlPV86JB72wEAAACUdoSsEqJaNev5n3/c2w4AAACgtCNklRCELAAAAMAzELJKCHvI2rfPve0AAAAASjtCVglBTxYAAADgGQhZJQQhCwAAAPAMhKwSwh6yDh6UUlLc2xYAAACgNCNklRCVKkl+flJaGmXcAQAAAHciZJUQvr5SdLT1miGDAAAAgPsQskoQ5mUBAAAA7kfIKkEIWQAAAID7EbJKEEIWAAAA4H6ErBKEkAUAAAC4HyGrBImJsZ4JWQAAAID7ELKyEB8fr9jYWLVu3drdTckXerIAAAAA97MZY4y7G+GpEhMTFR4erlOnTiksLMzdzcnVP/9YvVl+ftKFC5IPERoAAABwifxkA34NL0GioqxglZIiHTni7tYAAAAApRMhqwTx85OqVLFeM2QQAAAAcA9CVgnDvCwAAADAvQhZJQwhCwAAAHAvQlYJQ8gCAAAA3IuQVcLYQ9a+fe5tBwAAAFBaEbJKGHf1ZP39t3T+fPEeEwAAAPBEhKwSxh0ha+FCqU4daezY4jsmAAAA4KkIWSVMxpBVXLeZfucd63nVquI5HgAAAODJCFklTHS09ZycLB07VvTHO39e+vZb6/X+/UV/PAAAAMDTEbJKmIAAqXJl63VxDBlctEhKSrJeHz5shTsAAACgNCNklUDFOS/rv/91fn/gQNEfEwAAAPBkhKwSqLhC1sWL0ldfWa99/nclMWQQAAAApR0hqwQqrpC1bJl08qRUqZJ0xRXFc0wAAADA0xUoZO3bt0//ZPhtevXq1XrooYf0xhtvuKxhKLjiCln2oYJ9+0o1aliv6ckCAABAaVegkHXbbbdp6dKlkqRDhw7p2muv1erVqzV+/HhNnTrVpQ1E/sXEWM9FGbJSU6U5c6zX/ftLVasW/TEBAAAAb1CgkLVp0ya1adNGkvTZZ5+pSZMm+vnnn/Xhhx9q9uzZrmwfCqA4erJ+/lk6ckSKiJA6d3bPTZABAAAAT1SgkHXx4kUFBgZKkhYvXqzevXtLkho2bKiDBw+6rnUoEHvg2bev6G5IbB8q2Lu3VTbefkyGCwIAAKC0K1DIaty4sV577TWtWLFCixYtUrdu3SRJBw4cUIUKFVzaQORfVJT1fO6cdOaM6/dvjPTll9br/v2tZ4YLAgAAAJYChaxnnnlGr7/+ujp37qxbb71VzZs3lyR99dVXjmGEcJ+QECkoyHp97Jjr979vn/Xw85OuvdZaZu/JOnBASktz/TEBAAAAb+FXkA917txZx44dU2JiosqVK+dYPmLECIWEhLiscSgYm02qWNHqVTp2TKpZ07X7T062noODrYdk9Z75+EgpKdZcLXtvGgAAAFDaFKgn69y5c7pw4YIjYO3Zs0cvvfSStm7dqkqVKrm0gSiYihWt56LoyUpNtZ59fdOX+fmlByuGDAIAAKA0K1DI6tOnj9577z1JUkJCgtq2bavnn39effv21auvvurSBqJgiiNk+Vxy9VD8AgAAAChgyFq7dq06dOggSfriiy9UuXJl7dmzR++9955efvlllzYQBVPcPVkSxS8AAAAAqYAh6+zZsypbtqwkaeHChbrhhhvk4+OjK664Qnv27HFpA90hPj5esbGxat26tbubUmDuCFncKwsAAAAoYMiqW7eu5s6dq3379mnBggW67rrrJElHjhxRWFiYSxvoDiNHjtTmzZu1Zs0adzelwOwh6+hR1+/bXj0wu5DFcEEAAACUZgUKWRMnTtTYsWNVs2ZNtWnTRu3atZNk9WpddtllLm0gCiYy0npmuCAAAABQvApUwv3GG2/UVVddpYMHDzrukSVJXbp0Ub9+/VzWOBQcwwUBAAAA9yhQyJKkqKgoRUVF6Z///UZdrVo1bkTsQdxZ+GL/fskY635dAAAAQGlToOGCaWlpmjp1qsLDw1WjRg3VqFFDEREReuKJJ5Rmn7ADt3JnyDp7VkpIcP1xAQAAAG9QoJ6s8ePH6+2339bTTz+tK6+8UpL0008/afLkyTp//rz+85//uLSRyD97yDp+3CpUcek9rQoju/tkBQdLFSpYx9y/X/rfvaoBAACAUqVAIevdd9/VW2+9pd69ezuWNWvWTFWrVtV9991HyPIAFSpYz6mp0qlTrg082fVkSVZv1vHj1rysJk1cd0wAAADAWxSof+PEiRNq2LBhpuUNGzbUiRMnCt0oFF5goPS/W5m5fMhgdiXcJYpfAAAAAAUKWc2bN9fMmTMzLZ85c6aaNWtW6EbBNYrqXlk59WRxrywAAACUdgUaLjh9+nT17NlTixcvdtwja9WqVdq3b5/mz5/v0gai4CIjpV27XN+TldtwQYmeLAAAAJReBerJ6tSpk7Zt26Z+/fopISFBCQkJuuGGG/Tnn3/q/fffd3UbUUBFVWEwLz1ZhCwAAACUVgW+T1Z0dHSmAhcbNmzQ22+/rTfeeKPQDUPhuSNkZbxXFgAAAFAaubCwNzwNPVkAAABA8SNklWBFHbKyuveWPWSdPGndlBgAAAAobQhZJVhRhaycSriHhUllylivGTIIAACA0ihfc7JuuOGGHNcnJCQUpi1wMXcMF7TZrN6srVutIYP16rn22AAAAICny1fICg8Pz3X9oEGDCtUguI477pMlpYcserIAAABQGuUrZM2aNauo2oEi4I6eLIl7ZQEAAKB0Y05WCRYZaT0nJEgXL7puv7mFrEqVrGdXhzsAAADAGxCySrBy5aw5UpJ04oTr9ptbyCqqYYoAAACANyBklWC+vlL58tZrV/Yq5VTCXUrvQaMnCwAAAKURIauEK4p5WfRkAQAAANkjZJVwRRGycrpPlpTek0XIAgAAQGlEyCrhiqJXKbeeLIYLAgAAoDQjZJVw7hwumJQknT/vuuMCAAAA3oCQVcK5I2SFh0t+fq4/LgAAAOANCFklnDtCls1G8QsAAACUXoSsEq4o5kflFrKK6rgAAACANyBklXAF6clKTpYuXsx+fW73ycp4XHqyAAAAUNoQskq4/Iasixel2FipZUvJmKy3ya2Eu0QZdwAAAJRefu5uAIpWfkPWjh3Szp3W64QEqVy5zNswXBAAAADIHj1ZJZw9ZJ05I507l/v227env84uIOUlZDFcEAAAAKUVIauECwvLXE7dGGn37qyHA7oqZLmyJyu7YYsAAACAJyJklXAZy6nbA8/EiVKtWtJ772Xeftu29Nee0JM1ZozVVoYdAgAAwFsQsrIQHx+v2NhYtW7d2t1NcYmMIWv3bmn6dOv94sWZt/W0nqzPP5f27JF++61w+wEAAACKCyErCyNHjtTmzZu1Zs0adzfFJTIGnn//2yrRLkmbNmXe1pPmZBkjHT5svT55suD7AQAAAIoTIasUsAeehQulDz9MX75li5SSkv7+7Fnpn3/S3+cWsnK6T5Y92B0/nl7yPb8SEtIDISELAAAA3oKQVQrYQ9bs2dbzbbdJISHShQvp5dol59dS9iErL/fJsh8zLa3gAcneiyURsgAAAOA9CFmlgD3wSFJAgPTUU1Ljxtb7jEMGMxa9kAo3XNDfXwoPt14XdMjgoUPprwlZAAAA8BaErFIgY8h64AGpRo2sQ5Z9PlZoqPVcmJAlFb74BT1ZAAAA8EaErFKgcmXruVw56fHHrddNmljPf/6Zvp09ZLVtaz0XNmQVtvgFPVkAAADwRoSsUqBXL2n4cOmTT6ygJaWHrKx6stq1s57pyQIAAADyz8/dDUDRCwmR3njDeZk9ZG3bZhXACAzMHLJOnrSqD/pdcpW4oycrIaFg+wAAAACKGz1ZpVR0tBQRYQWmrVulxMT0UGMfLmhM1j1IeSnhLtGTBQAAgNKJkFVK2WzOxS927LBeR0ZKFSqkDyvMKiDlpYS7fV8Sc7IAAABQuhCySrGM87LsQwXr17ee7UP9sgpZxTVcMGNPVmJi+nEBAAAAT0bIKsUyVhi0h6x69aznnIb6paZKQTonX1tajvsvzHDBtDTnkCUxLwsAAADegZBVimXVk2UPWTn1ZAWeP6WFuk5XfPygNXErG4XpybIX3ZCkoKD0ZQAAAICnI2SVYvY5WX//La1bZ73OS8hqfHy5OugnNVw8Uxo/Ptv9F6Ynyz4fq3z59P0QsgAAAOANCFmlWGSkVKmS9XrjRus5LyFrZfleulevWG+mTbMeWbDv4+xZ65Ef9qGCUVHpRTgIWQAAAPAGhKxSzj5k0K5uXes5t8IXr+le/XH7dGvB449LM2dm2q5sWSkgIPv95MTek1W5MiELAAAA3oWQVcplDFnR0VJoqPU6p5BlL+G+o98j0oQJ1pv775dmz3bazmYreBl3erIAAADgrQhZpVzGkGUfKijlo4T7lCnSQw9ZC4YOlT7/3GnbnPaTk4w9WRER1mtCFgAAALwBIauUsxe/kLIOWVn1QDmFLJtNeuEFK2ClpUm33SbNn+/Ylp4sAAAAlDaErFIut5CVp5sR22zS669Lt9xi1V3v319atsxpP/kNWczJAgAAgLciZJVy4eFSTIz1OquQdfq0dOGC82cyhSz7m/fek3r1ks6ft55//bXAZdyz6sniZsQAAADwBoQsaPJkqU8f6brr0peFh6eHqOPHnbfPMmRJkr+/9NlnUpcuUlKS1L27YlP+kERPFgAAAEoPQhZ0113S3LlSmTLpy3x8pAoVrNeX9kJlG7IkKSjI2lm7dtLJkxr0wbWqr6356slKTU0PZczJAgAAgLchZCFb2c3LyjFkSVYd+PnzpRYtFHL6iBYrTj779uT5uMePW8ewl4AnZAEAAMCbELKQrexClv0+WT45XT0REdLChTpTvaFi9I+e39BFOngwT8e1z8eqWFHy8yNkAQAAwLsQspCtAvdk2UVG6p9Zi/W3aqnGxZ3StdfmqQJGxvlYUnrIOnUqPeABAAAAnoqQhWwVOmRJimhcVXFarP2Klv78U+rWzUpLOchYWVBKD1nG5PpRAAAAwO0IWciWK0JWhQrSLtVWnBYrrUJF6fffpeuvl86cyfYz9pBl78kKCJBCQqzXDBkEAACApyNkIVuuCFn2OVV/qZF2v77Qqg3/00/SDTdkvgHX/9iHC9p7siTmZQEAAMB7ELKQLVeErIz7OVD5MqvqYJky0sKF0q23Sikpmba/tCdLImQBAADAexCykC1XhayyZa3n06cltW8vzZtnjQGcM0e6885M1Syy6smKiLCeCVkAAADwdIQsZCu3Eu55DVmhodbz6dP/W9Cli/T559YOPvhAGjnSqmrxP/RkAQAAwJsRspCt3HqycrxPVgZOPVl2vXtL779v3XH4tdekf/3LEbSYkwUAAABvRshCtuwh69w56ezZ9OUFHS6YlHTJiltvld54w3r93HPSk08qNTU91GXVk5WQkNfWAwAAAO7h5+4GwHOFhlpTp5KTreBTvbq1vFBzsi41bJiVvh5+WJo4UWdMWaWlPSQfn/SQJ9GTBQAAAO9BTxayZbNlPWQwvyEr05ysSz30kDRliiQpbNLDGqq3FBnpvH9CFgAAALwFIQs5ckXIyrEny27CBGnsWEnSGxqhIYEfO60mZAEAAMBbELKQo0tDljHphQALPScrI5tNmj5dW6++Rz4y+s++O6SvvnKsJmQBAADAWxCykKNLQ1bGW1q5tCdLkmw2zbsuXu/rdvmaVOmmm6TFiyURsgAAAOA9CFnI0aUhyz5UUMp7Cfdc52T9z9q10mdf+OhOzdKmun2tiht9+kg//0zIAgAAgNcgZCFHOYUsV/VkbdlidVq1bCn9/rtk8/PTwRc/ka67zqod36OHIvetlWSVcM/YmwYAAAB4GkIWclTUIWvtWqlZM+mLL6xpWQMHWqHr2usDpTlzpKuukk6dUsXbu6qRNistLQ/DDgEAAAA3ImQhR/aQdfSo9VyYkJVV4Yt166SUFKl+femPP6QPPpDq1v3fypAQ6ZtvpJYtZTt2TIt1rWrpb4YMAgAAwKMRspCjyEjruTAhK6c5WadOWc+tWklNmmTx4fBw6fvvpdhYReuAFitOSVv35+3AAAAAgBsQspCjSpWs5yNHrOfCDhe0l3+3s4esiIgcdlCxorR4sfb611Ft7VKtEXHpDQIAAAA8DCELObKHrGPHrIBlD1k2m/XIC3vISk2Vzp93XpeQYD2Hh+eykypV9FDTJdqnaiqz9y+pa9f0DwMAAAAehJCFHNnnZBkjHT+eXtkvr71YUvpwQSnzvCx7T1auIUtScpUa6qIlOle2krR+vdS9ey53OAYAAACKHyErC/Hx8YqNjVXr1q3d3RS38/OTKlSwXh89mt6Tldd7ZNm3LVPGen3pvKw8DRf8n3LlpO2qr8+GLbQ+8Msv1n20Lu0eAwAAANyIkJWFkSNHavPmzVqzZo27m+IRMs7Lsoes/PRkSdkXv8jzcEHJcUPibcHNrWIYoaHSDz9IAwZIFy/mr0EAAABAESFkIVeuCFnZ3SsrP8MF7SHr5ElJbdtKX38tBQVZz7ffLl24kL9GAQAAAEWAkIVcuTJkZTcnK6/DBSWl3yerc2fpv/+1xjR+9pl09dXSgQP5axgAAADgYoQs5Kooe7IKMlzQ6WbEPXpYPVnh4dKqVVLLltLKlflrHAAAAOBChCzkqqjmZBlTiOGCGXXrJv32m3U340OHrB6uV17JfFMuAAAAoBgQspCrjCGrICXcpax7ss6eTQ9tBRoumFHdulZP1oABUkqKNHKkdNddVB4EAABAsSNkIVdFNVzQPlTQ11cKCcl9HzmGLMnqLvvkE2n6dKtu/OzZUocO0t69+WssAAAAUAiELOQqq5CVn/tkSVkXvsg4VNBmy30fGUNWtiMBbTbpkUekBQuk8uWtYYQtW0pLl+avwQAAAEABEbKQq6Kak5WfyoKSlZkkqw2XVinMJC5O+v13qUUL6dgx6dprpRdfZJ4WAAAAihwhC7mKjLSeExOteVSSa4cL5qXohSQFB1u3xZKk48fz8IGaNa1Kg7ffbiWz0aOt1/YvAQAAABQBQhZyFRFh3YpKkg4etJ5dEbLyU1nQzt6bdeJEHj8QEiK99570f/9nNfqjj6T27aVdu/J+UAAAACAfCFnIlc2WPmSwsCErqzlZeR0uKBUgZEnWF3jgAWnJEuuLbNggtWolLVyYj50AAAAAeUPIQp7YQ9ahQ9azK+Zk5Xe4oCRVqGA952m44KU6dbLmabVpY6W07t2lZ55hnhYAAABcipCFPHFVT1axDxe8VLVq0o8/SkOHWjf9evRR695aGRsGAAAAFAIhC3lSlCGryIcLXiooSHrzTem11yR/f+mLL6QrrpC2by/ETgEAAAALIQt5cmnIcsV9sgozXLBQIUuy5mndfbfVq1WlirR5s9S6tfTNN4XcMQAAAEo7QhbypLA9WRnnZNmnQBVmuGCB5mRlpV07a55W+/ZWg3r1kqZOtYYSAgAAAAVAyEKe2EOWvfepoMMFU1KkCxes126Zk5WVKlWkpUul++6z3k+aJPXrl95AAAAAIB8IWcgTe8iyK2hPlpQ+L8se2Ip9TlZWAgKk+Hhp1iwpMFD66iurCuGWLS4+EAAAAEo6QhbypLAhy9fXui+wlB6yCtKTVagS7nkxZIj0009STIy0bZsVtL78sogOBgAAgJKIkIU8KWzIktJ7s+zFLzxmuOClWrWSfvtN6tzZamz//tL48VJqahEeFAAAACUFIQt5Ehnp/L4gIStjGffUVCkx0Xpf0OGCRXoP4UqVpEWLpIcftt4/9ZTUs2cRpzsAAACUBIQs5ElIiPO8qvyWcJecQ1bG+2UVZLhgSopzOfgi4ecnvfCC9OGHUnCwtGCBVeb9jz+K+MAAAADwZoQs5FnGIYOF7cmyDxUMDLQeeRUcbN1LWCrCeVmXuu026eefpZo1pb//tsq+f/JJMR0cAAAA3oaQhTxzVchKSipYZUG7YpmXdakWLax5WtddJ509K916qzR2rNWlBgAAAGRAyEKeFTZkZbwhcUGKXti5JWRJ1ljF+fOlRx+13j//vNS1q3TsWDE3BAAAAJ6MkIU8K4rhggUJWUVexj0nvr7StGnS559LZcpIP/wgtWwprV3rhsYAAADAExGykGeuDFleN1zwUjfeKP36q1SvnrR3r3TlldJ777mxQQAAAPAUhCzkmaf0ZHlEyJKkxo2l1aut0u7nz0uDB0v33y9dvOjmhgEAAMCdCFnIM1fNyUpKcs1wQbeHLMnqivvqK2niROv9zJnSNddIP/5YxDfyAgAAgKciZCHPMoaswt4nyxXDBd0yJysrPj7SlCnSvHnWl/zpJ6lzZ6l+fesmxvv3u7uFAAAAKEaELOQZwwVz0bu3VQBj2DCr227HDmn8eKl6dWtI4ZdfSsnJ7m4lAAAAihghC3lGyMqDunWlN9+UDh2SZs2SOnSQ0tKs0u/9+0vVqkljxkibN7u7pQAAACgihCzkWYUKks1mvS7snKzCDBd0awn3vCpTRhoyRFq+XNq61bq3VpUq0tGj0gsvWEUzrrjCCmSJie5uLQAAAFyIkIU88/NLDzj0ZOVD/frWvbX27pW+/lrq29c6mb/+Ko0YYYUveyCjWAYAAIDXI2QhX+xDBj0lZHlVJvHzk66/XpozR/rnH+nZZ6WGDaWzZ6V335U6dZIaNJCeflo6cMDdrQUAAEABEbKQL64KWa4YLpiSYg099EqVK0tjx1pzs37+WRo61BpPuX279NhjUkyM1KuXFci47xYAAIBXIWQhXwoTsuxzslJSpGPHrNcF6ckKDpaCgqzXHj0vKy9sNqldO+mtt6SDB6V33pGuusoqlvHNN9INN1jFMsaOlbZscXdrAQAAkAeELORLVJT1HBCQ/8/aQ5ZkBS2pYCFL8sJ5WXkRGirdeae0YoX011/SuHHWCT9yRHr+eSk2Nj2QnT7t7tYCAAAgG4Qs5Mvw4dKAAdLtt+f/s35+Vi9URmFhBWtHiQxZGdnnZu3da93kuE8fq/vwl1+sH0JUlBXIfvrJyyamAQAAlHyELORLkybSp59KjRoV7PP2eVmS1XHj51ew/XhFGXdX8Pe3bnI8d65VLGP6dCuAnT0rzZ5t3YerYUPpmWes4YYAAABwO0IWilXGIYMFHSoolYKerKxERUmPPGLNzVq5UrrrLut+XNu2WffhiolJD2QUywAAAHAbQhaKVcaerIJUFrQrlSHLzmaT2reX3n5bOnTIer7ySik11boPV79+VrGMRx6x5nYBAACgWBGyUKwyhqzC9GTZhwuWypCVUWio1aP1009WD9cjj1jl4Y8ckZ57zhrXecMN0t9/u7ulAAAApQYhC8XKVSHL3pNV4udk5UfDhtacrX37rCGDvXtLPj7WvbYaNZIef5yqhAAAAMWAkIVixXDBYuDvb1UjnDdP+uMPKS5OSk6Wpk2T6teX3n3Xug8XAAAAigQhC8WKwhfFrHFjaeFCK3DVqWPN4RoyRLriCmnVKne3DgAAoEQiZKFYuXpOFsMF88Bms4YO/vmnNZywbFlpzRqreMbtt1ul4QEAAOAyhCwUK4YLulFgoFUYY9s2q1iGzSZ9+KF1360nn5TOnXN3CwEAAEoEQhaKlasLX5w4IRlTuDaVOlFRVtn3NWus0u9nz0oTJljFMT7/nBMKAABQSIQsFCtXzcmyDxdMSZGSkgrXplKrZUtpxQrp44+t+2rt2SMNGCB17iytX+/u1gEAAHgtQhaKlauGCwYHS0FB1mvmZRWCzSbdcou0das0ebJ1Ypcvly6/XBoxwrrfFgAAAPKFkIVi5arhghLzslwqJESaNEn66y8rdBkjvfmmVK+e9MILVgl4AAAA5AkhC8WKkOXhqle3hg+uWGH1ZiUmSmPGSE2bSt9+y3wtAACAPCBkoVhlnJNVmOGCEmXci9RVV1mFMd5+W6pc2apIeP31Uo8e0pYt7m4dAACARyNkoVjRk+VFfHysUu/btkn/+pfk7y99/73UrJn00EPSyZPubiEAAIBHImShWNmDUWCgc69WYfZFyCpiYWHSM89ImzdLffpYJR3/7/+s+VqvvWa9BwAAgAMhC8UqKkp67jnpjTeswnaFYR8uSMgqJnXrSnPnSgsXSrGx1jjNe++15m4tXeru1gEAAHgMQhaK3Zgx0qBBhd+PvSeLOVnF7NprpQ0bpBkzpHLlpI0bpWuukfr3l/7+292tAwAAcDtCFrwWwwXdyM9PGjVK2r7devb1lb780urhevxx6fRpd7cQAADAbQhZ8FqELA9QoYLVo7VhgxQXJ124IE2bJjVoIL33npSW5u4WAgAAFDtCFrxWs2bSE09YHSlws8aNrbla8+ZJdepIBw9KgwdL7dpJv/zi7tYBAAAUK5sx3F00O4mJiQoPD9epU6cUFhbm7uYA3uHCBenll60EbB82ePvt0tNPS1WrurdtAAAABZSfbEDIygEhCyiEQ4ek8eOlWbMkY6SQEKl3b6lWLalmzfRH9epSUJCbGwsAAJAzQpaLELIAF/j9d+nBB6WVK7PfpkoV5+BFCAMAAB6GkOUihCzARYyRFi+W/vhD2r07/bFrl3TmTO6fJ4QBAAA3I2S5CCELKGLGWOUh7YErYwCzPwhhAADAAxCyXISQBbiZMdbdprMKX/kJYdHRmcNXmzZS8+ZF1HAAAFDSELJchJAFeLjcQtiuXdLZs9l/vnt3adIkqW3b4mgtAADwYoQsFyFkAV4uuxC2fbu0ZImUmmpt162bFbauuMJ9bQUAAB6NkOUihCygBNuxQ/rPf6T3308PW127WmGrXTv3tg0AAHic/GQDn2JqEwB4lrp1rXt4bd0q3Xmn5OsrLVggtW8vXXddziXnAQAAckDIAlC61akjvfOOtG2bNHSo5OcnLVokXXWVdO210k8/ubuFAADAyxCyAECSateW3nrLClvDhllha/FiqUMHKS5OWrHC3S0EAABegpAFABnVqiW9+aZVHGP4cCtsLVkidewoXXONtHy5u1sIAAA8HCELALJSs6b0xhtWgYy775b8/aWlS6VOnaSrr5aWLXN3CwEAgIcqFSGrX79+KleunG688UZ3NwWAt6lRQ3rtNSts3XOPFbaWLbOCVufOVvCiSCsAAMigVISsBx98UO+99567mwHAm1WvLr36qhW27r1XCgiQfvzRGkLYubP0ww+ELQAAIKmUhKzOnTurbNmy7m4GgJKgenXplVessHXffVbYWr5c6tLFmre1ZAlhCwCAUs7tIWv58uXq1auXoqOjZbPZNHfu3EzbxMfHq2bNmgoKClLbtm21evXq4m8oAGQUEyPFx0s7d0qjRkmBgVa597g4qyLhokWELQAASim3h6wzZ86oefPmio+Pz3L9p59+qtGjR2vSpElau3atmjdvrq5du+rIkSOObVq0aKEmTZpkehw4cKC4vgaA0qpaNWnGDCts3X+/FbZWrrRuaHzVVdLChYQtAABKGZsxnvOvv81m05w5c9S3b1/HsrZt26p169aaOXOmJCktLU0xMTG6//779eijj+Z538uWLdPMmTP1xRdfZLvNhQsXdOHCBcf7xMRExcTE6NSpUwoLC8v/FwJQ+hw4ID3zjFWZ8Px5a9kVV0iTJ1vBy2Zza/MAAEDBJCYmKjw8PE/ZwO09WTlJTk7W77//rri4OMcyHx8fxcXFadWqVS4/3rRp0xQeHu54xMTEuPwYAEq46Gjp//5P+vtv6aGHpKAg6ZdfpG7dpHbtpO++o2cLAIASzqND1rFjx5SamqrKlSs7La9cubIOHTqU5/3ExcXppptu0vz581WtWrVsA9pjjz2mU6dOOR779u0rVPsBlGJVqkgvvmiFrYcfloKDpV9/lXr0sHq25s8nbAEAUEJ5dMhylcWLF+vo0aM6e/as/vnnH7Vr1y7L7QIDAxUWFub0AIBCqVJFeuEFK2yNHm2FrdWrpZ49pbZtpW+/JWwBAFDCeHTIqlixonx9fXX48GGn5YcPH1ZUVJSbWgUABRAVJT3/vLRrlzR2rBW21qyRrr9eatlSmj5d+usvd7cSAAC4gEeHrICAALVs2VJLlixxLEtLS9OSJUuy7Y0CAI9WubL07LPS7t3SI49IISHSunXSuHFSo0ZSgwbSv/5lVShMTXV3awEAQAG4PWQlJSVp/fr1Wr9+vSRp165dWr9+vfbu3StJGj16tN588029++672rJli+69916dOXNGd955pxtbDQCFVKmS1Xu1e7d1c+OuXSV/f2nbNiuEXXWVNdRw6FDpq6+ks2fd3WIAAJBHbi/hvmzZMl199dWZlg8ePFizZ8+WJM2cOVPPPvusDh06pBYtWujll19W27Zti7xt+SnTCACFlpgoff+9NG+eNVfr1Kn0dcHBVgn4Pn2sIYaRke5rJwAApVB+soHbQ5YnI2QBcJuLF6Xly63ANW+e9L/efUnWvbbat7cCV58+Uv367msnAAClBCHLRQhZADyCMdIff6QHrrVrndc3bJgeuNq2lXzcPhIcAIASh5DlIoQsAB5p3z5rnta8edLSpVJKSvq6SpWkXr2swBUXZw0zBAAAhUbIchFCFgCPd+qU9N13VuCaP9+a12UXEuI8j6tiRfe1EwAAL0fIchFCFgCvkpzsPI9r3770dT4+0pVXpg8rrFvXfe0EAMALEbIKKT4+XvHx8UpNTdW2bdsIWQC8jzHS+vXpget/t8lwiI2Veve2AlebNszjAgAgF4QsF6EnC0CJsWdP+jyuH390nscVFZU+j6tLFykoyH3tBADAQxGyXISQBaBESkhIn8f13XfO87iCgqRataSqVaVq1axHxtfVqkkVKlhl5AEAKEUIWS5CyAJQ4iUnS8uWWYHrq6+kf/7J/TOBgenBK7swFhUl+foWefMBACguhCwXIWQBKFWMkXbssApm/POPtH+/9Wx/7N8vHT6ct335+lpBK6sAljGgBQYW7XcCAMBFCFkuQsgCgEskJ0sHDmQdwOyvDxyQUlPztr/IyJyHJtapI/n7F+13AgAgD/KTDfyKqU0AgJIgIECqWdN6ZCc1VTpyJOsAlvH1+fPS0aPW49Lqh3aBgVLz5lLr1lKrVtZzw4YMRQQAeDR6snJATxYAFBFjpBMnsg9g//wj7d0rJSVl/myZMtLll1uByx6+6tShGAcAoEgxXNBFCFkA4EbGSDt3Sr/9Jq1ZYz3WrpXOnMm8bUREek+XPXhVq0bwAgC4DCHLRQhZAOBhUlOlv/6yApc9fK1fb80Vu1TlypmDV6VKxd5kAEDJQMhyEUIWAHiB5GRp0ybn4LVpU9bFN6pXdw5eLVtavWCeyhjp3DlraGXGh7+/1L69dc8yAECxIGS5CCELALzUuXNWD1fG4LV1qxVaLlWvnnPwuuwya96XKxkjnT6dOSzl5XHhQvb7bdJE6thR6tTJeo6Kcm27AQAOhCwXIWQBQAmSmGjN6coYvHbtyrydj48UG+tc0bBZM6vSYWqqdOpUwcJSXsvaZ8XPTypfPv1x8qS0ZUvm7erXTw9cnTpJMTEFPyYAwAkhy0UIWQBQwh0/nh647M8HDmTezt9fCg2VEhKy7g3Lq8BAa4hfxsCUl0doaOYiHkePSitWSD/+aD3++CNz22rVcu7pql2bYiClTVqatGiR1SPao4cV2AEUCCGrkOLj4xUfH6/U1FRt27aNkAUApcmBA5mD1/HjztuEhuY/KJUvLwUHF127T56UfvpJWr7cCl1r12buPata1bmnq0EDQldJde6c9P770vPPS9u2WcsaNpSmTZP69OHnDhQAIctF6MkCAMgYac8e6exZqxeqXDnrpsye7vRp6eef03u61qyRLl503qZSJeeeriZNrOGS8F7HjkmvvirNmGH1dkpSeLjVg2X/z4J27aRnnpE6dHBfOwEvRMhyEUIWAKDEOHtW+uWX9J6uX36Rzp933qZ8eesXb3vwatFC8vV1S3ORTzt2SC++KM2aZfViSVY1zYcfloYOtYYNPvus9MIL6euvv97q2WrSxH3tBrwIIctFCFkAgBLrwgWrd+vHH63gtXJl5hs9h4VJV16Z3tPVqpU1Pw2e45dfrPA0Z076nLzLL5ceeUS68cbMc7AOHpSmTpXefNMaTmqzSYMHS1OmWKEMQLYIWS5CyAIAlBoXL1rzuOw9XStWWBUZMwoJse7PZe/patNGCgpyT3tLs7Q06auvpOees8KxXY8e0tixUufOuc+52rZNGj9e+uIL631goHT//dJjj1k9mgAyIWS5CCELAFBqpaZaFQvtPV3Ll2cuABIYaJW4b9TIut9Y3brWc506RVvko7Q6d0567z2rmMX27dYyf3/p9tulMWOkxo3zv89ff5UefVRatsx6Hx5uvX/gAStUA3AgZLkIIQsAgP9JS5M2b07v6frxR+nw4ey3r1YtPXRlfK5Th1/e8+vYMemVV6SZM9OLWURESPfeK40aJUVHF27/xkgLFkjjxlnBWrL2OWWKNGQIZd+B/yFkuQghCwCAbBhj9ab8+qtVdGH79vTnhIScP1u1qhW6CGA5277dKmYxe3Z6sYoaNaxiFnfdJZUt69rjpaVJH30k/fvfVkVNibLvQAaELBchZAEAkE/GSCdOOIcu+3N+AtilvWClKYCtWmXNt8prMQtXu3DBKgP/5JOUfQcyIGS5CCELAAAXO348c/iyP588mfNnq1bNeghi3breH8BSU6Wvv7YqBf78c/ryHj2scNWpU/H3JJ06ZYW9F16wbgEgUfYdpRohy0UIWQAAFKOcesByC2DR0Vboql/f+VG7tmffPPrcOendd60gYy9mERBgFbMYPbpgxSxcjbLvgCRClssQsgAA8BDZBbAdO6x12fH1lWrVyhy+GjSwgpmPT/F9h4yOHk0vZnHsmLXMXszi/vulKlXc066cZFX2fdQoq+x7hQrubRtQDAhZLkLIAgDAC5w4kR66tm1zfiQlZf+5kJDMvV8NGljP5coVTVu3b7d6rWbPls6ft5bZi1kMHSqFhhbNcV2Jsu8opQhZLkLIAgDAixkjHTokbd2aOXzt3CmlpGT/2YoVsw5fBb0H2M8/W/Ob5s5NL2bRsqU136p/f+8rk07Zd5RChCwXIWQBAFBCXbwo7d6dHroyBrH9+7P/nM1mzUO6NHzVr28t9/VN3zY1VfrqK6uYxapV6ct79pTGjnVPMQtXy67s+1NPSX37ev/3Q/E7fdq6luyP3but56eesv6Tw40IWS5CyAIAoBRKSrKGH17aA7Z1q1VxLzsBAValw/r1pZo1pW++sfZjX3f77dKYMVJsbLF8jWKVVdn3K66Qpk+n7Lsx0pkzVnhITMz8bH99+rQ1XLRyZSkqynquXFmqVMmzi7fkh/0WD5cGqIzvsyty88031n9QuBEhq5Di4+MVHx+v1NRUbdu2jZAFAACsXxCPHs0cvLZts8JUcnLmz0RESPfdZxWI8MRiFq5WUsq+G2NVfswuEOUUli59Pn3a6vErjPLlM4evrF5XqiT5+7vmHBREWpp0+HDWAcq+7MyZ3PcTEWH9R0WNGumPPn3oySop6MkCAAB5kpoq7d2bHrx27rR6tAYP9o5iFq6WVdn3QYOsZYUp+56WZhUMOXcu6+ec1mW3TXZhKTXVdedDss5BWJhUtqz1nPF12bLWdZKUZM0jPHw4/ZHfdlSokLdAFhmZ/0CWkmINp80uQO3dm/V/NlyqcmXnAHVpoPLQ37sJWS5CyAIAACiErMq+Dxpk/RJdkHCUl1/gXclms8LPpYEoq5CU27qQkPzPUUtLs4bXHT7sHL6yen3kSP4DWcWK2QexixczD+vbvz/3Y/j4WDcOzy5AVa9esOIxHoCQ5SKELAAAABdYvdqqRGgv++4Kfn5SUJD1C3vG56yW5fRctmz2YalMGffdSy2/0tKs+XAZe8GyC2YFCWR2/v5WUMoqQNWsaQUsdw5ZLEKELBchZAEAALiIMdLChdJ331m/hOcnDGW1jDLxBZcxkGUXxHx8sh7SFxXlPcHTxQhZLkLIAgAAACDlLxuUzhgKAAAAAEWEkAUAAAAALkTIAgAAAAAXImQBAAAAgAsRsgAAAADAhQhZAAAAAOBChCwAAAAAcCFCFgAAAAC4ECELAAAAAFyIkAUAAAAALkTIAgAAAAAXImQBAAAAgAsRsgAAAADAhQhZWYiPj1dsbKxat27t7qYAAAAA8DI2Y4xxdyM8VWJiosLDw3Xq1CmFhYW5uzkAAAAA3CQ/2YCeLAAAAABwIUIWAAAAALgQIQsAAAAAXMjP3Q3wZPbpaomJiW5uCQAAAAB3smeCvJS0IGTl4PTp05KkmJgYN7cEAAAAgCc4ffq0wsPDc9yG6oI5SEtL04EDB1S2bFnZbDZ3N0eJiYmKiYnRvn37qHZYjDjvxY9z7h6cd/fgvLsH5909OO/uwXl3DWOMTp8+rejoaPn45Dzrip6sHPj4+KhatWrubkYmYWFh/AFxA8578eOcuwfn3T047+7BeXcPzrt7cN4LL7ceLDsKXwAAAACACxGyAAAAAMCFCFleJDAwUJMmTVJgYKC7m1KqcN6LH+fcPTjv7sF5dw/Ou3tw3t2D8178KHwBAAAAAC5ETxYAAAAAuBAhCwAAAABciJAFAAAAAC5EyAIAAAAAFyJkeYn4+HjVrFlTQUFBatu2rVavXu3uJnmNadOmqXXr1ipbtqwqVaqkvn37auvWrU7bdO7cWTabzelxzz33OG2zd+9e9ezZUyEhIapUqZIeeeQRpaSkOG2zbNkyXX755QoMDFTdunU1e/bsov56Hmvy5MmZzmnDhg0d68+fP6+RI0eqQoUKCg0NVf/+/XX48GGnfXDO869mzZqZzrvNZtPIkSMlca27yvLly9WrVy9FR0fLZrNp7ty5TuuNMZo4caKqVKmi4OBgxcXFafv27U7bnDhxQgMHDlRYWJgiIiI0dOhQJSUlOW3zxx9/qEOHDgoKClJMTIymT5+eqS2ff/65GjZsqKCgIDVt2lTz5893+ff1FDmd94sXL2rcuHFq2rSpypQpo+joaA0aNEgHDhxw2kdWf0aefvppp204785yu96HDBmS6Zx269bNaRuu9/zL7bxn9Xe9zWbTs88+69iG692NDDzeJ598YgICAsw777xj/vzzTzN8+HATERFhDh8+7O6meYWuXbuaWbNmmU2bNpn169ebHj16mOrVq5ukpCTHNp06dTLDhw83Bw8edDxOnTrlWJ+SkmKaNGli4uLizLp168z8+fNNxYoVzWOPPebY5u+//zYhISFm9OjRZvPmzWbGjBnG19fXfP/998X6fT3FpEmTTOPGjZ3O6dGjRx3r77nnHhMTE2OWLFlifvvtN3PFFVeY9u3bO9ZzzgvmyJEjTud80aJFRpJZunSpMYZr3VXmz59vxo8fb7788ksjycyZM8dp/dNPP23Cw8PN3LlzzYYNG0zv3r1NrVq1zLlz5xzbdOvWzTRv3tz88ssvZsWKFaZu3brm1ltvdaw/deqUqVy5shk4cKDZtGmT+fjjj01wcLB5/fXXHdusXLnS+Pr6munTp5vNmzebf//738bf399s3LixyM+BO+R03hMSEkxcXJz59NNPzV9//WVWrVpl2rRpY1q2bOm0jxo1apipU6c6/RnI+O8B5z2z3K73wYMHm27dujmd0xMnTjhtw/Wef7md94zn++DBg+add94xNpvN7Ny507EN17v7ELK8QJs2bczIkSMd71NTU010dLSZNm2aG1vlvY4cOWIkmR9//NGxrFOnTubBBx/M9jPz5883Pj4+5tChQ45lr776qgkLCzMXLlwwxhjzr3/9yzRu3NjpczfffLPp2rWra7+Al5g0aZJp3rx5lusSEhKMv7+/+fzzzx3LtmzZYiSZVatWGWM4567y4IMPmjp16pi0tDRjDNd6Ubj0l5+0tDQTFRVlnn32WceyhIQEExgYaD7++GNjjDGbN282ksyaNWsc23z33XfGZrOZ/fv3G2OMeeWVV0y5cuUc590YY8aNG2caNGjgeD9gwADTs2dPp/a0bdvW3H333S79jp4oq186L7V69WojyezZs8exrEaNGubFF1/M9jOc95xlF7L69OmT7We43gsvL9d7nz59zDXXXOO0jOvdfRgu6OGSk5P1+++/Ky4uzrHMx8dHcXFxWrVqlRtb5r1OnTolSSpfvrzT8g8//FAVK1ZUkyZN9Nhjj+ns2bOOdatWrVLTpk1VuXJlx7KuXbsqMTFRf/75p2ObjD8n+zal+ee0fft2RUdHq3bt2ho4cKD27t0rSfr999918eJFp/PVsGFDVa9e3XG+OOeFl5ycrA8++EB33XWXbDabYznXetHatWuXDh065HSOwsPD1bZtW6frOyIiQq1atXJsExcXJx8fH/3666+ObTp27KiAgADHNl27dtXWrVt18uRJxzb8LLJ36tQp2Ww2RUREOC1/+umnVaFCBV122WV69tlnnYbDct4LZtmyZapUqZIaNGige++9V8ePH3es43oveocPH9a3336roUOHZlrH9e4efu5uAHJ27NgxpaamOv3CI0mVK1fWX3/95aZWea+0tDQ99NBDuvLKK9WkSRPH8ttuu001atRQdHS0/vjjD40bN05bt27Vl19+KUk6dOhQlj8D+7qctklMTNS5c+cUHBxclF/N47Rt21azZ89WgwYNdPDgQU2ZMkUdOnTQpk2bdOjQIQUEBGT6xady5cq5nk/7upy2Ka3n/FJz585VQkKChgwZ4ljGtV707Ocpq3OU8RxWqlTJab2fn5/Kly/vtE2tWrUy7cO+rly5ctn+LOz7KM3Onz+vcePG6dZbb1VYWJhj+QMPPKDLL79c5cuX188//6zHHntMBw8e1AsvvCCJ814Q3bp10w033KBatWpp586devzxx9W9e3etWrVKvr6+XO/F4N1331XZsmV1ww03OC3nencfQhZKlZEjR2rTpk366aefnJaPGDHC8bpp06aqUqWKunTpop07d6pOnTrF3cwSoXv37o7XzZo1U9u2bVWjRg199tlnpf6X8OLy9ttvq3v37oqOjnYs41pHaXDx4kUNGDBAxhi9+uqrTutGjx7teN2sWTMFBATo7rvv1rRp0xQYGFjcTS0RbrnlFsfrpk2bqlmzZqpTp46WLVumLl26uLFlpcc777yjgQMHKigoyGk517v7MFzQw1WsWFG+vr6Zqq4dPnxYUVFRbmqVdxo1apS++eYbLV26VNWqVctx27Zt20qSduzYIUmKiorK8mdgX5fTNmFhYYQKSREREapfv7527NihqKgoJScnKyEhwWmbjNc157xw9uzZo8WLF2vYsGE5bse17nr285TT39tRUVE6cuSI0/qUlBSdOHHCJX8GSvO/D/aAtWfPHi1atMipFysrbdu2VUpKinbv3i2J8+4KtWvXVsWKFZ3+XuF6LzorVqzQ1q1bc/37XuJ6L06ELA8XEBCgli1basmSJY5laWlpWrJkidq1a+fGlnkPY4xGjRqlOXPm6IcffsjULZ6V9evXS5KqVKkiSWrXrp02btzo9I+E/R/v2NhYxzYZf072bfg5WZKSkrRz505VqVJFLVu2lL+/v9P52rp1q/bu3es4X5zzwpk1a5YqVaqknj175rgd17rr1apVS1FRUU7nKDExUb/++qvT9Z2QkKDff//dsc0PP/ygtLQ0R/Bt166dli9frosXLzq2WbRokRo0aKBy5co5tuFnkc4esLZv367FixerQoUKuX5m/fr18vHxcQxn47wX3j///KPjx487/b3C9V503n77bbVs2VLNmzfPdVuu92Lk7sobyN0nn3xiAgMDzezZs83mzZvNiBEjTEREhFP1L2Tv3nvvNeHh4WbZsmVOJUzPnj1rjDFmx44dZurUqea3334zu3btMvPmzTO1a9c2HTt2dOzDXtb6uuuuM+vXrzfff/+9iYyMzLKs9SOPPGK2bNli4uPjS11Z64zGjBljli1bZnbt2mVWrlxp4uLiTMWKFc2RI0eMMVYJ9+rVq5sffvjB/Pbbb6Zdu3amXbt2js9zzgsuNTXVVK9e3YwbN85pOde665w+fdqsW7fOrFu3zkgyL7zwglm3bp2jit3TTz9tIiIizLx588wff/xh+vTpk2UJ98suu8z8+uuv5qeffjL16tVzKmmdkJBgKleubO644w6zadMm88knn5iQkJBMpZX9/PzMc889Z7Zs2WImTZpUoksr53Tek5OTTe/evU21atXM+vXrnf6+t1dO+/nnn82LL75o1q9fb3bu3Gk++OADExkZaQYNGuQ4Buc9s5zO++nTp83YsWPNqlWrzK5du8zixYvN5ZdfburVq2fOnz/v2AfXe/7l9veMMVYJ9pCQEPPqq69m+jzXu3sRsrzEjBkzTPXq1U1AQIBp06aN+eWXX9zdJK8hKcvHrFmzjDHG7N2713Ts2NGUL1/eBAYGmrp165pHHnnE6d5Bxhize/du0717dxMcHGwqVqxoxowZYy5evOi0zdKlS02LFi1MQECAqV27tuMYpdHNN99sqlSpYgICAkzVqlXNzTffbHbs2OFYf+7cOXPfffeZcuXKmZCQENOvXz9z8OBBp31wzgtmwYIFRpLZunWr03KudddZunRpln+vDB482BhjlXGfMGGCqVy5sgkMDDRdunTJ9PM4fvy4ufXWW01oaKgJCwszd955pzl9+rTTNhs2bDBXXXWVCQwMNFWrVjVPP/10prZ89tlnpn79+iYgIMA0btzYfPvtt0X2vd0tp/O+a9eubP++t98n7vfffzdt27Y14eHhJigoyDRq1Mg89dRTTmHAGM77pXI672fPnjXXXXediYyMNP7+/qZGjRpm+PDhmf4jmOs9/3L7e8YYY15//XUTHBxsEhISMn2e6929bMYYU6RdZQAAAABQijAnCwAAAABciJAFAAAAAC5EyAIAAAAAFyJkAQAAAIALEbIAAAAAwIUIWQAAAADgQoQsAAAAAHAhQhYAAAAAuBAhCwAAAABciJAFAChVjh49qnvvvVfVq1dXYGCgoqKi1LVrV61cuVKSZLPZNHfuXPc2EgDg1fzc3QAAAIpT//79lZycrHfffVe1a9fW4cOHtWTJEh0/ftzdTQMAlBA2Y4xxdyMAACgOCQkJKleunJYtW6ZOnTplWl+zZk3t2bPH8b5GjRravXu3JGnevHmaMmWKNm/erOjoaA0ePFjjx4+Xn5/1/5U2m02vvPKKvvrqKy1btkxVqlTR9OnTdeONNxbLdwMAeA6GCwIASo3Q0FCFhoZq7ty5unDhQqb1a9askSTNmjVLBw8edLxfsWKFBg0apAcffFCbN2/W66+/rtmzZ+s///mP0+cnTJig/v37a8OGDRo4cKBuueUWbdmypei/GADAo9CTBQAoVf773/9q+PDhOnfunC6//HJ16tRJt9xyi5o1aybJ6pGaM2eO+vbt6/hMXFycunTposcee8yx7IMPPtC//vUvHThwwPG5e+65R6+++qpjmyuuuEKXX365XnnlleL5cgAAj0BPFgCgVOnfv78OHDigr776St26ddOyZct0+eWXa/bs2dl+ZsOGDZo6daqjJyw0NFTDhw/XwYMHdfbsWcd27dq1c/pcu3bt6MkCgFKIwhcAgFInKChI1157ra699lpNmDBBw4YN06RJkzRkyJAst09KStKUKVN0ww03ZLkvAAAyoicLAFDqxcbG6syZM5Ikf39/paamOq2//PLLtXXrVtWtWzfTw8cn/Z/SX375xelzv/zyixo1alT0XwAA4FHoyQIAlBrHjx/XTTfdpLvuukvNmjVT2bJl9dtvv2n69Onq06ePJKvC4JIlS3TllVcqMDBQ5cqV08SJE3X99derevXquvHGG+Xj46MNGzZo06ZNevLJJx37//zzz9WqVStdddVV+vDDD7V69Wq9/fbb7vq6AAA3ofAFAKDUuHDhgiZPnqyFCxdq586dunjxomJiYnTTTTfp8ccfV3BwsL7++muNHj1au3fvVtWqVR0l3BcsWKCpU6dq3bp18vf3V8OGDTVs2DANHz5cklX4Ij4+XnPnztXy5ctVpUoVPfPMMxowYIAbvzEAwB0IWQAAuEBWVQkBAKUTc7IAAAAAwIUIWQAAAADgQhS+AADABRh9DwCwoycLAAAAAFyIkAUAAAAALkTIAgAAAAAXImQBAAAAgAsRsgAAAADAhQhZAAAAAOBChCwAAAAAcCFCFgAAAAC40P8DD1mA3V/REhUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**\n",
        "\n",
        "Make an observation from the above plot. Do the test and train loss curves indicate that the model should train longer to improve accuracy? Or does it indicate that 20 epochs is too long? Edit the cell below to answer these questions."
      ],
      "metadata": {
        "id": "7LwAnncyLrv-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loss is still decreasing and the test loss is flat or slightly decreasing, the model can continue training for a few more epochs, especially if the test loss hasn’t reached its minimum yet.\n"
      ],
      "metadata": {
        "id": "nU8VwbfPMFT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Moving to the GPU\n",
        "\n",
        "Now that you have a model trained on the CPU, let's finally utilize the T4 GPU that we requested for this instance.\n",
        "\n",
        "Using a GPU with torch is relatively simple, but has a few gotchas. Torch abstracts away most of the CUDA runtime API, but has a few hold-over concepts such as moving data between devices.\n",
        "Additionally, since the GPU is treated as a device separate from the CPU, you cannot combine CPU and GPU based tensors in the same operation. Doing so will result in a device mismatch error. If this occurs, check where the tensors are located (you can always print `.device` on a tensor), and make sure they have been properly moved to the correct device.\n",
        "\n",
        "You will start by creating a new model, optimizer, and criterion (not really necessary in this case since you already did this above but it's better for clarity and completeness). However, one change that you'll make is moving the model to the GPU first. This can be done by calling `.cuda()` in general, or `.to(\"cuda\")` to be more explicit. In general specific GPU devices can be targetted such as `.to(\"cuda:0\")` for the first GPU (index 0), etc., but since there is only one GPU in Colab this is not necessary in this case."
      ],
      "metadata": {
        "id": "uTlmnBdaMOCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = MLP()\n",
        "\n",
        "# move the model to the GPU\n",
        "model.cuda()\n",
        "\n",
        "# for a critereon (loss) funciton, we will use Cross-Entropy Loss. This is the most common critereon used for multi-class prediction, and is also used by tokenized transformer models\n",
        "# it takes in an un-normalized probability distribution (i.e. without softmax) over N classes (in our case, 10 classes with MNIST). This distribution is then compared to an integer label\n",
        "# which is < N. For MNIST, the prediction might be [-0.0056, -0.2044,  1.1726,  0.0859,  1.8443, -0.9627,  0.9785, -1.0752, 1.1376,  1.8220], with the label 3.\n",
        "# Cross-entropy can be thought of as finding the difference between what the predicted distribution and the one-hot distribution\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# then you can instantiate the optimizer. You will use Stochastic Gradient Descent (SGD), and can set the learning rate to 0.1 with a momentum factor of 0.5\n",
        "# the first input to the optimizer is the list of model parameters, which is obtained by calling .parameters() on the model object\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "wFzM3dX0LbTr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0"
      ],
      "metadata": {
        "id": "icc6zF4RN0aX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.cuda()\n",
        "target = target.cuda()\n",
        "\n",
        "import time\n",
        "import torch\n",
        "\n",
        "# Function to track time during training\n",
        "for epoch in range(10, 20):  # Continue from epoch 10 to 19 for a total of 20 epochs\n",
        "    start_time = time.time()  # Record the start time of the epoch\n",
        "\n",
        "    # Train for one epoch\n",
        "    model.train()  # Set model to training mode\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "\n",
        "    for batch_idx, (data, target) in pbar:\n",
        "        # Move data and target to GPU\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # Zero the gradients before backpropagation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update step counter\n",
        "        current_step += 1\n",
        "\n",
        "        # Update progress bar\n",
        "        if batch_idx % 100 == 0:\n",
        "            train_losses.append(loss.item())\n",
        "            train_steps.append(current_step)\n",
        "            pbar.set_description(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
        "                                 f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "    # Measure time after epoch ends\n",
        "    epoch_time = time.time() - start_time  # Calculate time taken for the epoch\n",
        "    print(f'Epoch {epoch+1} completed in {epoch_time:.2f} seconds.')\n",
        "\n",
        "    # Test after training\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(test_loader, total=len(test_loader), desc=\"Testing...\")\n",
        "\n",
        "        for data, target in pbar:\n",
        "            # Move data and target to GPU\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            # Compute accuracy\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy.append(correct / len(test_loader.dataset))\n",
        "    test_steps.append(current_step)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Calculate the total time for the last 10 epochs (epoch 10 to 19)\n",
        "total_time = time.time() - start_time\n",
        "print(f'\\nTotal time for 10 epochs (with testing): {total_time:.2f} seconds.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gUK6vSBQ6JzB",
        "outputId": "8562db87-b6a2-4e5a-8cfe-63598211246c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.057679: 100%|██████████| 938/938 [00:15<00:00, 59.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 completed in 15.73 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 75.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0786, Accuracy: 9750/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.053418: 100%|██████████| 938/938 [00:13<00:00, 67.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 completed in 13.98 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 75.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0779, Accuracy: 9761/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.035589: 100%|██████████| 938/938 [00:14<00:00, 66.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 completed in 14.14 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 55.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0752, Accuracy: 9767/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.079151: 100%|██████████| 938/938 [00:14<00:00, 66.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 completed in 14.17 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 76.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0734, Accuracy: 9767/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.016529: 100%|██████████| 938/938 [00:14<00:00, 66.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 completed in 14.16 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 73.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0726, Accuracy: 9769/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.021297: 100%|██████████| 938/938 [00:14<00:00, 66.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 completed in 14.10 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 56.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0708, Accuracy: 9780/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.010828: 100%|██████████| 938/938 [00:14<00:00, 66.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 completed in 14.15 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 77.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0698, Accuracy: 9775/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.022888: 100%|██████████| 938/938 [00:13<00:00, 67.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 completed in 13.81 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 75.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0702, Accuracy: 9777/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.051657: 100%|██████████| 938/938 [00:16<00:00, 57.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 completed in 16.42 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:03<00:00, 50.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0703, Accuracy: 9773/10000 (98%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.029705: 100%|██████████| 938/938 [00:16<00:00, 57.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 completed in 16.43 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 57.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0726, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "\n",
            "Total time for 10 epochs (with testing): 19.18 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, copy your previous training code with the timing parameters below.\n",
        "It needs to be slightly modified to move everything to the GPU.\n",
        "\n",
        "Before the line `output = model(data)`, add:\n",
        "```\n",
        "data = data.cuda()\n",
        "target = target.cuda()\n",
        "```\n",
        "\n",
        "Note that this is needed in both the train and test functions.\n",
        "\n",
        "**Question 5**\n",
        "\n",
        "Please edit the cell below to show the new GPU train and test fucntions."
      ],
      "metadata": {
        "id": "3GF2ZC05N4bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the new GPU training functions\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def gpu_train(epoch, train_losses, steps, current_step, model, train_loader, optimizer, criterion):\n",
        "    # Set the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Create tqdm progress bar\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch}\")\n",
        "\n",
        "    for batch_idx, (data, target) in pbar:\n",
        "        # Move data and target to GPU\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # Zero the gradients before the backpropagation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "       # Forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Increment global step count\n",
        "        current_step += 1\n",
        "\n",
        "        # Update progress bar and log training loss\n",
        "        if batch_idx % 100 == 0:\n",
        "            train_losses.append(loss.item())\n",
        "            steps.append(current_step)\n",
        "            pbar.set_description(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}\"\n",
        "                                 f\" ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "    return current_step\n",
        "\n"
      ],
      "metadata": {
        "id": "WfWk9TD8OSMB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new GPU training for 10 epochs\n",
        "def gpu_test(test_losses, test_accuracy, steps, current_step, model, test_loader, criterion):\n",
        "    # Set the model in evaluation mode\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Create tqdm progress bar for testing\n",
        "    pbar = tqdm(test_loader, total=len(test_loader), desc=\"Testing...\")\n",
        "\n",
        "    # No need for gradients during testing\n",
        "    with torch.no_grad():\n",
        "        for data, target in pbar:\n",
        "            # Move data and target to GPU\n",
        "            data = data.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "\n",
        "            # Compute accuracy by getting the index of the max output\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Average test loss\n",
        "    test_loss /= len(test_loader)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    test_accuracy.append(accuracy)\n",
        "\n",
        "    steps.append(current_step)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({accuracy:.0f}%)\\n')\n"
      ],
      "metadata": {
        "id": "LWQq5oIHZvem"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6**\n",
        "\n",
        "Is training faster now that it is on a GPU? Is the speedup what you would expect? Why or why not? Edit the cell below to answer."
      ],
      "metadata": {
        "id": "mmAxl15QP15i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Without GPU 20 epochs completed in 17.49 secs where as with GPU 20 epochs completed in 14.18 secs"
      ],
      "metadata": {
        "id": "eCW8zvzjP-_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Another Model Type: CNN\n",
        "\n",
        "Until now you have trained a simple MLP for MNIST classification, however, MLPs are not a particularly good for images.\n",
        "\n",
        "Firstly, using a MLP will require that all images have the same size and shape, since they are unrolled in the input.\n",
        "\n",
        "Secondly, in general images can make use of translation invariance (a type of data symmetry), but this cannot but leveraged with a MLP.\n",
        "\n",
        "For these reasons, a convolutional network is more appropriate, as it will pass kernels over the 2D image, removing the requirement for a fixed image size and leveraging the translation invariance of the 2D images.\n",
        "\n",
        "Let's define a simple CNN below."
      ],
      "metadata": {
        "id": "oY3b857_X13z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    # define the constructor for the network\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # instead of declaring the layers independently, let's use the nn.Sequential feature\n",
        "        # these blocks will be executed in list order\n",
        "\n",
        "        # you will break up the model into two parts:\n",
        "        # 1) the convolutional network\n",
        "        # 2) the prediction head (a small MLP)\n",
        "\n",
        "        # the convolutional network\n",
        "        self.net = nn.Sequential(\n",
        "          nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # the input projection layer - note that a stride of 1 means you are not down-sampling\n",
        "          nn.ReLU(),                                             # activation\n",
        "          nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # an inner layer - note that a stride of 2 means you are down sampling. The output is 28x28 -> 14x14\n",
        "          nn.ReLU(),                                             # activation\n",
        "          nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),# an inner layer - note that a stride of 2 means you are down sampling. The output is 14x14 -> 7x7\n",
        "          nn.ReLU(),                                             # activation\n",
        "          nn.AdaptiveMaxPool2d(1),                               # a pooling layer which will output a 1x1 vector for the prediciton head\n",
        "        )\n",
        "\n",
        "        # the prediction head\n",
        "        self.head = nn.Sequential(\n",
        "          nn.Linear(128, 64),      # input projection, the output from the pool layer is a 128 element vector\n",
        "          nn.ReLU(),               # activation\n",
        "          nn.Linear(64, 10)        # class projection to one of the 10 classes (digits 0-9)\n",
        "        )\n",
        "\n",
        "\n",
        "    # define the forward pass compute graph\n",
        "    def forward(self, x):\n",
        "\n",
        "        # pass the input through the convolution network\n",
        "        x = self.net(x)\n",
        "\n",
        "        # reshape the output from Bx128x1x1 to Bx128\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # pass the pooled vector into the prediction head\n",
        "        x = self.head(x)\n",
        "\n",
        "        # the output here is Bx10\n",
        "        return x"
      ],
      "metadata": {
        "id": "qPrKiCT-QY4I"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = CNN()\n",
        "\n",
        "# print the model and the parameter count\n",
        "print(model)\n",
        "param_count = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Model has {param_count:,} trainable parameters\")\n",
        "\n",
        "# the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# then you can intantiate the optimizer. You will use Stochastic Gradient Descent (SGD), and can set the learning rate to 0.1 with a\n",
        "# momentum factor of 0.5\n",
        "# the first input to the optimizer is the list of model parameters, which is obtained by calling .parameters() on the model object\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "-nNkxodMTfs3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "66de0244-c1dc-4607-d11e-7efde139ca5d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (net): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): AdaptiveMaxPool2d(output_size=1)\n",
            "  )\n",
            "  (head): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Model has 101,578 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7**\n",
        "\n",
        "Notice that this model now has fewer parameters than the MLP. Let's see how it trains.\n",
        "\n",
        "Using the previous code to train on the CPU with timing, edit the cell below to execute 2 epochs of training."
      ],
      "metadata": {
        "id": "yFTQhVJLcqFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0"
      ],
      "metadata": {
        "id": "8lEsCyWFdBLs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train for 2 epochs on the CPU\n",
        "import time\n",
        "\n",
        "# create a new array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0\n",
        "\n",
        "# Train for 2 epochs with timing\n",
        "for epoch in range(0, 2):\n",
        "    start_time = time.time()  # Start timing the epoch\n",
        "\n",
        "    current_step = cpu_train(current_epoch, train_losses, train_steps, current_step)\n",
        "    cpu_test(test_losses, test_accuracy, test_steps, current_step)\n",
        "\n",
        "    epoch_time = time.time() - start_time  # Calculate the time taken for this epoch\n",
        "    print(f\"Epoch {epoch + 1} took {epoch_time:.2f} seconds.\")\n",
        "\n",
        "    current_epoch += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "RNrJypFFdEWU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b014c975-f51a-4667-a205-d9bf51438032"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.493148: 100%|██████████| 938/938 [01:16<00:00, 12.18it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:06<00:00, 25.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.5423, Accuracy: 8207/10000 (82%)\n",
            "\n",
            "Epoch 1 took 83.23 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.208951: 100%|██████████| 938/938 [01:15<00:00, 12.41it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:06<00:00, 22.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.2933, Accuracy: 9084/10000 (91%)\n",
            "\n",
            "Epoch 2 took 82.56 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8**\n",
        "\n",
        "Now, let's move the model to the GPU and try training for 2 epochs there."
      ],
      "metadata": {
        "id": "KzTD8AHOdIlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "model = CNN()\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "# print the model and the parameter count\n",
        "print(model)\n",
        "param_count = sum([p.numel() for p in model.parameters()])\n",
        "print(f\"Model has {param_count:,} trainable parameters\")\n",
        "\n",
        "# the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# then you can instantiate the optimizer. You will use Stochastic Gradient Descent (SGD), and can set the learning rate to 0.1 with a momentum factor of 0.5\n",
        "# the first input to the optimizer is the list of model parameters, which is obtained by calling .parameters() on the model object\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "DUZs-CRbdTys",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "36d347c9-6eff-4fa7-e585-560191dfc5e3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (net): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): AdaptiveMaxPool2d(output_size=1)\n",
            "  )\n",
            "  (head): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Model has 101,578 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new array to log the loss and accuracy\n",
        "train_losses = []\n",
        "train_steps = []\n",
        "test_steps = []\n",
        "test_losses = []\n",
        "test_accuracy = []\n",
        "current_step = 0  # Start with global step 0\n",
        "current_epoch = 0 # Start with epoch 0"
      ],
      "metadata": {
        "id": "_OBIhnMbdZX0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train for 2 epochs on the GPU\n",
        "def gpu_train(epoch, train_losses, steps, current_step):\n",
        "    model.train()\n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "\n",
        "    for batch_idx, (data, target) in pbar:\n",
        "        data, target = data.cuda(), target.cuda()  # Move data and target to GPU\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        current_step += 1\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            train_losses.append(loss.item())\n",
        "            steps.append(current_step)\n",
        "\n",
        "            desc = (f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}'\n",
        "                    f' ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "            pbar.set_description(desc)\n",
        "\n",
        "    return current_step\n",
        "\n",
        "def gpu_test(test_losses, test_accuracy, steps, current_step):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # Create tqdm progress bar\n",
        "    pbar = tqdm(test_loader, total=len(test_loader), desc=\"Testing...\")\n",
        "\n",
        "    # Use no_grad() as we do not need gradients for evaluation\n",
        "    with torch.no_grad():\n",
        "        for data, target in pbar:\n",
        "            # Move data and target to the GPU\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # Forward pass through the model\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # Add the loss for this batch\n",
        "\n",
        "            # Calculate the number of correct predictions\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()  # Compare predictions to the true labels\n",
        "\n",
        "    test_loss /= len(test_loader)  # Calculate the average test loss\n",
        "\n",
        "    # Log the test loss and accuracy\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy.append(correct / len(test_loader.dataset))  # Accuracy as a fraction\n",
        "    steps.append(current_step)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
        "          f' ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "\n",
        "# Train for 2 epochs with timing on GPU\n",
        "for epoch in range(0, 2):\n",
        "    start_time = time.time()  # Start timing the epoch\n",
        "\n",
        "    current_step = gpu_train(current_epoch, train_losses, train_steps, current_step)  # Use GPU version of train function\n",
        "    gpu_test(test_losses, test_accuracy, test_steps, current_step)  # Use GPU version of test function\n",
        "\n",
        "    epoch_time = time.time() - start_time  # Calculate the time taken for this epoch\n",
        "    print(f\"Epoch {epoch + 1} took {epoch_time:.2f} seconds.\")\n",
        "\n",
        "    current_epoch += 1"
      ],
      "metadata": {
        "id": "Vz69D4h_dahU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d051866e-443d-49a8-c3f5-7bb24d1de3d8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.412111: 100%|██████████| 938/938 [00:15<00:00, 62.27it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 68.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.2596, Accuracy: 9204/10000 (92%)\n",
            "\n",
            "Epoch 1 took 17.35 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.155251: 100%|██████████| 938/938 [00:15<00:00, 60.39it/s]\n",
            "Testing...: 100%|██████████| 157/157 [00:02<00:00, 59.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1737, Accuracy: 9457/10000 (95%)\n",
            "\n",
            "Epoch 2 took 18.20 seconds.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9**\n",
        "\n",
        "How do the CPU and GPU versions compare for the CNN? Is one faster than the other? Why do you think this is, and how does it differ from the MLP? Edit the cell below to answer."
      ],
      "metadata": {
        "id": "Nd3Lv1AKddG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For CPU version Epoch 1 took 83.23 seconds and Epoch 2 took 82.56 seconds.\n",
        "\n",
        "For GPU version Epoch 1 took 17.35 seconds and Epoch 2 took 18.20 seconds.\n",
        "\n",
        "The GPU version is faster for several reasons:\n",
        "\n",
        "1. Parallelization:\n",
        "GPU: GPUs are designed to perform many operations simultaneously. They excel at handling parallel tasks, especially for operations like matrix multiplications, convolutions, and other tensor-based operations, which are common in deep learning. The large number of cores in GPUs allows them to execute many computations at once, significantly speeding up tasks such as training and inference in deep neural networks.\n",
        "CPU: The CPU has fewer cores (typically 4 to 16), which makes it more suited for single-threaded tasks, and while it can execute parallel operations, it does so with far fewer resources compared to a GPU. This limits its ability to efficiently handle large-scale operations like those required in deep learning.\n",
        "2. CNN vs MLP (Multi-Layer Perceptron):\n",
        "MLP: A Multi-Layer Perceptron is a fully connected network where each neuron in a layer is connected to every neuron in the next layer. This creates a large number of parameters, and each input feature is multiplied by all weights in the layer, making the computation intensive, especially for large datasets like images. While the computations themselves can be parallelized, the dense connections in an MLP can result in fewer advantages from GPU acceleration compared to a convolutional approach.\n",
        "CNN: A Convolutional Neural Network (CNN) is specifically designed for image data and utilizes convolutional layers that apply filters (kernels) across the input image. These convolutional operations are highly parallelizable because:\n",
        "The same filter is applied repeatedly across different parts of the image, allowing for efficient computation of feature maps.\n",
        "Convolutions are localized, meaning fewer parameters need to be learned per filter, which makes the process less computationally expensive than an MLP.\n",
        "CNNs often have pooling layers that downsample the feature maps, reducing the size of the data and computation required in later layers.\n",
        "\n",
        "Key Differences in Operation:\n",
        "MLP requires flattening the entire image into a vector before feeding it into the network. This means the input image size must be fixed, and every pixel is connected to every node in the network. For large images, this can result in a huge number of weights and computations.\n",
        "\n",
        "CNN works directly with the 2D structure of the image, applying filters across small regions (local receptive fields) to capture spatial hierarchies in the data. The convolution operation, combined with pooling, reduces the computational complexity by reducing the number of connections and focusing on local features first before moving on to higher-level features.\n",
        "\n",
        "Why CNNs Are Faster:\n",
        "CNNs exploit the spatial structure of images by applying filters locally, which reduces the number of parameters compared to MLPs.\n",
        "CNNs benefit greatly from GPUs because convolutions and matrix multiplications can be performed in parallel, taking full advantage of the GPU's parallel processing capabilities.\n",
        "The use of pooling layers in CNNs reduces the data size, meaning fewer computations are needed as the network progresses.\n",
        "\n",
        "In summary, the GPU is faster because it can handle parallel operations more efficiently, and CNNs are faster than MLPs on image data due to their local, sparse connections and the ability to exploit spatial hierarchies, making them more computationally efficient for tasks like image classification."
      ],
      "metadata": {
        "id": "GsZLGgvYdo2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final comparison, you can profile the FLOPs (floating-point operations) executed by each model. You will use the thop.profile function for this and consider an MNIST batch size of 1."
      ],
      "metadata": {
        "id": "YPM7A8rNdqr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the input shape of a MNIST sample with batch_size = 1\n",
        "input = torch.randn(1, 1, 28, 28)\n",
        "\n",
        "# create a copy of the models on the CPU\n",
        "mlp_model = MLP()\n",
        "cnn_model = CNN()\n",
        "\n",
        "# profile the MLP\n",
        "flops, params = thop.profile(mlp_model, inputs=(input, ), verbose=False)\n",
        "print(f\"MLP has {params:,} params and uses {flops:,} FLOPs\")\n",
        "\n",
        "# profile the CNN\n",
        "flops, params = thop.profile(cnn_model, inputs=(input, ), verbose=False)\n",
        "print(f\"CNN has {params:,} params and uses {flops:,} FLOPs\")"
      ],
      "metadata": {
        "id": "H2PiALplYywV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6aecd875-f7cb-4abc-b48f-b95c86a98ad8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP has 109,386.0 params and uses 109,184.0 FLOPs\n",
            "CNN has 101,578.0 params and uses 7,459,968.0 FLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ukX7O7eMHd2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10**\n",
        "\n",
        "Are these results what you would have expected? Do they explain the performance difference between running on the CPU and GPU? Why or why not? Edit the cell below to answer."
      ],
      "metadata": {
        "id": "lrnbL1fIe0Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the CNN has fewer parameters than the MLP, it has a significantly higher number of FLOPs. This is because convolutions require computing the dot product of filters with patches of the input image. These operations are computationally expensive due to the number of times the kernel is applied across the entire image.\n",
        "\n",
        "Additionally, CNNs often perform more advanced operations like pooling, activation functions, and downsampling, all of which contribute to the high FLOPs count.\n",
        "However, the CNN has fewer parameters because it shares weights across the entire image, making it more efficient in terms of parameterization."
      ],
      "metadata": {
        "id": "uWnlxMVjfDUe"
      }
    }
  ]
}